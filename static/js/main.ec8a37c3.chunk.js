(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[,,,,,,,,,function(e,t,a){e.exports=a.p+"static/media/logo.77022b1f.png"},,function(e,t,a){e.exports=a.p+"static/media/gan.217ad21e.png"},function(e,t,a){e.exports=a.p+"static/media/me.6177414b.jpeg"},function(e,t,a){e.exports=a.p+"static/media/city.1ac50d22.jpg"},function(e,t,a){e.exports=a.p+"static/media/elephants.dc5d8d90.jpg"},function(e,t,a){e.exports=a.p+"static/media/zebras.36bf930a.jpg"},,,,,,function(e,t,a){e.exports=a.p+"static/media/jojo.ffc5e361.png"},,,,,,,,,,function(e,t,a){e.exports=a.p+"static/media/generate.e3f97e13.svg"},function(e,t,a){e.exports=a.p+"static/media/logocircle.db2e1285.png"},function(e,t,a){e.exports=a.p+"static/media/best.db06e2f4.mp4"},function(e,t,a){e.exports=a.p+"static/media/gd.d1e7aee6.mp4"},function(e,t,a){e.exports=a.p+"static/media/satelite.bedf6a1b.jpg"},function(e,t,a){e.exports=a.p+"static/media/mountain.ec15cce5.jpeg"},function(e,t,a){e.exports=a.p+"static/media/monc.23ab5362.jpeg"},function(e,t,a){e.exports=a.p+"static/media/cameramotion.b286947f.png"},function(e,t,a){e.exports=a.p+"static/media/fewshotsegmentation.9ed7ba63.png"},function(e,t,a){e.exports=a.p+"static/media/fewshotsegmentation.9ed7ba63.png"},function(e,t,a){e.exports=a.p+"static/media/image classification.c8491766.jpeg"},function(e,t,a){e.exports=a.p+"static/media/objectdetection.90f7d919.jpg"},function(e,t,a){e.exports=a.p+"static/media/yolov3.9b235650.png"},function(e,t,a){e.exports=a.p+"static/media/segmentation.d53478c8.jpg"},function(e,t,a){e.exports=a.p+"static/media/style transfer.4659ed8c.jpg"},function(e,t,a){e.exports=a.p+"static/media/selfsupervb.61921a3c.png"},function(e,t,a){e.exports=a.p+"static/media/super resolution.5d60f59b.jpg"},function(e,t,a){e.exports=a.p+"static/media/generalization.54021714.png"},function(e,t,a){e.exports=a.p+"static/media/unsupervised.fcf583ad.png"},function(e,t,a){e.exports=a.p+"static/media/partial.05498c50.png"},function(e,t,a){e.exports=a.p+"static/media/background.b67ca5e8.jpeg"},function(e,t,a){e.exports=a.p+"static/media/me.6177414b.jpeg"},function(e,t,a){e.exports=a.p+"static/media/kaiminghe.689fa05d.jpg"},function(e,t,a){e.exports=a.p+"static/media/rohit.50d86237.png"},function(e,t,a){e.exports=a.p+"static/media/thomas.dcae42a0.jpg"},function(e,t,a){e.exports=a.p+"static/media/logocircle.db2e1285.png"},,function(e,t,a){e.exports=a(97)},,,,,function(e,t,a){},,,,,,function(e,t,a){},,function(e,t,a){e.exports=a.p+"static/media/cvlab.76b6db16.png"},,,,,,,,,,,,,,,,,,,,,function(e,t,a){},,function(e,t,a){e.exports=a.p+"static/media/streetview.dc6c314a.jpg"},function(e,t,a){e.exports=a.p+"static/media/computer vision.44f88a0d.jpg"},,function(e,t,a){"use strict";a.r(t);var n,i,r,o,s,l,c,m,d,p,h,g,u,f,b,w,E,k,v,x,y,C,S,N,I,A,M=a(0),j=a.n(M),O=a(10),D=a.n(O),T=(a(63),a(1)),L=a(5),z=a(3),R=a(2),V=Object(M.createContext)(),B=function(){var e=Object(M.useContext)(V);return{darkMode:e.darkMode,setDarkMode:e.setDarkMode}},P=function(){var e=Object(M.useContext)(V);return{sideBar:e.sideBar,setSideBar:e.setSideBar}},F=function(){var e=Object(M.useContext)(V);return{visualizer:e.visualizer,setVisualizer:e.setVisualizer}},H=function(e){var t=e.children,a=Object(M.useState)(!0),n=Object(T.a)(a,2),i=n[0],r=n[1],o=Object(M.useState)(!1),s=Object(T.a)(o,2),l=s[0],c=s[1],m=Object(M.useState)(!1),d=Object(T.a)(m,2),p=d[0],h=d[1];return j.a.createElement(V.Provider,{value:{darkMode:i,setDarkMode:r,sideBar:l,setSideBar:c,visualizer:p,setVisualizer:h}},t)},W=R.c.span(n||(n=Object(z.a)(["\n    font-weight: ",";\n    font-size: ","px;\n    line-height: ","px;\n    color: ",";\n    ",";\n    ",";\n    flex: ",";\n    font-family: ",";\n    z-index: ",";\n    ",";\n    ",";\n    ",";\n    transition: all 500ms ease;\n"])),function(e){return e.weight},function(e){return e.size?e.size:15},function(e){return e.lh},function(e){return e.color?e.color:B().darkMode?"white":"black"},function(e){return e.margin},function(e){return e.padding},function(e){return e.flex},function(e){return e.ff},function(e){return e.zIndex},function(e){return e.to?"cursor: pointer;":null},function(e){return e.to?":hover { opacity: 0.85 }":null},function(e){return"none"===e.us?"user-select: none":null}),G=R.c.div(i||(i=Object(z.a)(["\n    display: flex;\n    align-items: ",";\n    justify-content: ",";\n    ",";\n    width: ",";\n    height: ",";\n    background-color: ",";\n    ",";\n    border-radius: ",";\n    box-sizing: border-box;\n    flex: ",";\n    z-index: ",";\n    transition: all 500ms ease;\n    ",";\n    ",";\n    ",";\n    opacity: ",";\n    position: ",";\n    :hover {\n        img {\n            opacity: ","\n        }\n        span {\n            opacity: ","\n        }\n    };\n    ","\n    ",";\n    ",";\n    ","\n"])),function(e){return e.align},function(e){return e.justify},function(e){return e.margin},function(e){return e.size?e.size:e.width},function(e){return e.size?e.size:e.height},function(e){return e.bg},function(e){return e.padding},function(e){return e.circle?"50%":e.br},function(e){return e.flex},function(e){return e.zIndex},function(e){return e.to?"cursor: pointer;":null},function(e){return e.to&&!e.cursorOpaFalse?":hover { opacity: 0.85 }":null},function(e){return"none"===e.us?"user-select: none":null},function(e){return e.opacity},function(e){return e.position},function(e){return e.to&&!e.cursorOpaFalse?.85:1},function(e){return e.to&&!e.cursorOpaFalse?.85:1},function(e){return e.shadow&&"transition: background-color .3s,color .3s,box-shadow .3s;"},function(e){return e.shadow&&"box-shadow: 0 0 0.1rem rgb(0 0 0 / 10%), 0 0.15rem 0.9rem rgb(0 0 0 / 10%);"},function(e){return e.scrollbarFalse&&"\n    ::-webkit-scrollbar {\n        display: none;\n        -ms-overflow-style: none;  /* IE and Edge */\n        scrollbar-width: none;\n      }"},function(e){return(e.bw||e.bc)&&"border: ".concat(e.bw?e.bw:1,"px solid ").concat(e.bc?e.bc:"white"," ")}),_=Object(R.c)(G)(r||(r=Object(z.a)(["\n    flex-direction: row;\n"]))),U=Object(R.c)(G)(o||(o=Object(z.a)(["\n    flex-direction: column;\n"]))),Z=Object(R.c)(U)(s||(s=Object(z.a)(["\n    transition: background-color .3s,color .3s,box-shadow .3s;\n    box-shadow: 0 0 0.1rem rgb(0 0 0 / 10%), 0 0.15rem 0.9rem rgb(0 0 0 / 10%);\n    border: 0;\n    ::-webkit-scrollbar {\n        display: none;\n        -ms-overflow-style: none;  /* IE and Edge */\n        scrollbar-width: none;\n      }\n"]))),K=R.c.img(l||(l=Object(z.a)(["\n    position: ",";\n    object-fit: ",";\n    ",";\n    width: ",";\n    height: ",";\n    opacity: ",";\n    z-index: ",";\n"])),function(e){return e.position},function(e){return e.of},function(e){return e.margin},function(e){return e.width},function(e){return e.height},function(e){return e.opacity},function(e){return e.zIndex}),Y=Object(R.c)(G)(c||(c=Object(z.a)(["\n    align-items: center;\n    justify-content: center;\n    border-radius: 50%;\n    overflow: hidden;\n    width: ","px;\n    height: ","px;\n    background-color: ",";\n    margin: ","px;\n"])),function(e){return e.width},function(e){return e.height},function(e){return e.bg},function(e){return e.margin?e.margin:0}),J=function(e){var t=e.image,a=e.size,n=e.flex,i=e.margin,r=e.bg,o=e.of,s=e.style,l=e.scale,c=void 0===l?1:l;return j.a.createElement(Y,{style:s,bg:r,margin:i,width:a,height:a,flex:n},j.a.createElement(K,{of:o,width:a+"px",height:a+"px",style:{transform:"scale(".concat(c,")")},src:t}))},q=R.c.input(m||(m=Object(z.a)(["\n    ",";\n    width: ",";\n    height: ",";\n    background-color: ",";\n    ",";\n    border-radius: ",";\n    box-sizing: border-box;\n    flex: ",";\n    z-index: ",";\n    font-weight: ",";\n    font-size: ","px;\n    line-height: ","px;\n    color: ",";\n    border: 0;\n    color: ",";\n    transition: all 300ms ease;\n    ::placeholder {\n        color: ",";\n    };\n"])),function(e){return e.margin},function(e){return e.width},function(e){return e.height},function(e){return e.bg},function(e){return e.padding},function(e){return e.circle?"50%":e.br},function(e){return e.flex},function(e){return e.zIndex},function(e){return e.weight},function(e){return e.size?e.size:15},function(e){return e.lh},function(e){return e.color},function(e){return e.color?e.color:B().darkMode?"white":"black"},function(e){return e.placeholderTextColor}),X=R.c.textarea(d||(d=Object(z.a)(["\n    ",";\n    width: ",";\n    height: ",";\n    background-color: ",";\n    ",";\n    border-radius: ",";\n    box-sizing: border-box;\n    flex: ",";\n    z-index: ",";\n    font-weight: ",";\n    font-size: ","px;\n    line-height: ","px;\n    color: ",";\n    border: 0;\n    color: ",";\n    transition: all 300ms ease;\n    ::placeholder {\n        color: ",";\n    };\n    resize: none;\n    ::-webkit-scrollbar {\n        display: none;\n    }\n"])),function(e){return e.margin},function(e){return e.width},function(e){return e.height},function(e){return e.bg},function(e){return e.padding},function(e){return e.circle?"50%":e.br},function(e){return e.flex},function(e){return e.zIndex},function(e){return e.weight},function(e){return e.size?e.size:15},function(e){return e.lh},function(e){return e.color},function(e){return e.color?e.color:B().darkMode?"white":"black"},function(e){return e.placeholderTextColor}),Q=function(e){var t=e.width,a=void 0===t?99:t,n=e.height,i=void 0===n?134:n;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 99 134",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{d:"M6.04034 84.8608L49.5 67.9367L92.9597 84.8608L49.5 103.121L6.04034 84.8608Z",stroke:"#9E35AD",strokeWidth:"5"}),j.a.createElement("path",{d:"M6.04034 65.8608L49.5 48.9367L92.9597 65.8608L49.5 84.121L6.04034 65.8608Z",stroke:"#25A2F9",strokeWidth:"5"}),j.a.createElement("path",{d:"M6.04034 49.5704L49.5 32.6463L92.9597 49.5704L49.5 67.8306L6.04034 49.5704Z",stroke:"#ED2166",strokeWidth:"5"}))},$=function(e){var t=e.width,a=void 0===t?92:t,n=e.height,i=void 0===n?126:n,r=e.stroke1,o=void 0===r?"#25A2F9":r,s=e.stroke2,l=void 0===s?"#9C15B0":s;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 92 126",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("line",{x1:"14.4061",y1:"13.0862",x2:"77.4061",y2:"41.0862",stroke:o,strokeWidth:"3"}),j.a.createElement("line",{x1:"13.6838",y1:"62.0513",x2:"76.6838",y2:"41.0513",stroke:o,strokeWidth:"3"}),j.a.createElement("line",{x1:"14.7385",y1:"13.3257",x2:"77.7385",y2:"82.3257",stroke:o,strokeWidth:"3"}),j.a.createElement("line",{x1:"13.5939",y1:"110.086",x2:"76.5939",y2:"82.0862",stroke:o,strokeWidth:"3"}),j.a.createElement("line",{x1:"14.3026",y1:"62.0469",x2:"77.3026",y2:"82.0469",stroke:o,strokeWidth:"3"}),j.a.createElement("line",{x1:"13.2615",y1:"110.326",x2:"76.2615",y2:"41.3257",stroke:o,strokeWidth:"3"}),j.a.createElement("circle",{cx:"14.0059",cy:"14.0059",r:"8.1",transform:"rotate(-52.9579 14.0059 14.0059)",fill:l,stroke:l,strokeWidth:"3.8"}),j.a.createElement("circle",{cx:"14.0059",cy:"111.006",r:"8.1",transform:"rotate(-52.9579 14.0059 111.006)",fill:l,stroke:l,strokeWidth:"3.8"}),j.a.createElement("circle",{cx:"77.006",cy:"42.0059",r:"8.1",transform:"rotate(-52.9579 77.006 42.0059)",fill:l,stroke:l,strokeWidth:"3.8"}),j.a.createElement("circle",{cx:"77.006",cy:"83.006",r:"8.1",transform:"rotate(-52.9579 77.006 83.006)",fill:l,stroke:l,strokeWidth:"3.8"}),j.a.createElement("circle",{cx:"14.0059",cy:"63.0059",r:"8.1",transform:"rotate(-52.9579 14.0059 63.0059)",fill:l,stroke:l,strokeWidth:"3.8"}))},ee=function(e){var t=e.width,a=void 0===t?104:t,n=e.height,i=void 0===n?91:n,r=e.stroke1,o=void 0===r?"#25A2F9":r,s=e.stroke2,l=void 0===s?"#ED2166":s;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 104 91",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("rect",{x:"2.5",y:"2.5",width:"74",height:"55.7931",stroke:o,strokeWidth:"5"}),j.a.createElement("rect",{x:"27.5",y:"32.5",width:"74",height:"55.7931",stroke:l,strokeWidth:"5"}))},te=function(e){var t=e.width,a=void 0===t?477:t,n=e.height,i=void 0===n?241:n,r=e.stroke1,o=void 0===r?"#25A2F9":r,s=e.stroke2,l=void 0===s?"#ED2166":s,c=e.stroke3,m=void 0===c?"#9C15B0":c;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 477 241",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("line",{x1:"297.144",y1:"38.4917",x2:"223.003",y2:"205.308",stroke:o,strokeWidth:"8"}),j.a.createElement("line",{x1:"167.52",y1:"35.9634",x2:"223.126",y2:"202.78",stroke:o,strokeWidth:"8"}),j.a.createElement("line",{x1:"296.306",y1:"39.6549",x2:"113.602",y2:"206.471",stroke:o,strokeWidth:"8"}),j.a.createElement("line",{x1:"40.3",y1:"35.6487",x2:"114.441",y2:"202.465",stroke:o,strokeWidth:"8"}),j.a.createElement("line",{x1:"167.536",y1:"38.1292",x2:"114.578",y2:"204.945",stroke:o,strokeWidth:"8"}),j.a.createElement("line",{x1:"39.4616",y1:"34.4855",x2:"222.165",y2:"201.302",stroke:o,strokeWidth:"8"}),j.a.createElement("line",{x1:"455.019",y1:"100.8",x2:"358.204",y2:"214.857",stroke:l,strokeWidth:"8"}),j.a.createElement("line",{x1:"358.265",y1:"96.3451",x2:"449.776",y2:"210.401",stroke:l,strokeWidth:"8"}),j.a.createElement("circle",{cx:"355.592",cy:"98.5924",r:"15.0585",transform:"rotate(37.0421 355.592 98.5924)",fill:m,stroke:m,strokeWidth:"6"}),j.a.createElement("circle",{cx:"355.592",cy:"212.649",r:"15.0585",transform:"rotate(37.0421 355.592 212.649)",fill:m,stroke:m,strokeWidth:"6"}),j.a.createElement("circle",{cx:"452.408",cy:"212.649",r:"15.0585",transform:"rotate(37.0421 452.408 212.649)",fill:m,stroke:m,strokeWidth:"6"}),j.a.createElement("circle",{cx:"452.408",cy:"98.5924",r:"15.0585",transform:"rotate(37.0421 452.408 98.5924)",fill:m,stroke:m,strokeWidth:"6"}),j.a.createElement("circle",{cx:"293.93",cy:"37.086",r:"23.9788",transform:"rotate(37.0421 293.93 37.086)",fill:m,stroke:m,strokeWidth:"6"}),j.a.createElement("circle",{cx:"37.0859",cy:"37.086",r:"23.9788",transform:"rotate(37.0421 37.0859 37.086)",fill:m,stroke:m,strokeWidth:"6"}),j.a.createElement("circle",{cx:"219.789",cy:"203.902",r:"23.9788",transform:"rotate(37.0421 219.789 203.902)",fill:m,stroke:m,strokeWidth:"6"}),j.a.createElement("circle",{cx:"111.227",cy:"203.902",r:"23.9788",transform:"rotate(37.0421 111.227 203.902)",fill:m,stroke:m,strokeWidth:"6"}),j.a.createElement("circle",{cx:"164.184",cy:"37.086",r:"23.9788",transform:"rotate(37.0421 164.184 37.086)",fill:m,stroke:m,strokeWidth:"6"}))},ae=function(e){var t=e.width,a=void 0===t?332:t,n=e.height,i=void 0===n?241:n,r=e.stroke1,o=void 0===r?"#25A2F9":r,s=e.stroke2,l=void 0===s?"#9C15B0":s;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 332 241",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("line",{x1:"297.144",y1:"38.4917",x2:"223.003",y2:"205.308",stroke:o,strokeWidth:"7"}),j.a.createElement("line",{x1:"167.52",y1:"35.9634",x2:"223.125",y2:"202.78",stroke:o,strokeWidth:"7"}),j.a.createElement("line",{x1:"296.305",y1:"39.6549",x2:"113.602",y2:"206.471",stroke:o,strokeWidth:"7"}),j.a.createElement("line",{x1:"40.3001",y1:"35.6487",x2:"114.441",y2:"202.465",stroke:o,strokeWidth:"7"}),j.a.createElement("line",{x1:"167.536",y1:"38.1292",x2:"114.578",y2:"204.945",stroke:o,strokeWidth:"7"}),j.a.createElement("line",{x1:"39.4617",y1:"34.4855",x2:"222.165",y2:"201.302",stroke:o,strokeWidth:"7"}),j.a.createElement("circle",{cx:"293.93",cy:"37.086",r:"23.9788",transform:"rotate(37.0421 293.93 37.086)",fill:l,stroke:l,strokeWidth:"5"}),j.a.createElement("circle",{cx:"37.0859",cy:"37.086",r:"23.9788",transform:"rotate(37.0421 37.0859 37.086)",fill:l,stroke:l,strokeWidth:"5"}),j.a.createElement("circle",{cx:"219.789",cy:"203.902",r:"23.9788",transform:"rotate(37.0421 219.789 203.902)",fill:l,stroke:l,strokeWidth:"5"}),j.a.createElement("circle",{cx:"111.227",cy:"203.902",r:"23.9788",transform:"rotate(37.0421 111.227 203.902)",fill:l,stroke:l,strokeWidth:"5"}),j.a.createElement("circle",{cx:"164.184",cy:"37.086",r:"23.9788",transform:"rotate(37.0421 164.184 37.086)",fill:l,stroke:l,strokeWidth:"5"}))},ne=function(e){var t=e.width,a=void 0===t?539:t,n=e.height,i=void 0===n?539:n,r=e.bg,o=e.stroke;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 539 539",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("circle",{cx:"269.5",cy:"269.5",r:"269.5",fill:r}),j.a.createElement("path",{d:"M132.5 150L393 410.5",stroke:o,strokeWidth:"40"}),j.a.createElement("path",{d:"M393 150L132.5 410.5",stroke:o,strokeWidth:"40"}))},ie=function(e){var t=e.width,a=void 0===t?108:t,n=e.height,i=void 0===n?46:n,r=e.stroke,o=void 0===r?"white":r;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 108 46",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{d:"M3 4L54 41L105 4",stroke:o,strokeWidth:"8"}))},re=function(e){var t=e.width,a=void 0===t?490:t,n=e.height,i=void 0===n?506:n,r=e.stroke,o=void 0===r?"black":r;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 490 506",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("circle",{cx:"188",cy:"188",r:"173",stroke:o,strokeWidth:"40"}),j.a.createElement("path",{d:"M303.5 320L475 491.5",stroke:o,strokeWidth:"40"}))},oe=function(e){var t=e.width,a=void 0===t?512:t,n=e.height,i=void 0===n?480:n,r=e.stroke,o=void 0===r?"black":r;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 512 480",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{d:"M512 243.2C512 209.6 484.8 192 448 192H340.8C348.8 163.2 352 136 352 112C352 19.2 326.4 0 304 0C289.6 0 278.4 1.6 264 9.6C259.2 12.8 257.6 16 256 20.8L240 107.2C222.4 152 179.2 192 144 219.2V448C156.8 448 169.6 454.4 185.6 462.4C203.2 470.4 220.8 480 240 480H392C424 480 448 454.4 448 432C448 427.2 448 424 446.4 420.8C465.6 412.8 480 396.8 480 376C480 366.4 478.4 358.4 475.2 350.4C488 342.4 499.2 328 499.2 312C499.2 302.4 494.4 292.8 489.6 284.8C502.4 275.2 512 259.2 512 243.2ZM478.4 243.2C478.4 264 457.6 265.6 454.4 275.2C451.2 286.4 467.2 289.6 467.2 308.8C467.2 328 443.2 328 440 339.2C436.8 352 448 355.2 448 374.4V377.6C444.8 393.6 420.8 395.2 416 401.6C411.2 409.6 416 412.8 416 430.4C416 440 404.8 446.4 392 446.4H240C227.2 446.4 214.4 440 198.4 432C185.6 425.6 172.8 419.2 160 416V248C200 217.6 251.2 172.8 270.4 116.8V113.6L284.8 33.6C291.2 32 296 32 304 32C307.2 32 320 51.2 320 112C320 136 315.2 161.6 307.2 192H304C294.4 192 288 198.4 288 208C288 217.6 294.4 224 304 224H448C464 224 478.4 232 478.4 243.2Z",fill:o}),j.a.createElement("path",{d:"M128 480H32C14.4 480 0 465.6 0 448V224C0 206.4 14.4 192 32 192H128C145.6 192 160 206.4 160 224V448C160 465.6 145.6 480 128 480ZM32 224V448H128V224H32Z",fill:o}))},se=function(e){var t=e.width,a=void 0===t?980:t,n=e.height,i=void 0===n?940:n,r=e.stroke,o=void 0===r?"black":r;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 980 940",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{d:"M710.5 445.3H269.5C254.8 445.3 245 455.2 245 470C245 484.8 254.8 494.7 269.5 494.7H710.5C725.2 494.7 735 484.8 735 470C735 455.2 725.2 445.3 710.5 445.3ZM882 0.899994H98C44.1 0.899994 0 45.3 0 99.7V692.3C0 746.6 44.1 791.1 98 791.1H588L725.2 929.4C732.6 936.8 739.9 939.3 749.7 939.3C766.9 939.3 784 924.5 784 904.7V791H882C935.9 791 980 746.6 980 692.2V99.6C980 45.3 935.9 0.899994 882 0.899994ZM931 692.2C931 719.4 908.9 741.6 882 741.6H784H735V791V870L622.3 756.4L607.6 741.6H588H98C71 741.6 49 719.4 49 692.2V99.6C49 72.4 71 50.2 98 50.2H882C908.9 50.2 931 72.4 931 99.6V692.2ZM710.5 297.2H269.5C254.8 297.2 245 307.1 245 321.9C245 336.7 254.8 346.6 269.5 346.6H710.5C725.2 346.6 735 336.7 735 321.9C735 307 725.2 297.2 710.5 297.2Z",fill:o}))},le=function(e){var t=e.width,a=void 0===t?876:t,n=e.height,i=void 0===n?840:n,r=e.stroke,o=void 0===r?"black":r;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 876 840",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{d:"M844.6 300.791C829.017 300.791 815.66 314.148 815.66 329.731V781.626H58.7921V329.73C58.7921 314.148 45.4361 300.791 29.8531 300.791C14.2701 300.791 0.914062 314.148 0.914062 329.731V810.565C0.914062 826.148 14.2711 839.505 29.8541 839.505H846.827C862.41 839.505 875.767 826.148 875.767 810.565V329.73C873.54 314.148 860.184 300.791 844.601 300.791H844.6Z",fill:o}),j.a.createElement("path",{d:"M261.365 227.33C268.043 227.33 276.948 225.104 281.4 218.426L408.287 95.991V407.643C408.287 423.226 421.643 436.583 437.227 436.583C452.809 436.583 466.165 423.226 466.165 407.643V95.991L593.052 218.426C599.73 222.878 606.409 227.33 613.087 227.33C619.765 227.33 628.67 225.104 633.122 218.426C635.767 215.803 637.866 212.682 639.299 209.243C640.732 205.805 641.47 202.116 641.47 198.391C641.47 194.666 640.732 190.977 639.299 187.539C637.866 184.1 635.767 180.979 633.122 178.356L457.26 6.94899C452.808 2.49699 446.13 0.270996 439.451 0.270996H437.225H435C428.322 0.270996 421.643 2.49699 417.191 6.94899L241.331 176.13C238.686 178.753 236.586 181.874 235.153 185.313C233.721 188.751 232.983 192.44 232.983 196.165C232.983 199.89 233.721 203.579 235.153 207.017C236.586 210.456 238.686 213.577 241.331 216.2C245.783 225.104 254.687 227.33 261.365 227.33Z",fill:o}))},ce=function(e){var t=e.width,a=void 0===t?980:t,n=e.height,i=void 0===n?980:n,r=e.stroke,o=void 0===r?"black":r;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 980 980",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{d:"M848.9 191.6L665.6 9C653.6 -3 634.2 -3 622.2 9C610.2 21 610.2 40.4 622.2 52.4L753.2 183.4L30.6 183C13.7 183 0 196.7 0 213.6V458.6C0 475.5 13.7 489.2 30.6 489.2C47.5 489.2 61.2 475.5 61.2 458.6V244.5H752.6L622.6 374.6C610.6 386.6 610.6 406 622.6 418C634.6 430 654 430 666 418L848.9 235C860.8 222.9 860.8 203.6 848.9 191.6Z",fill:o}),j.a.createElement("path",{d:"M131.1 788.4L314.3 971C326.3 983 345.7 983 357.7 971C369.7 959 369.7 939.6 357.7 927.6L226.7 796.7L949.4 797C966.3 797 980 783.3 980 766.4V521.4C980 504.5 966.3 490.8 949.4 490.8C932.5 490.8 918.8 504.5 918.8 521.4V735.5H227.4L357.5 605.4C369.5 593.4 369.5 574 357.5 562C345.5 550 326.1 550 314.1 562L131.1 745C119.2 757.1 119.2 776.4 131.1 788.4Z",fill:o}))},me=function(e){var t=e.width,a=void 0===t?262:t,n=e.height,i=void 0===n?308:n,r=e.stroke,o=void 0===r?"black":r;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 262 308",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{d:"M218.264 200.541C218.264 203.232 216.063 205.436 213.371 205.436H48.2319C45.5389 205.436 43.3369 203.233 43.3369 200.541V197.28C43.3369 194.589 45.5379 192.386 48.2319 192.386H213.371C216.062 192.386 218.264 194.588 218.264 197.28V200.541Z",fill:o}),j.a.createElement("path",{d:"M117.293 64.99C117.293 67.68 115.092 69.883 112.398 69.883H48.2319C45.5389 69.883 43.3369 67.681 43.3369 64.99V61.727C43.3369 59.036 45.5379 56.833 48.2319 56.833H112.398C115.091 56.833 117.293 59.035 117.293 61.727V64.99V64.99Z",fill:o}),j.a.createElement("path",{d:"M215.787 245.727C215.787 248.417 213.584 250.619 210.89 250.619H146.724C144.031 250.619 141.829 248.417 141.829 245.727V242.464C141.829 239.773 144.03 237.57 146.724 237.57H210.89C213.583 237.57 215.787 239.772 215.787 242.464V245.727Z",fill:o}),j.a.createElement("path",{d:"M151.312 110.172C151.312 112.863 149.111 115.067 146.419 115.067H48.2309C45.5379 115.067 43.3359 112.864 43.3359 110.172V106.911C43.3359 104.22 45.5369 102.018 48.2309 102.018H146.419C149.11 102.018 151.312 104.22 151.312 106.911V110.172Z",fill:o}),j.a.createElement("path",{d:"M218.264 155.357C218.264 158.048 216.063 160.25 213.371 160.25H48.2319C45.5389 160.25 43.3369 158.048 43.3369 155.357V152.095C43.3369 149.404 45.5379 147.201 48.2319 147.201H213.371C216.062 147.201 218.264 149.403 218.264 152.095V155.357Z",fill:o}),j.a.createElement("path",{d:"M249.475 74.521L170.352 9.454C163.801 4.062 152.463 0 143.981 0H19.516C8.756 0 0 8.754 0 19.516C0 19.516 0 229.54 0 287.355C0 307.454 19.01 307.454 19.463 307.454C67.803 307.454 242.086 307.454 242.086 307.454C252.848 307.454 261.602 298.699 261.602 287.937V100.198C261.602 91.247 256.387 80.207 249.475 74.521ZM164.941 29.475C164.941 24.694 168.9 28.362 168.9 28.362L231.617 81.947C231.617 81.947 235.631 85.896 228.752 85.896C214.064 85.896 170.006 85.896 170.006 85.896C167.213 85.896 164.941 83.625 164.941 80.832C164.941 80.832 164.941 42.315 164.941 29.475ZM242.086 292.999C242.086 292.999 63.031 292.999 18.096 292.999C17.295 292.999 14.453 292.77 14.453 288.817C14.453 234.41 14.453 19.515 14.453 19.515C14.453 16.77 16.773 14.452 19.516 14.452H143.98C146.087 14.452 150.49 15.57 150.49 21.59V80.832C150.49 91.593 159.244 100.348 170.006 100.348H243.529C244.871 100.348 247.142 101.27 247.142 104.517C247.142 104.568 247.15 287.936 247.15 287.936C247.15 290.729 244.879 292.999 242.086 292.999Z",fill:o}))},de=function(e){var t=e.width,a=void 0===t?262:t,n=e.height,i=void 0===n?308:n,r=e.stroke;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 262 308",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{d:"M249.475 74.521L170.352 9.454C163.801 4.062 152.463 0 143.981 0H19.516C8.756 0 0 8.754 0 19.516C0 19.516 0 229.54 0 287.355C0 307.454 19.01 307.454 19.463 307.454C67.803 307.454 242.086 307.454 242.086 307.454C252.848 307.454 261.602 298.699 261.602 287.937V100.198C261.602 91.247 256.387 80.207 249.475 74.521ZM164.941 29.475C164.941 24.694 168.9 28.362 168.9 28.362L231.617 81.947C231.617 81.947 235.631 85.896 228.752 85.896C214.064 85.896 170.006 85.896 170.006 85.896C167.213 85.896 164.941 83.625 164.941 80.832C164.941 80.832 164.941 42.315 164.941 29.475ZM242.086 292.999C242.086 292.999 63.031 292.999 18.096 292.999C17.295 292.999 14.453 292.77 14.453 288.817C14.453 234.41 14.453 19.515 14.453 19.515C14.453 16.77 16.773 14.452 19.516 14.452H143.98C146.087 14.452 150.49 15.57 150.49 21.59V80.832C150.49 91.593 159.244 100.348 170.006 100.348H243.529C244.871 100.348 247.142 101.27 247.142 104.517C247.142 104.568 247.15 287.936 247.15 287.936C247.15 290.729 244.879 292.999 242.086 292.999Z",fill:r}),j.a.createElement("path",{d:"M124.5 166L127.554 171.192C132.141 178.99 138.195 185.826 145.382 191.322L172.601 212.136C175.329 214.222 178.953 214.719 182.142 213.443V213.443C191.475 209.71 189.732 196.004 179.761 194.726L159.917 192.182C158.975 192.061 158.022 192.03 157.074 192.09L148.829 192.612C129.358 193.844 110.657 200.673 94.9723 212.277L58.0689 239.579C57.6919 239.858 57.3606 240.194 57.0869 240.575V240.575C54.3083 244.441 58.6241 249.421 62.8461 247.221L79.8839 238.342C80.9507 237.786 81.8965 237.024 82.6666 236.1L87.4117 230.406C91.7504 225.199 94.887 219.1 96.5977 212.542L103 188L129.538 116.443C130.488 113.88 130.608 111.082 129.879 108.447L128.497 103.452C126.624 96.6781 118.38 94.0999 112.981 98.5994V98.5994C110.775 100.437 109.5 103.16 109.5 106.031V123.039C109.5 129.604 110.905 136.092 113.622 142.068L124.5 166Z",stroke:r,strokeWidth:"8"}))},pe=function(e){var t=e.width,a=void 0===t?523:t,n=e.height,i=void 0===n?393:n,r=e.stroke,o=void 0===r?"black":r,s=e.stroke1,l=e.stroke2,c=e.stroke3;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 523 393",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{d:"M325.762 5.51303L308.056 0.659023C305.777 -0.100977 303.532 0.138027 301.349 1.37403C299.159 2.61103 297.68 4.46803 296.92 6.94203L190.426 375.53C189.666 378.005 189.904 380.339 191.141 382.525C192.378 384.715 194.231 386.19 196.709 386.95L214.41 391.806C216.694 392.572 218.931 392.332 221.12 391.094C223.31 389.851 224.786 388 225.545 385.53L332.042 16.936C332.801 14.462 332.565 12.128 331.326 9.93702C330.088 7.74702 328.237 6.27203 325.762 5.51303Z",fill:s||o}),j.a.createElement("path",{d:"M166.167 77.465C166.167 74.991 165.214 72.8 163.311 70.898L149.034 56.622C147.131 54.719 144.941 53.765 142.467 53.765C139.993 53.765 137.802 54.72 135.9 56.622L2.856 189.666C0.95 191.569 0 193.759 0 196.233C0 198.707 0.953 200.897 2.856 202.799L135.899 335.843C137.801 337.749 139.988 338.697 142.466 338.697C144.944 338.697 147.131 337.746 149.033 335.843L163.31 321.575C165.213 319.673 166.166 317.482 166.166 315.005C166.166 312.534 165.213 310.344 163.31 308.442L51.107 196.233L163.311 84.032C165.217 82.13 166.167 79.939 166.167 77.465Z",fill:l||o}),j.a.createElement("path",{d:"M519.615 189.663L386.567 56.619C384.665 54.717 382.474 53.762 380.004 53.762C377.526 53.762 375.343 54.717 373.434 56.619L359.163 70.894C357.261 72.797 356.312 74.984 356.312 77.461C356.312 79.938 357.26 82.126 359.163 84.028L471.369 196.232L359.163 308.442C357.261 310.344 356.312 312.535 356.312 315.005C356.312 317.483 357.26 319.673 359.163 321.575L373.434 335.843C375.343 337.749 377.527 338.697 380.004 338.697C382.475 338.697 384.665 337.746 386.567 335.843L519.615 202.8C521.518 200.898 522.469 198.704 522.469 196.23C522.469 193.755 521.518 191.565 519.615 189.663Z",fill:c||o}))},he=function(e){var t=e.width,a=void 0===t?405:t,n=e.height,i=void 0===n?354:n,r=e.stroke,o=void 0===r?"#00B3DA":r;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 405 354",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("line",{x1:"20.9999",y1:"0.0636951",x2:"19.9999",y2:"314.064",stroke:o,strokeWidth:"30"}),j.a.createElement("line",{x1:"405",y1:"334",y2:"334",stroke:o,strokeWidth:"30"}),j.a.createElement("path",{d:"M20 212L84.7776 280.771C89.0577 285.315 96.4088 284.87 100.11 279.843L179.078 172.582C182.881 167.416 190.495 167.116 194.692 171.967L243.957 228.895C248.686 234.359 257.471 233.158 260.559 226.625L362.5 11",stroke:o,strokeWidth:"30"}))},ge=function(e){var t=e.width,a=void 0===t?157:t,n=e.height,i=void 0===n?157:n,r=e.stroke,o=void 0===r?"#00B3DA":r;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 157 157",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{d:"M61 104.851V52.6166C61 51.0441 62.7313 50.0867 64.0632 50.9227L105.324 76.8202C106.567 77.6007 106.574 79.4102 105.337 80.2L64.0761 106.537C62.7447 107.386 61 106.43 61 104.851Z",stroke:o,strokeWidth:"10"}),j.a.createElement("circle",{cx:"78.5",cy:"78.5",r:"73.5",stroke:o,strokeWidth:"10"}))},ue=function(e){var t=e.width,a=void 0===t?262:t,n=e.height,i=void 0===n?308:n,r=e.stroke,o=void 0===r?"#00B3DA":r;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 262 308",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{d:"M117.293 64.99C117.293 67.68 115.092 69.883 112.398 69.883H48.2319C45.5389 69.883 43.3369 67.681 43.3369 64.99V61.727C43.3369 59.036 45.5379 56.833 48.2319 56.833H112.398C115.091 56.833 117.293 59.035 117.293 61.727V64.99V64.99Z",fill:o}),j.a.createElement("path",{d:"M151.312 110.172C151.312 112.863 149.111 115.067 146.419 115.067H48.2309C45.5379 115.067 43.3359 112.864 43.3359 110.172V106.911C43.3359 104.22 45.5369 102.018 48.2309 102.018H146.419C149.11 102.018 151.312 104.22 151.312 106.911V110.172Z",fill:o}),j.a.createElement("path",{d:"M249.475 74.521L170.352 9.454C163.801 4.062 152.463 0 143.981 0H19.516C8.756 0 0 8.754 0 19.516C0 19.516 0 229.54 0 287.355C0 307.454 19.01 307.454 19.463 307.454C67.803 307.454 242.086 307.454 242.086 307.454C252.848 307.454 261.602 298.699 261.602 287.937V100.198C261.602 91.247 256.387 80.207 249.475 74.521ZM164.941 29.475C164.941 24.694 168.9 28.362 168.9 28.362L231.617 81.947C231.617 81.947 235.631 85.896 228.752 85.896C214.064 85.896 170.006 85.896 170.006 85.896C167.213 85.896 164.941 83.625 164.941 80.832C164.941 80.832 164.941 42.315 164.941 29.475ZM242.086 292.999C242.086 292.999 63.031 292.999 18.096 292.999C17.295 292.999 14.453 292.77 14.453 288.817C14.453 234.41 14.453 19.515 14.453 19.515C14.453 16.77 16.773 14.452 19.516 14.452H143.98C146.087 14.452 150.49 15.57 150.49 21.59V80.832C150.49 91.593 159.244 100.348 170.006 100.348H243.529C244.871 100.348 247.142 101.27 247.142 104.517C247.142 104.568 247.15 287.936 247.15 287.936C247.15 290.729 244.879 292.999 242.086 292.999Z",fill:o}),j.a.createElement("path",{d:"M153.596 148.551L147.259 147.127C146.443 146.904 145.64 146.974 144.858 147.337C144.075 147.7 143.545 148.245 143.273 148.97L105.157 257.097C104.885 257.823 104.97 258.508 105.413 259.149C105.856 259.791 106.519 260.224 107.406 260.447L113.742 261.872C114.559 262.096 115.36 262.026 116.143 261.663C116.927 261.298 117.455 260.755 117.727 260.03L155.844 151.902C156.116 151.176 156.031 150.492 155.588 149.849C155.145 149.207 154.482 148.774 153.596 148.551Z",fill:o}),j.a.createElement("path",{d:"M96.4739 169.659C96.4739 168.933 96.1328 168.29 95.4517 167.732L90.3417 163.544C89.6606 162.986 88.8768 162.706 87.9913 162.706C87.1058 162.706 86.3216 162.986 85.6408 163.544L38.0222 202.573C37.34 203.131 37 203.774 37 204.5C37 205.225 37.3411 205.868 38.0222 206.426L85.6405 245.455C86.3212 246.014 87.104 246.292 87.9909 246.292C88.8778 246.292 89.6606 246.013 90.3414 245.455L95.4513 241.269C96.1325 240.711 96.4735 240.068 96.4735 239.342C96.4735 238.617 96.1325 237.974 95.4513 237.416L55.292 204.5L95.4517 171.585C96.1339 171.027 96.4739 170.384 96.4739 169.659Z",fill:o}),j.a.createElement("path",{d:"M222.979 202.572L175.359 163.543C174.678 162.985 173.894 162.705 173.01 162.705C172.123 162.705 171.342 162.985 170.659 163.543L165.551 167.731C164.87 168.289 164.53 168.931 164.53 169.657C164.53 170.384 164.87 171.026 165.551 171.584L205.711 204.499L165.551 237.416C164.87 237.974 164.53 238.617 164.53 239.342C164.53 240.069 164.87 240.711 165.551 241.269L170.659 245.455C171.342 246.014 172.123 246.292 173.01 246.292C173.894 246.292 174.678 246.013 175.359 245.455L222.979 206.426C223.66 205.868 224 205.224 224 204.499C224 203.773 223.66 203.13 222.979 202.572Z",fill:o}))},fe=function(e){var t=e.width,a=void 0===t?58:t,n=e.height,i=void 0===n?104:n,r=e.stroke;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 58 104",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{d:"M54.5 3.5L6 52L54.5 100.5",stroke:r,strokeWidth:"15"}))},be=function(e){var t=e.width,a=void 0===t?1024:t,n=e.height,i=void 0===n?1024:n,r=e.stroke,o=void 0===r?"white":r;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 1024 1024",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{fillRule:"evenodd",clipRule:"evenodd",d:"M8 0C3.58 0 0 3.58 0 8C0 11.54 2.29 14.53 5.47 15.59C5.87 15.66 6.02 15.42 6.02 15.21C6.02 15.02 6.01 14.39 6.01 13.72C4 14.09 3.48 13.23 3.32 12.78C3.23 12.55 2.84 11.84 2.5 11.65C2.22 11.5 1.82 11.13 2.49 11.12C3.12 11.11 3.57 11.7 3.72 11.94C4.44 13.15 5.59 12.81 6.05 12.6C6.12 12.08 6.33 11.73 6.56 11.53C4.78 11.33 2.92 10.64 2.92 7.58C2.92 6.71 3.23 5.99 3.74 5.43C3.66 5.23 3.38 4.41 3.82 3.31C3.82 3.31 4.49 3.1 6.02 4.13C6.66 3.95 7.34 3.86 8.02 3.86C8.7 3.86 9.38 3.95 10.02 4.13C11.55 3.09 12.22 3.31 12.22 3.31C12.66 4.41 12.38 5.23 12.3 5.43C12.81 5.99 13.12 6.7 13.12 7.58C13.12 10.65 11.25 11.33 9.47 11.53C9.76 11.78 10.01 12.26 10.01 13.01C10.01 14.08 10 14.94 10 15.21C10 15.42 10.15 15.67 10.55 15.59C13.71 14.53 16 11.53 16 8C16 3.58 12.42 0 8 0Z",transform:"scale(64)",fill:o}))},we=function(e){var t=e.width,a=void 0===t?1e3:t,n=e.height,i=void 0===n?1e3:n,r=e.stroke,o=void 0===r?"#000000":r;return j.a.createElement("svg",{role:"img",xmlns:"http://www.w3.org/2000/svg",width:a,height:i,viewBox:"0 0 1000 1000"},j.a.createElement("path",{id:"path",fill:o,d:" M 665 375C 633 375 602 388 579 411C 556 434 543 465 543 498C 543 530 556 561 579 584C 602 607 633 620 665 620C 698 620 729 607 752 584C 775 561 788 530 788 498C 788 430 733 375 665 375C 665 375 665 375 665 375M 338 261C 390 261 440 278 481 310C 481 310 424 411 424 411C 401 388 370 375 338 375C 305 375 274 388 251 411C 228 434 215 465 215 498C 215 530 228 561 251 584C 274 607 305 620 338 620C 369 620 399 608 422 587C 422 587 480 686 480 686C 439 717 389 734 338 734C 275 734 215 709 171 665C 126 620 101 560 101 498C 101 367 207 261 338 261C 338 261 338 261 338 261M 665 261C 796 261 902 367 902 498C 902 628 796 734 665 734C 603 734 543 709 498 665C 454 620 429 560 429 498C 429 367 535 261 665 261C 665 261 665 261 665 261",transform:""}))},Ee=function(e){var t=e.width,a=void 0===t?207:t,n=e.height,i=void 0===n?207:n,r=e.stroke,o=void 0===r?"#25A2F9":r;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 207 207",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("circle",{cx:"103.5",cy:"103.5",r:"96",stroke:o,strokeWidth:"15"}),j.a.createElement("path",{d:"M108.303 48.6967C105.374 45.7678 100.626 45.7678 97.6967 48.6967L49.967 96.4264C47.0381 99.3553 47.0381 104.104 49.967 107.033C52.8959 109.962 57.6447 109.962 60.5736 107.033L103 64.6066L145.426 107.033C148.355 109.962 153.104 109.962 156.033 107.033C158.962 104.104 158.962 99.3553 156.033 96.4264L108.303 48.6967ZM110.5 153L110.5 54L95.5 54L95.5 153L110.5 153Z",fill:o}))},ke=(a(69),function(e){var t=e.width,a=void 0===t?24:t,n=e.height,i=void 0===n?24:n;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 24 24",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{stroke:"white",fill:"white",d:"M12 11.807C10.7418 10.5483 9.88488 8.94484 9.53762 7.1993C9.19037 5.45375 9.36832 3.64444 10.049 2C8.10826 2.38205 6.3256 3.33431 4.92899 4.735C1.02399 8.64 1.02399 14.972 4.92899 18.877C8.83499 22.783 15.166 22.782 19.072 18.877C20.4723 17.4805 21.4245 15.6983 21.807 13.758C20.1625 14.4385 18.3533 14.6164 16.6077 14.2692C14.8622 13.9219 13.2588 13.0651 12 11.807V11.807Z"}))}),ve=function(e){var t=e.width,a=void 0===t?24:t,n=e.height,i=void 0===n?24:n;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 24 24",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{stroke:"white",strokeWidth:"1.8",d:"M6.995 12C6.995 14.761 9.241 17.007 12.002 17.007C14.763 17.007 17.009 14.761 17.009 12C17.009 9.239 14.763 6.993 12.002 6.993C9.241 6.993 6.995 9.239 6.995 12ZM11 19H13V22H11V19ZM11 2H13V5H11V2ZM2 11H5V13H2V11ZM19 11H22V13H19V11Z"}),j.a.createElement("path",{stroke:"white",strokeWidth:"1.8",d:"M5.63702 19.778L4.22302 18.364L6.34402 16.243L7.75802 17.657L5.63702 19.778Z"}),j.a.createElement("path",{stroke:"white",strokeWidth:"1.8",d:"M16.242 6.34405L18.364 4.22205L19.778 5.63605L17.656 7.75805L16.242 6.34405Z"}),j.a.createElement("path",{stroke:"white",strokeWidth:"1.8",d:"M6.34402 7.75902L4.22302 5.63702L5.63802 4.22302L7.75802 6.34502L6.34402 7.75902Z"}),j.a.createElement("path",{stroke:"white",strokeWidth:"1.8",d:"M19.778 18.3639L18.364 19.7779L16.242 17.6559L17.656 16.2419L19.778 18.3639Z"}))},xe=function(e){var t=e.darkMode,a=e.setDarkMode;Object(M.useEffect)(function(){!function(e){var t=getComputedStyle(document.body),a=t.getPropertyValue("--black"),n=t.getPropertyValue("--white"),i=document.documentElement;e?(i.style.setProperty("--background",a),i.style.setProperty("--foreground",n),document.querySelector("html").classList.add("darkmode")):(i.style.setProperty("--background",n),i.style.setProperty("--foreground",a),document.querySelector("html").classList.remove("darkmode"))}(t)},[t]);return j.a.createElement("label",{id:"themeChanger",className:"toggle-wrapper",htmlFor:"toggle"},j.a.createElement("div",{className:"toggle ".concat(t?"enabled":"disabled")},j.a.createElement("span",{className:"hidden"},t?"Enable Light Mode":"Enable Dark Mode"),j.a.createElement("div",{className:"icons"},j.a.createElement(ve,null),j.a.createElement(ke,null)),j.a.createElement("input",{id:"toggle",name:"toggle",type:"checkbox",checked:t,onClick:function(){a(!t)}})))},ye=a(9),Ce=a.n(ye),Se=function(e){var t=e.route,a=e.headerShown;return j.a.createElement(_,{align:"center",justify:"space-between",flex:4==t?a?0:3.8:0,style:{opacity:4==t&&a?0:1,transition:"all 400ms ease",overflow:"hidden"}},j.a.createElement(_,{align:"center",style:{minWidth:300}},j.a.createElement(ee,{width:22}),j.a.createElement(G,{to:"cursor",padding:"padding: 10px 15px 10px 15px;"},j.a.createElement(W,{className:"light",size:"12.8"},"STARTING FROM BENCHMARKS"))),j.a.createElement(_,{align:"center"},j.a.createElement(ee,{width:22}),j.a.createElement(G,{to:"cursor",padding:"padding: 10px 15px 10px 15px;"},j.a.createElement(W,{className:"light",size:"12.8"},"COMPONENTS"))),j.a.createElement(_,{align:"center"},j.a.createElement(ee,{width:22}),j.a.createElement(G,{to:"cursor",padding:"padding: 10px 15px 10px 15px;"},j.a.createElement(W,{className:"light",size:"12.8"},"MY MODELS"))))},Ne=function(e){var t=e.route,a=e.headerShown;return j.a.createElement(_,{flex:5==t?a?0:3.8:0,style:{opacity:5==t&&a?0:1,transition:"all 500ms ease",overflow:"hidden"}})},Ie=a(12),Ae=a.n(Ie),Me="#00B3DA",je=function(e){return e?"#9D15B0":"#25A2F9"},Oe=function(e,t){return t?e?function(e){return"rgba(30, 30, 30, ".concat(e,")")}(t):function(e){return"rgba(255, 255, 255, ".concat(e,")")}(t):e?"rgb(30, 30, 30)":"white"},De=function(e){return e?"rgb(15, 15, 15)":"rgb(255, 255, 255)"},Te=Object(R.c)(_)(p||(p=Object(z.a)(["\n    transition: background-color .3s,color .3s,box-shadow .3s ease;\n    box-shadow: 0 0 0.1rem rgb(0 0 0 / 10%), 0 0.15rem 0.9rem rgb(0 0 0 / 10%);\n    border: 0;\n    ::-webkit-scrollbar {\n        display: none;\n        -ms-overflow-style: none;  /* IE and Edge */\n        scrollbar-width: none;\n    }\n"]))),Le=function(e){var t=e.loggedIn,a=void 0!==t&&t,n=e.setRoute,i=e.route,r=Object(M.useState)(!1),o=Object(T.a)(r,2),s=(o[0],o[1],Object(M.useState)(!1)),l=Object(T.a)(s,2),c=l[0],m=l[1],d=Object(M.useState)(!1),p=Object(T.a)(d,2),h=p[0],g=p[1],u=Object(M.useState)(!1),f=Object(T.a)(u,2),b=f[0],w=f[1],E=B(),k=E.darkMode,v=E.setDarkMode,x=Object(M.useState)(!1),y=Object(T.a)(x,2),C=y[0],S=y[1];return j.a.createElement(Te,{position:"fixed",align:"center",justify:"space-between",zIndex:1e3,bg:k?"rgba(15,15,15,0.9)":"rgba(255, 255, 255, 0.9)",style:{width:"100vw",height:80,top:0,backdropFilter:"blur(2px)"}},j.a.createElement(_,{flex:2.2},j.a.createElement("a",{href:"/",onClick:function(){return n(0)}},j.a.createElement(_,{to:"cursor",align:"center",padding:"padding: 30px;",position:"relative",style:{width:300}},j.a.createElement(K,{width:"43px;",src:Ce.a}),j.a.createElement(G,{position:"absolute",style:{left:73,opacity:0==i?1:0,transition:"all 500ms ease"}},j.a.createElement(W,{margin:"margin-left: 15px;",className:"light",weight:"700",size:"18"},"NEURALVERSE")),j.a.createElement(G,{position:"absolute",style:{left:73,opacity:1==i?1:0,transition:"all 500ms ease"}},j.a.createElement(W,{margin:"margin-left: 15px;",className:"light",weight:"700",size:"18"},"NEURAL VERSE")),j.a.createElement(G,{position:"absolute",style:{left:73,opacity:3==i?1:0,transition:"all 500ms ease"}},j.a.createElement(W,{margin:"margin-left: 15px;",className:"light",weight:"700",size:"18"},"NEURAL SPACE")),j.a.createElement(G,{position:"absolute",style:{left:73,opacity:4==i?1:0,transition:"all 500ms ease"}},j.a.createElement(W,{margin:"margin-left: 15px;",className:"light",weight:"700",size:"18"},"NEURAL STUDIO")),j.a.createElement(G,{position:"absolute",style:{left:73,opacity:5==i?1:0,transition:"all 500ms ease"}},j.a.createElement(W,{margin:"margin-left: 15px;",className:"light",weight:"700",size:"18"},"NEURAL OPS")))),(4==i||5==i)&&j.a.createElement(W,{onClick:function(){return S(!C)}},"See Header")),j.a.createElement(Se,{route:i,headerShown:C}),j.a.createElement(Ne,{route:i,headerShown:C}),j.a.createElement(_,{align:"center",justify:"space-between",flex:4==i||5==i?C?4:0:4,height:"100%",style:{opacity:4==i||5==i?C?1:0:1,transition:"all 500ms ease",overflow:"hidden"}},j.a.createElement("a",{onClick:function(){return n(1)}},j.a.createElement(_,{align:"center",hover:h,onMouseOver:function(){return g(!0)},onMouseLeave:function(){return g(!1)}},j.a.createElement(ee,{width:22}),j.a.createElement(G,{to:"cursor",padding:"padding: 10px 0px 10px 15px;"},j.a.createElement(W,{className:"light",size:"12.8",weight:"700"},"VERSE")))),j.a.createElement("a",{onClick:function(){return n(3)}},j.a.createElement(_,{align:"center",hover:c,onMouseOver:function(){return m(!0)},onMouseLeave:function(){return m(!1)}},j.a.createElement(Q,{width:22}),j.a.createElement(G,{to:"cursor",padding:"padding: 10px 0px 10px 15px;"},j.a.createElement(W,{className:"light",size:"12.8",weight:"700"},"SPACE")))),j.a.createElement("a",{onClick:function(){return n(4)}},j.a.createElement(_,{align:"center",hover:b,onMouseOver:function(){return w(!0)},onMouseLeave:function(){return w(!1)}},j.a.createElement(te,{width:30,height:30}),j.a.createElement(G,{to:"cursor",padding:"padding: 10px 0px 10px 15px;"},j.a.createElement(W,{className:"light",size:"12.8",weight:"700"},"STUDIO")))),j.a.createElement("a",{onClick:function(){return n(5)}},j.a.createElement(_,{align:"center",hover:b,onMouseOver:function(){return w(!0)},onMouseLeave:function(){return w(!1)}},j.a.createElement(K,{src:Ce.a,width:30}),j.a.createElement(G,{to:"cursor",padding:"padding: 10px 0px 10px 12.8px;"},j.a.createElement(W,{className:"light",size:"12.8",weight:"700"},"OPS")))),j.a.createElement("a",{onClick:function(){return n(5)}},j.a.createElement(_,{align:"center",hover:b,onMouseOver:function(){return w(!0)},onMouseLeave:function(){return w(!1)}},j.a.createElement(pe,{stroke1:"#ED2166",stroke2:"#25A2F9",stroke3:"#25A2F9",height:15,width:25}),j.a.createElement(G,{to:"cursor",padding:"padding: 10px 0px 10px 12.8px;"},j.a.createElement(W,{className:"light",size:"12.8",weight:"700"},"RESEARCH"))))),j.a.createElement(_,{flex:2,align:"center",justify:"flex-end",padding:"padding-right: 38px;"},j.a.createElement(G,{width:"100px",margin:"margin-right: 30px;"},j.a.createElement(xe,{darkMode:k,setDarkMode:v})),a?j.a.createElement("a",{onClick:function(){return n(7)}},j.a.createElement(_,{align:"center",to:"cursor"},j.a.createElement(J,{margin:15,size:32,image:Ae.a}),j.a.createElement(W,{className:"light",size:"13"},"JONATHAN"))):j.a.createElement(j.a.Fragment,null,j.a.createElement("a",{onClick:function(){return n(5)}},j.a.createElement(G,{margin:"margin-left: 10px;",to:"cursor",width:"80px",height:"25px",justify:"center",align:"center",br:"15px",bw:2,bc:"#25A2F9"},j.a.createElement(W,{className:"light",size:"12",weight:"600"},"LOGIN"))),j.a.createElement("a",{onClick:function(){return n(5)}},j.a.createElement(G,{margin:"margin-left: 30px;",to:"cursor",width:"80px",height:"25px",justify:"center",align:"center",br:"15px",bw:2,bc:"#25A2F9"},j.a.createElement(W,{className:"light",size:"12",weight:"600"},"SIGNUP"))))))},ze=a(11),Re=a.n(ze),Ve=function(e){var t=e.route;return j.a.createElement("div",{style:{display:4==t?"block":"none"}})},Be=a(8),Pe=function(e){var t=e.route;return j.a.createElement("div",{style:{display:5==t?"block":"none"}})},Fe=function(e){var t=e.route;return j.a.createElement("div",{style:{display:6==t?"block":"none"}})},He=a(31),We=a.n(He),Ge=function(e){e.label;var t=Object(M.useState)(!1),a=Object(T.a)(t,2),n=a[0],i=a[1],r=P().setSideBar;return j.a.createElement(G,{onClick:function(){return r(!0)},onMouseOver:function(){return i(!0)},onMouseLeave:function(){return i(!1)},us:"none",to:"cursor",cursorOpaFalse:!0,position:"fixed",style:{width:50,height:50,padding:20,right:"1.5%",bottom:"12%",zIndex:10}},j.a.createElement(G,{align:"center",padding:"padding-left: 20px;",bg:"black",style:{width:n?270:0,opacity:n?1:0,height:50,right:0,position:"absolute",transition:"all 300ms"},br:"25px"},j.a.createElement(W,{style:{width:250,position:"absolute"},color:"white",className:"light"},"MY MODELS & DATASETS")),j.a.createElement(G,{style:{right:0,position:"absolute",zIndex:10}},j.a.createElement(K,{style:{width:50,height:50,transition:"all 300ms",transform:n?"rotate(45deg)":"rotate(0deg)"},src:We.a})))},_e=window,Ue=_e.outerHeight,Ze=window,Ke=(Ze.outerWidth,Ue),Ye=Object(R.c)(U)(h||(h=Object(z.a)(["\n    transition: background-color .3s,color .3s,box-shadow .3s;\n    box-shadow: 0 0 0.1rem rgb(0 0 0 / 10%), 0 0.15rem 0.9rem rgb(0 0 0 / 10%);\n    border: 0;\n    ::-webkit-scrollbar {\n        display: none;\n        -ms-overflow-style: none;  /* IE and Edge */\n        scrollbar-width: none;\n      }\n      \n"]))),Je=function(e){var t=e.darkMode;return j.a.createElement(Ye,{margin:"margin: 10px 0px;",padding:"padding: 15px",style:{flexDirection:"row",height:180},bg:Oe(t),br:"20px;"},j.a.createElement(U,null,j.a.createElement(_,{align:"center"},j.a.createElement(G,{flex:1,style:{maxHeight:100,maxWidth:100,overflow:"hidden"},br:"10px",align:"center",justify:"center"},j.a.createElement(K,{src:Re.a,style:{height:"100%",width:"100%"},of:"cover"})),j.a.createElement(G,{flex:6,margin:"margin-left: 10px;"},j.a.createElement(W,{className:"bold",size:"14"},"JoJoGAN: One Shot Face Stylization"))),j.a.createElement(W,{style:{opacity:.85},size:"12.8",className:"light",weight:"500",margin:"margin: 10px 0px;"},"mchong6/JoJoGAN \u2022  \u2022 arXiv 2021"),j.a.createElement(W,{margin:"margin-bottom: 5px;",size:"12.8",style:{height:50,overflow:"hidden"}},"While there have been recent advances in few-shot image stylization, these methods fail to capture stylistic details that are obvious to humans. Details such as the shape of the eyes, the boldness of the lines, are especially difficult for a model to learn, especially so under a limited data setting. In this work, we aim to perform oneshot image stylization that gets the details right. Given a reference style image, we approximate paired real data using GAN inversion and finetune a pretrained StyleGAN using that approximate paired data. We then encourage the StyleGAN to generalize so that the learned style can be applied to all other images."),j.a.createElement(_,{margin:"margin: 5px 0px;"},j.a.createElement(W,null,"GAN Inversion"),j.a.createElement(W,null,"Image Stylization"))))},qe=Ke-80,Xe=function(e){var t;Object(L.a)(e);var a=Object(M.useState)(!0),n=Object(T.a)(a,2),i=n[0],r=n[1],o=B().darkMode,s=P(),l=s.sideBar,c=s.setSideBar;return j.a.createElement(Ye,{position:"fixed",padding:"padding: 20px;",style:{zIndex:100,right:l?0:-400,width:400,height:qe,transition:"all 300ms ease",top:0,marginTop:80,overflowY:"scroll"},bg:o?"black":"white"},j.a.createElement(G,{position:"absolute",onClick:function(){return c(!1)},style:{right:20,top:20},to:"cursor"},j.a.createElement(ne,{bg:Oe(o),stroke:"white",width:23,height:23})),j.a.createElement(U,null,j.a.createElement(_,{align:"center",justify:"space-between",margin:"margin-right: 40px;"},j.a.createElement(W,{to:"cursor",size:"20",className:"bold",margin:"margin-bottom: 10px;"},1==i?"My Models":"My Datasets"),j.a.createElement(W,(t={to:"cursor",onClick:function(){return r(!i)},size:"20",className:"bold"},Object(Be.a)(t,"size","12.8"),Object(Be.a)(t,"color","rgb(100, 100, 100)"),Object(Be.a)(t,"margin","margin-bottom: 10px;"),t),1==i?"My Datasets":"My Models")),j.a.createElement(Je,{darkMode:o}),j.a.createElement(Je,{darkMode:o}),j.a.createElement(Je,{darkMode:o}),j.a.createElement(G,{justify:"center"},j.a.createElement(W,null,"More"))),j.a.createElement(U,{margin:"margin-top: 20px;"},j.a.createElement(W,{size:"20",className:"bold",margin:"margin-bottom: 10px;"},"Saved"),j.a.createElement(Je,{darkMode:o}),j.a.createElement(Je,{darkMode:o}),j.a.createElement(Je,{darkMode:o}),j.a.createElement(Je,{darkMode:o}),j.a.createElement(Je,{darkMode:o})))},Qe=Xe,$e=a(32),et=a.n($e),tt=Object(R.c)(W)(g||(g=Object(z.a)(["\n    :hover {\n        font-weight: 800;\n    };\n    transition: all 0.3s;\n"]))),at=function(e){var t=e.scale,a=void 0===t?1:t,n=B().darkMode,i=Object(M.useState)(!1),r=Object(T.a)(i,2),o=(r[0],r[1]),s=Object(M.useState)(!1),l=Object(T.a)(s,2),c=l[0],m=l[1];return j.a.createElement(U,{onClick:function(){return m(!c)},to:"cursor",cursorOpaFalse:!0,onMouseOver:function(){return o(!0)},onMouseLeave:function(){return o(!1)},style:{transform:"scale(".concat(a,")"),borderColor:c?"#D84476":"#1C8BD8",borderStyle:"solid",borderWidth:1.5},align:"center",height:"23px",width:"70px",justify:"center",br:"20px"},j.a.createElement(tt,{weight:"600",className:"light",color:n?"white":"black",padding:"padding: 8px 10px 8px 10px",size:"9"},c?"Unfollow":"+ Follow"))},nt=function(e){return Object(L.a)(e),j.a.createElement("div",{id:"sidebar",className:"sidebar"})},it=function(e){return Object(L.a)(e),j.a.createElement("div",{id:"toolbar",className:"toolbar"},j.a.createElement("button",{id:"menu-button",className:"toolbar-button",title:"Menu"},j.a.createElement("svg",{className:"icon",viewBox:"0 0 100 100"},j.a.createElement("rect",{className:"border",x:"12",y:"12",width:"76",height:"76",rx:"16",ry:"16",strokeWidth:"8"}),j.a.createElement("line",{className:"border",x1:"30",y1:"37",x2:"70",y2:"37",strokeWidth:"8",strokeLinecap:"round",stroke:"#fff"}),j.a.createElement("line",{className:"border",x1:"30",y1:"50",x2:"70",y2:"50",strokeWidth:"8",strokeLinecap:"round",stroke:"#fff"}),j.a.createElement("line",{className:"border",x1:"30",y1:"63",x2:"70",y2:"63",strokeWidth:"8",strokeLinecap:"round",stroke:"#fff"}),j.a.createElement("rect",{className:"stroke",x:"12",y:"12",width:"76",height:"76",rx:"16",ry:"16",strokeWidth:"4"}),j.a.createElement("line",{className:"stroke",x1:"30",y1:"37",x2:"70",y2:"37",strokeWidth:"4",strokeLinecap:"round"}),j.a.createElement("line",{className:"stroke",x1:"30",y1:"50",x2:"70",y2:"50",strokeWidth:"4",strokeLinecap:"round"}),j.a.createElement("line",{className:"stroke",x1:"30",y1:"63",x2:"70",y2:"63",strokeWidth:"4",strokeLinecap:"round"}))),j.a.createElement("button",{id:"zoom-in-button",className:"toolbar-button",title:"Zoom In"},j.a.createElement("svg",{className:"icon",viewBox:"0 0 100 100"},j.a.createElement("circle",{className:"border",cx:"50",cy:"50",r:"35",strokeWidth:"8",stroke:"#fff"}),j.a.createElement("line",{className:"border",x1:"50",y1:"38",x2:"50",y2:"62",strokeWidth:"8",strokeLinecap:"round",stroke:"#fff"}),j.a.createElement("line",{className:"border",x1:"38",y1:"50",x2:"62",y2:"50",strokeWidth:"8",strokeLinecap:"round",stroke:"#fff"}),j.a.createElement("line",{className:"border",x1:"78",y1:"78",x2:"82",y2:"82",strokeWidth:"12",strokeLinecap:"square",stroke:"#fff"}),j.a.createElement("circle",{className:"stroke",cx:"50",cy:"50",r:"35",strokeWidth:"4"}),j.a.createElement("line",{className:"stroke",x1:"50",y1:"38",x2:"50",y2:"62",strokeWidth:"4",strokeLinecap:"round"}),j.a.createElement("line",{className:"stroke",x1:"38",y1:"50",x2:"62",y2:"50",strokeWidth:"4",strokeLinecap:"round"}),j.a.createElement("line",{className:"stroke",x1:"78",y1:"78",x2:"82",y2:"82",strokeWidth:"8",strokeLinecap:"square"}))),j.a.createElement("button",{id:"zoom-out-button",className:"toolbar-button",title:"Zoom Out"},j.a.createElement("svg",{className:"icon",viewBox:"0 0 100 100"},j.a.createElement("circle",{className:"border",cx:"50",cy:"50",r:"35",strokeWidth:"8",stroke:"#fff"}),j.a.createElement("line",{className:"border",x1:"38",y1:"50",x2:"62",y2:"50",strokeWidth:"8",strokeLinecap:"round",stroke:"#fff"}),j.a.createElement("line",{className:"border",x1:"78",y1:"78",x2:"82",y2:"82",strokeWidth:"12",strokeLinecap:"square",stroke:"#fff"}),j.a.createElement("circle",{className:"stroke",cx:"50",cy:"50",r:"35",strokeWidth:"4"}),j.a.createElement("line",{className:"stroke",x1:"38",y1:"50",x2:"62",y2:"50",strokeWidth:"4",strokeLinecap:"round"}),j.a.createElement("line",{className:"stroke",x1:"78",y1:"78",x2:"82",y2:"82",strokeWidth:"8",strokeLinecap:"square"}))),j.a.createElement("button",{id:"back-button",className:"toolbar-back-button",title:"Back"},"\u276e"),j.a.createElement("button",{id:"name-button",className:"toolbar-name-button",title:"Name"}),j.a.createElement("div",{id:"menu-dropdown",className:"dropdown",style:{display:"none"}}))},rt=function(e){e.darkMode;return j.a.createElement(G,{style:{width:"100%",overflow:"hidden",position:"relative"}},j.a.createElement("div",{id:"graph",className:"graph",tabIndex:0},j.a.createElement("svg",{id:"canvas",className:"canvas",preserveAspectRatio:"xMidYMid meet",width:"100%",height:"100%"})),j.a.createElement(nt,null),j.a.createElement(it,null))},ot=function(e){var t=e.setVisualizer,a=(e.visualizer,e.darkMode);return j.a.createElement(Z,{width:"100%",padding:"padding: 25px 10%",bg:a?"rgba(15, 15, 15, 0.98)":"rgba(255, 255, 255, 0.95)",style:{backdropFilter:"blur(2px)",height:"100%",alignSelf:"center",justifySelf:"flex-end",overflowY:"scroll",position:"relative"}},j.a.createElement(G,{cursorOpaFalse:!0,onClick:function(){return t(!1)},to:"cursor",position:"absolute",style:{right:0,top:0}},j.a.createElement(ne,{width:30,height:30,bg:"rgb(47, 47, 47)",stroke:"white"})),j.a.createElement(U,{padding:"padding: 25px;",style:{overflow:"hidden"}},j.a.createElement(_,{align:"center"},j.a.createElement(J,{bg:Oe(a),image:et.a,size:33}),j.a.createElement(U,{margin:"margin-left: 15px;"},j.a.createElement(_,{align:"center"},j.a.createElement(W,{size:"15",weight:"600",margin:"margin-right: 8px;"},"Neuralverse")),j.a.createElement(W,{size:"12",margin:"margin-top: 2.5px;",style:{opacity:.8}},"Beyond the State of the Arts"))),j.a.createElement(_,{align:"center"},j.a.createElement(U,{margin:"margin: 20px 0px; margin-right: 20px;",flex:1},j.a.createElement(W,{className:"bold",weight:"600",size:"18"},"A ConvNet for the 2020s"),j.a.createElement(W,{className:"light",size:"12.8",weight:"400",margin:"margin-top: 5px;",style:{opacity:.8}},"Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie")),j.a.createElement(st,{darkMode:a})),j.a.createElement(rt,{darkMode:a})))},st=function(e){var t=e.darkMode;return j.a.createElement(G,{height:"27px;",ato:"cursor",margin:"margin-bottom: 25px;",cursorOpaFalse:!0,width:"170px",opacity:.9,bg:je(t),br:"20px;"},j.a.createElement(W,{size:"12",className:"bold",color:"white",padding:"padding: 8px 15px; padding-top: 6px;"},"Customize this Model"))},lt=function(){var e=F(),t=e.visualizer,a=e.setVisualizer,n=B().darkMode;return j.a.createElement(U,{className:"visualbox",bg:"rgba(0,0,0,0)",position:"fixed",style:{width:"100%",height:Ke-148,overflow:"hidden",opacity:t?1:0,top:t?80:"100%",transition:"all 300ms",zIndex:1e3}},j.a.createElement(ot,{setVisualizer:a,visualizer:t,darkMode:n}))},ct=(a(71),Object(R.c)(W)(u||(u=Object(z.a)(["\n    font-size: 12.8px;\n    font-weight: 400;\n    cursor: pointer;\n    opacity: 0.7;\n    :hover {\n        opacity: 1;\n    }\n"]))),function(e){return{fontSize:12.8,fontWeight:400,opacity:e?.7:1}}),mt=function(e){var t=e.darkMode;return j.a.createElement(_,{height:"300px;",align:"center",justify:"space-between",padding:"padding: 55px; padding-right: 150px",bg:Oe(t)},j.a.createElement(U,{align:"center"},j.a.createElement(K,{height:"25px",src:Ce.a}),j.a.createElement(W,{style:ct(t),className:"light",margin:"margin-top: 20px;"},"\xa9 2022 NeuralVerse. All right reserved."),j.a.createElement(_,{margin:"margin-top: 30px;"},j.a.createElement(W,{style:ct(t),className:"light"},"Technical Supports:"),j.a.createElement(W,{style:ct(t),className:"light",margin:"margin-left: 5px;"},"support@neuralverse.ai")),j.a.createElement(_,{margin:"margin-top: 10px;"},j.a.createElement(W,{style:ct(t),className:"light"},"Other Supports:"),j.a.createElement(W,{style:ct(t),className:"light",margin:"margin-left: 5px;"},"team@neuralverse.ai"))),j.a.createElement(U,{align:"center"},j.a.createElement(W,{className:"light"},"SERVICES"),j.a.createElement(U,{align:"center"},j.a.createElement(W,{className:"light",margin:"margin-top: 20px;",style:ct(t)},"Neural Verse"),j.a.createElement(W,{className:"light",margin:"margin-top: 15px;",style:ct(t)},"Neural Space"),j.a.createElement(W,{className:"light",margin:"margin-top: 15px;",style:ct(t)},"Neural Studio"),j.a.createElement(W,{className:"light",margin:"margin-top: 15px;",style:ct(t)},"Neural Ops"),j.a.createElement(W,{className:"light",margin:"margin-top: 15px;",style:ct(t)},"Neural Workspace"))),j.a.createElement(U,{align:"center"},j.a.createElement(W,{className:"light"},"DEVELOPERS"),j.a.createElement(U,{align:"center"},j.a.createElement(W,{style:ct(t),margin:"margin-top: 20px;",className:"light"},"Overview"),j.a.createElement(W,{style:ct(t),margin:"margin-top: 15px",className:"light"},"Neural Blog"),j.a.createElement(W,{style:ct(t),margin:"margin-top: 15px;",className:"light"},"API Documentation"),j.a.createElement(W,{style:ct(t),margin:"margin-top: 15px;",className:"light"},"API Token"),j.a.createElement(W,{style:ct(t),margin:"margin-top: 15px;",className:"light"},"Neural Discuss"))),j.a.createElement(U,{align:"center"},j.a.createElement(W,{className:"light"},"NEURALVERSE"),j.a.createElement(U,{align:"center"},j.a.createElement(W,{style:ct(t),margin:"margin-top: 20px;",className:"light"},"About Us"),j.a.createElement(W,{style:ct(t),margin:"margin-top: 15px;",className:"light"},"Contact Us"),j.a.createElement(W,{style:ct(t),margin:"margin-top: 15px;",className:"light"},"Terms of Service"),j.a.createElement(W,{style:ct(t),margin:"margin-top: 15px;",className:"light"},"Privacy Policy"),j.a.createElement(W,{style:ct(t),margin:"margin-top: 15px;",className:"light"},"Github"))))},dt=a(33),pt=a.n(dt),ht=a(19),gt=a.n(ht),ut=function(e){e.darkMode;return j.a.createElement(U,{justify:"center",style:{height:"100vh",width:"100vw",overflow:"hidden"},position:"relative"},j.a.createElement("div",{className:"landing",style:{position:"absolute",width:"100%",height:"100%",zIndex:10}}),j.a.createElement(U,{justify:"center",align:"center",style:{alignSelf:"center"},position:"absolute",zIndex:10},j.a.createElement(W,{zIndex:10,className:"bold",size:"30",color:"white",style:{textShadow:"1.5px 1.5px 1.5px rgba(20, 20, 20, 0.9)"}},"BEYOND THE STATE OF THE ARTS"),j.a.createElement(W,{zIndex:10,margin:"margin: 10px",color:"white",className:"bold",style:{textShadow:"1.5px 1.5px 1.5px rgba(20, 20, 20, 0.9)"},size:"20"},"WE BELIEVE THE HUMAN COLLECTIVE INTELLIGENCE")),j.a.createElement(gt.a,{mute:!0,playing:!0,loop:!0,url:pt.a,width:"100%",height:"100%",style:{position:"absolute"}}))},ft=function(e){return Object(L.a)(e),j.a.createElement(U,{align:"center",justify:"center",style:{height:"120vh"},position:"relative"},j.a.createElement(_,{align:"center",position:"absolute",style:{right:"15%",top:"5%"}},j.a.createElement(ee,{width:30,height:30}),j.a.createElement(W,{className:"bold",size:"18",margin:"margin-left: 15px;"},"NEURAL VERSE")),j.a.createElement(_,null,j.a.createElement(U,{flex:1,align:"center",justify:"center"},j.a.createElement(W,{className:"bold",size:"30"},"Share More"),j.a.createElement(W,{className:"bold",size:"30"},"Grow Fast")),j.a.createElement(U,{flex:1,align:"center",justify:"center"},j.a.createElement(W,null,"Sharing your idea helps growing ideas"))),j.a.createElement(_,{align:"center",justify:"center"},j.a.createElement(U,{flex:1,align:"center",justify:"center"},j.a.createElement(W,{className:"bold",size:"30"},"Share More"),j.a.createElement(W,{className:"bold",size:"30"},"Grow Fast")),j.a.createElement(U,{flex:1,align:"center",justify:"center"},j.a.createElement(W,null,"Sharing your idea helps growing ideas"))))},bt=function(e){return Object(L.a)(e),j.a.createElement(U,{align:"center",justify:"center",style:{height:"70vh"},position:"relative"},j.a.createElement(_,{align:"center",position:"absolute",style:{left:"15%",top:"5%"}},j.a.createElement(Q,{width:50,height:50}),j.a.createElement(W,{className:"bold",size:"18",margin:"margin-left: 15px;"},"NEURAL SPACE")),j.a.createElement(W,{className:"bold",size:"30"},"CUSTOMIZE SOTAS TO YOUR OWN"),j.a.createElement(W,{className:"bold",size:"30"},"GENERATE YOUR MODEL IN INSTANCE"))},wt=function(e){return Object(L.a)(e),j.a.createElement(U,{align:"center",justify:"center",style:{height:"70vh"},position:"relative"},j.a.createElement(_,{align:"center",position:"absolute",style:{right:"15%",top:"5%"}},j.a.createElement(te,{width:50,height:50}),j.a.createElement(W,{className:"bold",size:"18",margin:"margin-left: 15px;"},"NEURAL STUDIO")),j.a.createElement(W,{className:"bold",size:"30"},"GENERATE NFT OF YOUR OWN MODEL"))},Et=a(34),kt=a.n(Et),vt=function(e){return Object(L.a)(e),j.a.createElement(_,{align:"center",justify:"center",style:{height:"70vh"},padding:"padding: 10%",position:"relative"},j.a.createElement(_,{align:"center",position:"absolute",style:{left:"15%",top:"5%"}},j.a.createElement(K,{src:Ce.a,width:50}),j.a.createElement(W,{className:"bold",size:"18",margin:"margin-left: 15px;"},"NEURAL OPS")),j.a.createElement(U,{flex:1},j.a.createElement(W,{className:"bold",size:"30"},"TRAIN YOUR MODEL ON BATCH FLY")),j.a.createElement(U,{flex:1},j.a.createElement(G,{position:"relative",width:"100%",height:"100%"},j.a.createElement(G,{style:{position:"absolute",width:300,height:30,right:0},bg:"black"}),j.a.createElement(gt.a,{muted:!0,playing:!0,loop:!0,width:"100%",height:"100%",style:{objectFit:"cover"},url:kt.a})),j.a.createElement(W,null,"3Brown1Eye")))},xt=function(e){return Object(L.a)(e),j.a.createElement(G,{align:"center",justify:"center",style:{height:"70vh"}},j.a.createElement(W,{className:"bold",size:"30"},"RATE AND COMPETE WITH OTHER MODELS"))},yt=function(e){return Object(L.a)(e),j.a.createElement(G,{align:"center",justify:"center",style:{height:"70vh"}},j.a.createElement(W,{size:"30",className:"bold"},"CHECK OUT SOTAS IN NEURALVERSE"))},Ct=function(e){e.route;var t=e.setRoute;return j.a.createElement(U,{align:"center",justify:"center",style:{height:"70vh"},padding:"padding: 15%"},j.a.createElement(W,{className:"bold",size:"30"},"THE MOST SOPHISCATED END-TO-END DEEP-LEARNING OPS"),j.a.createElement(_,{align:"center",justify:"space-between",flex:1},j.a.createElement("a",{onClick:function(){return t(1)}},j.a.createElement(U,{padding:"padding: 20px",width:"300px",height:"90%",br:"20px",align:"center"},j.a.createElement(_,{align:"center"},j.a.createElement(ee,{width:22}),j.a.createElement(G,{to:"cursor",padding:"padding: 10px 15px 10px 15px;"},j.a.createElement(W,{className:"light",size:"12.8",weight:"700"},"NEURAL VERSE"))))),j.a.createElement("a",{onClick:function(){return t(3)}},j.a.createElement(U,{padding:"padding: 20px",width:"300px",height:"90%",br:"20px",align:"center"},j.a.createElement(_,{align:"center"},j.a.createElement(Q,{width:22}),j.a.createElement(G,{to:"cursor",padding:"padding: 10px 15px 10px 15px;"},j.a.createElement(W,{className:"light",size:"12.8",weight:"700"},"NEURAL SPACE"))))),j.a.createElement("a",{onClick:function(){return t(4)}},j.a.createElement(U,{padding:"padding: 20px",width:"300px",height:"90%",br:"20px",align:"center"},j.a.createElement(_,{align:"center"},j.a.createElement(te,{width:30,height:30}),j.a.createElement(G,{to:"cursor",padding:"padding: 10px 15px 10px 15px;"},j.a.createElement(W,{className:"light",size:"12.8",weight:"700"},"NEURAL STUDIO"))))),j.a.createElement("a",{onClick:function(){return t(5)}},j.a.createElement(U,{padding:"padding: 20px",width:"300px",height:"90%",br:"20px",align:"center"},j.a.createElement(_,{align:"center"},j.a.createElement(K,{src:Ce.a,width:30}),j.a.createElement(G,{to:"cursor",padding:"padding: 10px 15px 10px 12.8px;"},j.a.createElement(W,{className:"light",size:"12.8",weight:"700"},"NEURAL OPS"))))),j.a.createElement("a",{onClick:function(){return t(5)}},j.a.createElement(U,{padding:"padding: 20px",width:"300px",height:"90%",br:"20px",align:"center"},j.a.createElement(_,{align:"center"},j.a.createElement(pe,{stroke1:"#ED2166",stroke2:"#25A2F9",stroke3:"#25A2F9",height:15,width:25}),j.a.createElement(G,{to:"cursor",padding:"padding: 10px 15px 10px 12.8px;"},j.a.createElement(W,{className:"light",size:"12.8",weight:"700"},"NEURAL RESEARCH")))))))},St=function(e){var t=e.route,a=e.setRoute,n=B().darkMode;return j.a.createElement(G,{style:{display:0==t?"block":"none",backgroundColor:n?"black":"white"}},j.a.createElement(ut,{darkMode:n}),j.a.createElement(Ct,{setRoute:a}),j.a.createElement(ft,null),j.a.createElement(bt,null),j.a.createElement(wt,null),j.a.createElement(vt,null),j.a.createElement(xt,null),j.a.createElement(yt,null),j.a.createElement(mt,{darkMode:n}))},Nt=R.c.a(f||(f=Object(z.a)(["\n    :hover {\n        cursor: pointer;\n    }\n"]))),It=function(e){var t=e.darkMode,a=e.arg;return j.a.createElement(Nt,null,j.a.createElement(Z,{padding:"padding: 0px 12px; padding-bottom: 6px;",margin:"margin-right: 10px; margin-bottom: 10px;",br:"5px",style:{display:"inline-block"},bg:Oe(t)},j.a.createElement(W,{className:"light",weight:"500",size:"11"},"# ",a)))},At=Object(R.c)(G)(b||(b=Object(z.a)(["\n    margin: 10px 12.8px;\n    &:last-child {\n        margin-right: 0px;\n    };\n    position: relative;\n    cursor: pointer;\n    display: inline-block;\n    width: 31%;\n    height: 190px;\n    bottom: 0px;\n    :hover {\n        bottom: 5px;\n    };\n    transition: 300ms;\n"]))),Mt=function(e){var t=e.model;e.darkMode;return j.a.createElement(_,{justify:"space-between",margin:"margin-top: 10px;"},j.a.createElement(_,{padding:"padding: 3px",align:"center"},j.a.createElement(G,{margin:"margin-bottom: 2px;"},j.a.createElement(me,{stroke:Me,height:12.8,width:12.8})),j.a.createElement(W,{className:"light",margin:"margin-left: 10px;",size:"10",weight:"600"},"PAPER")),j.a.createElement(_,{padding:"padding: 3px",align:"center"},j.a.createElement(pe,{stroke:Me,height:15,width:15}),j.a.createElement(W,{className:"light",margin:"margin-left: 10px;",size:"10",weight:"600"},"CODE")),j.a.createElement(_,{padding:"padding: 3px",align:"center"},j.a.createElement(ae,{stroke1:Me,stroke2:Me,height:15,width:15}),j.a.createElement(W,{className:"light",margin:"margin-left: 10px;",size:"10",weight:"600"},"STUDIO")),t.demo&&j.a.createElement(_,{padding:"padding: 3px",align:"center"},j.a.createElement(ge,{stroke1:Me,stroke2:Me,height:15,width:15}),j.a.createElement(W,{className:"light",margin:"margin-left: 10px;",size:"10",weight:"600"},"DEMO")))},jt=function(e){var t=e.setModel,a=e.darkMode,n=e.content;return j.a.createElement(At,{onClick:function(){return t(n)},shadow:!0,bg:Oe(a),position:"relative",br:"20px",style:{overflow:"hidden"}},j.a.createElement(U,{padding:"padding: 0px 15px;",width:"100%",height:"50%",position:"absolute",justify:"center",zIndex:100,style:{bottom:-1,backdropFilter:"blur(2px)"},bg:a?"rgba(0,0,0,0.8)":"rgba(255, 255, 255, 0.95)"},j.a.createElement(W,{className:"light",size:"17",weight:"700",margin:"margin: 5px 0px;"},n.title),j.a.createElement(W,{className:"light",size:"12.8",weight:"600"},n.subtitle),j.a.createElement(Mt,{model:n,darkMode:a})),j.a.createElement(K,{src:n.img,of:"cover",position:"absolute",width:"108%",height:"108%"}))},Ot=function(e){var t=e.task,a=e.subTask,n=(e.options,e.setOptions),i=e.optionBtn,r=e.setOptionbtn,o=e.darkMode;return j.a.createElement(j.a.Fragment,null,j.a.createElement(_,{align:"center",justify:"space-between"},j.a.createElement(_,{align:"center"},j.a.createElement(W,{className:"bold",size:"20",margin:"margin: 20px 0px;"},"TRENDINGS ON ",a?a.title.toUpperCase():t.title.toUpperCase())),j.a.createElement(_,{to:"cursor",cursorOpaFalse:!0,align:"center",us:"none",onClick:function(){return r(!i)}},j.a.createElement(W,{className:"light",weight:"500",size:"12.8",margin:"margin-right: 10px;"},"View Options"),j.a.createElement(G,{style:{transform:i?"rotate(180deg)":"rotate(0deg)"}},j.a.createElement(ie,{width:20,height:25,stroke:o?"white":"black"})))),j.a.createElement(U,{style:{width:"100%",height:i?35:0,transition:"all 300ms",overflow:"hidden"}},j.a.createElement(U,null,j.a.createElement(G,{margin:"margin-top: 5px;",style:{display:"block"}},j.a.createElement(_,{align:"center"},j.a.createElement(_,{onClick:function(){return n(0)},to:"cursor",align:"center",margin:"margin-right: 30px;"},j.a.createElement(W,{className:"light",weight:"500",size:"12.8"},"See All")),j.a.createElement(_,{onClick:function(){return n(0)},to:"cursor",align:"center",margin:"margin-right: 30px;"},j.a.createElement(W,{className:"light",weight:"500",size:"12.8",margin:"margin-left: 7px;"},"Most Implemented")),j.a.createElement(_,{onClick:function(){return n(0)},to:"cursor",align:"center",margin:"margin-right: 30px;"},j.a.createElement(W,{className:"light",weight:"500",size:"12.8",margin:"margin-left: 7px;"},"Latest")),j.a.createElement(_,{onClick:function(){return n(1)},to:"cursor",align:"center",margin:"margin-right: 30px;"},j.a.createElement(ge,{width:18,height:18,stroke:o?"rgb(150, 150, 150)":"rgb(100, 100, 100)"}),j.a.createElement(W,{className:"light",weight:"500",size:"12.8",margin:"margin-left: 7px;"},"Demo Availables")),j.a.createElement(_,{onClick:function(){return n(2)},to:"cursor",align:"center",margin:"margin-right: 30px;"},j.a.createElement(he,{width:18,height:18,stroke:o?"rgb(150, 150, 150)":"rgb(100, 100, 100)"}),j.a.createElement(W,{className:"light",weight:"500",size:"12.8",margin:"margin-left: 7px;"},"Benchmarks")),j.a.createElement(_,{onClick:function(){return n(2)},to:"cursor",align:"center",margin:"margin-right: 30px;"},j.a.createElement(he,{width:18,height:18,stroke:o?"rgb(150, 150, 150)":"rgb(100, 100, 100)"}),j.a.createElement(W,{className:"light",weight:"500",size:"12.8",margin:"margin-left: 7px;"},"Datasets"))),j.a.createElement(G,{style:{display:"block"},margin:"margin: 15px 0px;"},j.a.createElement(It,{arg:"All Dataset",darkMode:o}),a?a.datasets.map(function(e){return j.a.createElement(It,{key:e,arg:e,darkMode:o})}):t.datasets.map(function(e){return j.a.createElement(It,{arg:e,darkMode:o,key:e})}))))))},Dt=function(e){var t=e.setModel,a=e.darkMode,n=e.task,i=e.subTask,r=Object(M.useState)(!1),o=Object(T.a)(r,2),s=o[0],l=o[1],c=Object(M.useState)(!1),m=Object(T.a)(c,2),d=(m[0],m[1],Object(M.useState)(0)),p=Object(T.a)(d,2),h=p[0],g=p[1],u=Object(M.useState)(["All"]),f=Object(T.a)(u,2),b=(f[0],f[1],Object(M.useState)(1)),w=Object(T.a)(b,2),E=w[0],k=w[1],v=Object(M.useState)(n),x=Object(T.a)(v,2),y=x[0],C=x[1],S=Object(M.useState)(i),N=Object(T.a)(S,2),I=N[0],A=N[1];return Object(M.useEffect)(function(){k(0),setTimeout(function(){k(1),C(n),A(i)},300)},[n,i]),j.a.createElement(U,{flex:3,margin:"margin-top: 50px;"},j.a.createElement(Ot,{task:n,subTask:i,options:h,setOptions:g,optionBtn:s,setOptionbtn:l,darkMode:a}),j.a.createElement(U,{opacity:E,position:"relative",style:{left:-15,width:"calc(100% + 30px)"}},j.a.createElement(G,{margin:"margin: 12px 0px;",style:{display:"block"}},void 0!=I&&I.models?I.models.map(function(e){return j.a.createElement(jt,{darkMode:a,setModel:t,content:e,key:e.title})}):void 0!=y&&y.models&&y.models.map(function(e,n){return j.a.createElement(jt,{darkMode:a,setModel:t,content:e,key:n})}))))},Tt=(a(92),function(){return j.a.createElement("div",{id:"load"},j.a.createElement("div",null,"G"),j.a.createElement("div",null,"N"),j.a.createElement("div",null,"I"),j.a.createElement("div",null,"D"),j.a.createElement("div",null,"A"),j.a.createElement("div",null,"O"),j.a.createElement("div",null,"L"))}),Lt=function(e){var t=e.text;return j.a.createElement(G,null,j.a.createElement(W,null,t||"Please Input Any Image to Model \ud83d\ude0a"))},zt=function(e){var t=e.defaultSec,a=Object(M.useState)(0),n=Object(T.a)(a,2),i=n[0],r=n[1];return Object(M.useEffect)(function(){setInterval(function(){var e=i;r(e+=1)},1e3)}),j.a.createElement(W,{style:{opacity:.7}},i,".0 / ",t,".0s")},Rt=function(e,t,a,n){var i=null;i=Array.isArray(e)?{data:e}:n?{data:[e,n]}:{data:[e]},console.log(i),fetch(t,{method:"POST",body:JSON.stringify(i),headers:{"Content-Type":"application/json"}}).then(function(e){if(200===e.status)return console.log("hi",e),e.json();console.log(e)}).then(function(e){return console.log("output",e),a(e),e})},Vt=a(13),Bt=a.n(Vt),Pt=a(14),Ft=a.n(Pt),Ht=a(15),Wt=a.n(Ht),Gt=function(e){var t=e.darkMode;e.img,e.setImg;return j.a.createElement(G,{margin:"margin-bottom: 10px;",to:"cursor"},j.a.createElement("form",null,j.a.createElement("input",{accept:"image/*",style:{display:"none"},type:"file"}),j.a.createElement(G,{br:"20px",width:"100%",height:"100px;",align:"center",justify:"center",style:{border:"2px dashed ".concat(t?"rgb(70, 70, 70)":"rgb(224, 224, 224)")}},j.a.createElement(W,{size:"12",color:t?"rgb(150, 150, 150)":"rgb(124, 124, 124)",style:{textAlign:"center"}},"Drag image file here or click to browse from your device"))))},_t=Object(R.c)(G)(w||(w=Object(z.a)(["\n    opacity: ",";\n    :hover {\n        opacity: 1\n    };\n    cursor: pointer;\n    border-radius: 20px;\n    overflow: hidden;\n    width: 100%;\n    height: 100px;\n    margin: 10px 0px;\n"])),function(e){return e.img==e.item?1:.7}),Ut=[Bt.a,Ft.a,Wt.a],Zt=function(e){var t=e.run,a=e.displayImage;return j.a.createElement(U,null,Ut.map(function(e,n){return j.a.createElement(_t,{onClick:function(){return t(e)},img:a,item:e,key:n},j.a.createElement(K,{width:"100%",height:"100px",of:"cover",src:e}))}))},Kt=function(e){var t=e.run,a=e.displayImage,n=e.open,i=(e.setOpen,e.darkMode);e.type;return j.a.createElement(Z,{width:"20%",height:"100%",padding:"padding: 15px;",position:"absolute",bg:Oe(i),style:{left:n?0:"-25%",overflow:"hidden",overflowY:"scroll"}},j.a.createElement(Gt,{darkMode:i}),j.a.createElement(Zt,{displayImage:a,run:t}))},Yt=function(e){return fetch(e).then(function(e){return e.blob()}).then(function(e){return new Promise(function(t,a){var n=new FileReader;n.onloadend=function(){return t(n.result)},n.onerror=a,n.readAsDataURL(e)})})},Jt=function(e){var t=e.model,a=e.open,n=e.setOpen,i=e.darkMode,r=Object(M.useState)(null),o=Object(T.a)(r,2),s=o[0],l=o[1],c=Object(M.useState)(null),m=Object(T.a)(c,2),d=m[0],p=m[1],h=Object(M.useState)(!1),g=Object(T.a)(h,2),u=g[0],f=g[1],b=Object(M.useState)(!1),w=Object(T.a)(b,2),E=w[0],k=w[1],v=Object(M.useState)(null),x=Object(T.a)(v,2),y=x[0],C=x[1];return Object(M.useEffect)(function(){d&&function(){var e=d.data[0];C(e),f(!1),setTimeout(function(){return k(!0)},100)}()},[d]),j.a.createElement(U,{position:"relative",width:"100%",height:"100%",align:"center",justify:"center"},j.a.createElement(Kt,{run:function(e){k(!1),l(e),f(!0),Yt(e).then(function(e){Rt(e,t.demo.api,p)})},displayImage:s,open:a,setOpen:n,darkMode:i}),j.a.createElement(_,{align:"center",justify:"center",width:"100%",height:"100%",padding:"padding-left: ".concat(a?"20%":0)},j.a.createElement(U,{flex:3,width:"100%",height:"100%",align:"center",justify:"center"},s?u?j.a.createElement(Tt,null):j.a.createElement(K,{width:"100%",height:"100%",of:"cover",style:{opacity:u?.3:1,transition:"all 300ms"},src:s}):j.a.createElement(Lt,null)),u&&j.a.createElement(G,{position:"absolute",style:{right:10,top:10},zIndex:1e3},j.a.createElement(zt,{defaultSec:3})),j.a.createElement(U,{flex:1,height:"100%"},y&&!u&&j.a.createElement(U,{padding:"padding: 15px;",height:"100%",bg:Oe(i)},j.a.createElement(W,{className:"light",size:"14",margin:"margin-bottom: 20px;",style:{alignSelf:"center"}},y.label),y.confidences.map(function(e,t){return j.a.createElement(_,{align:"center",margin:"margin: 5px 0px;",key:t},j.a.createElement(U,{width:"100%"},j.a.createElement(_,{justify:"space-between"},j.a.createElement(W,{className:"light",weight:"500",size:"12.8",margin:"margin-right: 20px;"},e.label),j.a.createElement(W,{size:"12.8",style:{opacity:.7}},"(",(100*e.confidence).toFixed(2),"%)")),j.a.createElement(G,{margin:"margin-top: 5px;",style:{width:E?"".concat(100*e.confidence,"%"):0,transition:"all 300ms"},height:"1px",bg:"#00dcff"})))})))))},qt=function(e){var t=e.darkMode;e.img,e.setImg;return j.a.createElement(G,{margin:"margin-bottom: 10px;",to:"cursor"},j.a.createElement("form",null,j.a.createElement("input",{accept:"image/*",style:{display:"none"},type:"file"}),j.a.createElement(G,{br:"20px",width:"100%",height:"100px;",align:"center",justify:"center",style:{border:"2px dashed ".concat(t?"rgb(70, 70, 70)":"rgb(224, 224, 224)")}},j.a.createElement(W,{size:"12",color:t?"rgb(150, 150, 150)":"rgb(124, 124, 124)",style:{textAlign:"center"}},"Drag image file here or click to browse from your device"))))},Xt=Object(R.c)(G)(E||(E=Object(z.a)(["\n    opacity: ",";\n    :hover {\n        opacity: 1\n    };\n    cursor: pointer;\n    border-radius: 20px;\n    overflow: hidden;\n    width: 100%;\n    height: 100px;\n    margin: 10px 0px;\n"])),function(e){return e.img==e.item?1:.7}),Qt=[Bt.a,Ft.a,Wt.a],$t=function(e){var t=e.run,a=e.displayImage;return j.a.createElement(U,null,Qt.map(function(e,n){return j.a.createElement(Xt,{onClick:function(){return t(e)},img:a,item:e,key:n},j.a.createElement(K,{width:"100%",height:"100px",of:"cover",src:e}))}))},ea=function(e){var t=e.run,a=e.displayImage,n=e.open,i=(e.setOpen,e.darkMode);e.type;return j.a.createElement(Z,{width:"20%",height:"100%",padding:"padding: 15px;",position:"absolute",bg:Oe(i),style:{left:n?0:"-25%",overflow:"hidden",overflowY:"scroll"}},j.a.createElement(qt,{darkMode:i}),j.a.createElement($t,{displayImage:a,run:t}))},ta=Object(R.c)(G)(k||(k=Object(z.a)(["\n    opacity: 0.8;\n    :hover {\n        opacity: 1;\n    };\n    transition: all 300ms;\n    cursor: pointer;\n"]))),aa=function(e){var t=e.model,a=e.open,n=e.setOpen,i=e.darkMode,r=(e.type,Object(M.useState)()),o=Object(T.a)(r,2),s=o[0],l=o[1],c=Object(M.useState)(null),m=Object(T.a)(c,2),d=m[0],p=m[1],h=Object(M.useState)(!1),g=Object(T.a)(h,2),u=g[0],f=g[1],b=Object(M.useState)(!1),w=Object(T.a)(b,2),E=w[0],k=w[1],v=Object(M.useState)(null),x=Object(T.a)(v,2),y=x[0],C=x[1];Object(M.useEffect)(function(){t.demo.models&&C(t.demo.models[0])},[]);return Object(M.useEffect)(function(){d&&function(){var e=d.data[0];l(e),f(!1)}()},[d]),j.a.createElement(U,{position:"relative",width:"100%",height:"100%",align:"center",justify:"center"},j.a.createElement(ea,{run:function(e){f(!0),Yt(e).then(function(e){Rt(e,t.demo.api,p,y)})},displayImage:s,open:a,setOpen:n,darkMode:i}),j.a.createElement(_,{align:"center",justify:"center",width:"100%",height:"100%",padding:"padding-left: ".concat(a?"20%":0)},j.a.createElement(U,{flex:3,width:"100%",height:"100%",align:"center",justify:"center"},s?u?j.a.createElement(Tt,null):j.a.createElement(K,{width:"100%",height:"100%",of:"cover",src:s}):u?j.a.createElement(Tt,null):j.a.createElement(Lt,null)),u&&j.a.createElement(G,{position:"absolute",style:{right:10,top:10},zIndex:1e3},j.a.createElement(zt,{defaultSec:3}))),t.demo.models&&!u&&j.a.createElement(U,{position:"absolute",shadow:!0,onClick:function(){return k(!E)},bg:Oe(i),padding:"padding: 0px 5px",style:{width:"auto",height:E?45*t.demo.models.length:40,transition:"all 300ms",right:10,top:10,overflow:"hidden"},br:"20px"},j.a.createElement(W,{color:i?"#00dcff":"black",to:"cursor",size:"14",weight:"600",padding:"padding: 10px 15px;"},"Model Selected: ",y),t.demo.models.map(function(e){return j.a.createElement(ta,{padding:"padding: 10px 15px;",to:"cursor",key:e,onClick:function(){return C(e)}},j.a.createElement(W,{size:"14",color:i?"#00dcff":"black"},e))})))},na=function(e){var t=Object(M.useState)(e),a=Object(T.a)(t,2),n=a[0],i=a[1];return{value:n,onChange:function(e){i(e.target.value)}}},ia=function(e){var t=e.run,a=e.open,n=(e.setOpen,e.darkMode),i=na(""),r=i.value,o=i.onChange,s=Object(M.useState)(38),l=Object(T.a)(s,2),c=l[0],m=l[1],d=Object(M.useRef)(null);return j.a.createElement(Z,{width:"33%",height:"100%",padding:"padding: 15px;",position:"absolute",bg:Oe(n),style:{left:a?0:"-33%",overflow:"hidden",overflowY:"scroll"}},j.a.createElement(_,{align:"center"},j.a.createElement(X,{ref:d,onChange:function(e){o(e),m(r?d.current.scrollHeight:38)},value:r,size:"14",placeholder:"What do you want to see?",padding:"padding: 10px 15px;",br:"10px",bg:n?"rgb(40, 40, 40)":"white",style:{width:"90%",height:c}}),j.a.createElement(G,{margin:"margin-left: 10px",to:"cursor",onClick:function(){return t(r)}},j.a.createElement(Ee,{width:25,height:25}))))},ra=Object(R.c)(G)(v||(v=Object(z.a)(["\n    opacity: 0.8;\n    :hover {\n        opacity: 1;\n    };\n    transition: all 300ms;\n    cursor: pointer;\n"]))),oa=function(e){var t=e.model,a=e.open,n=e.setOpen,i=e.darkMode,r=Object(M.useState)(),o=Object(T.a)(r,2),s=o[0],l=o[1],c=Object(M.useState)(null),m=Object(T.a)(c,2),d=m[0],p=m[1],h=Object(M.useState)(!1),g=Object(T.a)(h,2),u=g[0],f=g[1],b=Object(M.useState)(!1),w=Object(T.a)(b,2),E=w[0],k=w[1],v=Object(M.useState)(null),x=Object(T.a)(v,2),y=x[0],C=x[1];Object(M.useEffect)(function(){t.demo.models&&C(t.demo.models[0])},[]);return Object(M.useEffect)(function(){d&&function(){var e=d.data[0];l(e),f(!1)}()},[d]),j.a.createElement(U,{position:"relative",width:"100%",height:"100%",align:"center",justify:"center"},j.a.createElement(ia,{run:function(e){f(!0),function(e,t,a){var n={data:[e]};console.log(n),fetch(t,{method:"POST",body:JSON.stringify(n),headers:{"Content-Type":"application/json"}}).then(function(e){if(200===e.status)return console.log("hi",e),e.json();console.log(e)}).then(function(e){return console.log("output",e),a(e),e})}(e,t.demo.api,p)},open:a,setOpen:n,darkMode:i}),j.a.createElement(_,{align:"center",justify:"center",width:"100%",height:"100%",padding:"padding-left: ".concat(a?"20%":0)},j.a.createElement(U,{flex:3,width:"100%",height:"100%",align:"center",justify:"center"},s?u?j.a.createElement(Tt,null):j.a.createElement(K,{width:"100%",height:"100%",of:"cover",src:s}):u?j.a.createElement(Tt,null):j.a.createElement(Lt,{text:"Please Input Any words or sentences to Model \ud83d\ude0a"})),u&&j.a.createElement(G,{position:"absolute",style:{right:10,top:10},zIndex:1e3},j.a.createElement(zt,{defaultSec:3}))),t.demo.models&&!u&&j.a.createElement(U,{position:"absolute",shadow:!0,onClick:function(){return k(!E)},bg:Oe(i),padding:"padding: 0px 5px",style:{width:"auto",height:E?45*t.demo.models.length:40,transition:"all 300ms",right:10,top:10,overflow:"hidden"},br:"20px"},j.a.createElement(W,{color:i?"#00dcff":"black",to:"cursor",size:"14",weight:"600",padding:"padding: 10px 15px;"},"Model Selected: ",y),t.demo.models.map(function(e){return j.a.createElement(ra,{padding:"padding: 10px 15px;",to:"cursor",key:e,onClick:function(){return C(e)}},j.a.createElement(W,{size:"14",color:i?"#00dcff":"black"},e))})))},sa=function(e){var t=e.model,a=e.open,n=e.setOpen,i=e.darkMode,r=e.subtype;return j.a.createElement(j.a.Fragment,null,"imagetoimage"==r&&j.a.createElement(aa,{model:t,open:a,setOpen:n,darkMode:i}),"texttoimage"==r&&j.a.createElement(oa,{model:t,open:a,setOpen:n,darkMode:i}))},la=(a(94),a(35)),ca=a.n(la),ma=function(e){var t=e.darkMode;e.img,e.setImg;return j.a.createElement(G,{margin:"margin-bottom: 10px;",to:"cursor"},j.a.createElement("form",null,j.a.createElement("input",{accept:"image/*",style:{display:"none"},type:"file"}),j.a.createElement(G,{br:"20px",width:"100%",height:"100px;",align:"center",justify:"center",style:{border:"2px dashed ".concat(t?"rgb(70, 70, 70)":"rgb(224, 224, 224)")}},j.a.createElement(W,{size:"12",color:t?"rgb(150, 150, 150)":"rgb(124, 124, 124)",style:{textAlign:"center"}},"Drag image file here or click to browse from your device"))))},da=Object(R.c)(G)(x||(x=Object(z.a)(["\n    opacity: ",";\n    :hover {\n        opacity: 1\n    };\n    cursor: pointer;\n    border-radius: 20px;\n    overflow: hidden;\n    width: 100%;\n    height: 100px;\n    margin: 10px 0px;\n"])),function(e){return e.img==e.item?1:.7}),pa=[Bt.a,Ft.a,ca.a,Wt.a],ha=function(e){var t=e.run,a=e.displayImage;return j.a.createElement(U,null,pa.map(function(e,n){return j.a.createElement(da,{onClick:function(){return t(e)},img:a,item:e,key:n},j.a.createElement(K,{width:"100%",height:"100px",of:"cover",src:e}))}))},ga=function(e){var t=e.run,a=e.displayImage,n=e.open,i=(e.setOpen,e.darkMode);e.type;return j.a.createElement(Z,{width:"20%",height:"100%",padding:"padding: 15px;",position:"absolute",bg:Oe(i),style:{left:n?0:"-25%",overflow:"hidden",overflowY:"scroll"}},j.a.createElement(ma,{darkMode:i}),j.a.createElement(ha,{displayImage:a,run:t}))},ua=function(e){var t=e.model,a=e.open,n=e.setOpen,i=e.darkMode,r=Object(M.useState)(),o=Object(T.a)(r,2),s=o[0],l=o[1],c=Object(M.useState)(),m=Object(T.a)(c,2),d=m[0],p=m[1],h=Object(M.useState)(null),g=Object(T.a)(h,2),u=g[0],f=g[1],b=Object(M.useState)(!1),w=Object(T.a)(b,2),E=w[0],k=w[1];return Object(M.useEffect)(function(){u&&function(){var e=u.data[0];2==u.data.length&&p(u.data[1]),l(e),k(!1)}()},[u]),j.a.createElement(U,{position:"relative",width:"100%",height:"100%",align:"center",justify:"center"},j.a.createElement(ga,{run:function(e){k(!0),Yt(e).then(function(e){Rt(e,t.demo.api,f)})},displayImage:s,open:a,setOpen:n,darkMode:i}),j.a.createElement(_,{align:"center",justify:"center",width:"100%",height:"100%",padding:"padding-left: ".concat(a?"20%":0)},j.a.createElement(U,{flex:3,width:"100%",height:"100%",align:"center",justify:"center"},s?E?j.a.createElement(Tt,null):j.a.createElement(_,null,j.a.createElement(K,{flex:1,height:"100%",of:"cover",src:s}),d&&j.a.createElement(K,{flex:1,height:"100%",of:"cover",src:d})):E?j.a.createElement(Tt,null):j.a.createElement(Lt,null)),E&&j.a.createElement(G,{position:"absolute",style:{right:10,top:10},zIndex:1e3},j.a.createElement(zt,{defaultSec:3}))))},fa=a(36),ba=a.n(fa),wa=a(37),Ea=a.n(wa),ka=function(e){var t=e.darkMode,a=(e.img,e.setImg,t?"rgb(150, 150, 150)":"rgb(124, 124, 124)");return j.a.createElement(U,{margin:"margin-bottom: 10px;",to:"cursor"},j.a.createElement("form",null,j.a.createElement("input",{accept:"image/*",style:{display:"none"},type:"file"}),j.a.createElement(U,{br:"20px",width:"100%",height:"100px;",align:"center",justify:"center",style:{border:"2px dashed ".concat(t?"rgb(70, 70, 70)":"rgb(224, 224, 224)")}},j.a.createElement(W,{margin:"margin-bottom: 2.5px;",color:a},"Content Image"),j.a.createElement(W,{margin:"margin-top: 2.5px;",size:"12",color:a,style:{textAlign:"center"}},"Drag image file here or click to browse from your device"))),j.a.createElement("form",null,j.a.createElement("input",{accept:"image/*",style:{display:"none"},type:"file"}),j.a.createElement(U,{br:"20px",width:"100%",height:"100px;",align:"center",margin:"margin-top: 20px;",justify:"center",style:{border:"2px dashed ".concat(t?"rgb(70, 70, 70)":"rgb(224, 224, 224)")}},j.a.createElement(W,{margin:"margin-bottom: 2.5px;",color:a},"Style Image"),j.a.createElement(W,{margin:"margin-top: 2.5px;",size:"12",color:a,style:{textAlign:"center"}},"Drag image file here or click to browse from your device"))))},va=Object(R.c)(G)(y||(y=Object(z.a)(["\n    opacity: ",";\n    :hover {\n        opacity: 1\n    };\n    cursor: pointer;\n    border-radius: 20px;\n    overflow: hidden;\n    width: 100%;\n    height: 100px;\n    margin: 10px 0px;\n"])),function(e){return e.img==e.item?1:.7}),xa=[[ba.a,Ea.a]],ya=function(e){var t=e.run,a=e.displayImage;return j.a.createElement(U,null,xa.map(function(e,n){return j.a.createElement(va,{onClick:function(){return t(e)},item:e,img:a,key:n},j.a.createElement(K,{width:"50%",height:"100px",of:"cover",src:e[0]}),j.a.createElement(K,{width:"50%",height:"100px",of:"cover",src:e[1]}))}))},Ca=function(e){var t=e.run,a=e.displayImage,n=e.open,i=(e.setOpen,e.darkMode);e.type;return j.a.createElement(Z,{width:"20%",height:"100%",padding:"padding: 15px;",position:"absolute",bg:Oe(i),style:{left:n?0:"-25%",overflow:"hidden",overflowY:"scroll"}},j.a.createElement(ka,{darkMode:i}),j.a.createElement(ya,{run:t,displayImage:a}))},Sa=function(e){var t=e.model,a=e.open,n=e.setOpen,i=e.darkMode,r=Object(M.useState)([null,null]),o=Object(T.a)(r,2),s=o[0],l=o[1],c=Object(M.useState)(),m=Object(T.a)(c,2),d=m[0],p=m[1],h=Object(M.useState)(null),g=Object(T.a)(h,2),u=g[0],f=g[1],b=Object(M.useState)(!1),w=Object(T.a)(b,2),E=w[0],k=w[1];return Object(M.useEffect)(function(){u&&function(){var e=u.data[0];p(e),k(!1)}()},[u]),j.a.createElement(U,{position:"relative",width:"100%",height:"100%",align:"center",justify:"center"},j.a.createElement(Ca,{run:function(e){p(e),k(!0),console.log(e),Yt(e[0]).then(function(e){var t=s;t[0]=e,l(t)}),Yt(e[1]).then(function(e){var a=s;a[1]=e,l(a),Rt(s,t.demo.api,f)})},displayImage:d,open:a,setOpen:n,darkMode:i}),j.a.createElement(_,{align:"center",justify:"center",width:"100%",height:"100%",padding:"padding-left: ".concat(a?"20%":0)},j.a.createElement(U,{flex:3,width:"100%",height:"100%",align:"center",justify:"center"},d?E?j.a.createElement(Tt,null):j.a.createElement(K,{width:"100%",height:"100%",of:"cover",src:d}):j.a.createElement(Lt,null)),E&&j.a.createElement(G,{position:"absolute",style:{right:10,top:10},zIndex:1e3},j.a.createElement(zt,{defaultSec:t.demo.defaultSec?t.demo.defaultSec:8}))))},Na=function(e){var t=e.model,a=e.darkMode,n=Object(M.useState)(!0),i=Object(T.a)(n,2),r=i[0],o=i[1],s=t.demo.type.split("/");console.log(s);var l=s[0],c=s[1];return j.a.createElement(U,null,j.a.createElement(_,{align:"center",margin:"margin-bottom: 30px; margin-left: 20px;"},j.a.createElement(ge,{width:23,height:23}),j.a.createElement(W,{className:"bold",size:"22",margin:"margin-left: 12px;"},t.demo.title?t.demo.title:t.title," Demo")),j.a.createElement(G,{shadow:!0,height:"470px",width:"100%",margin:"margin-bottom: 30px;",br:"20px",bg:a?"rgba(30, 30, 30, 0.5)":"rgba(255, 255, 255, 0.5)",style:{overflow:"hidden"}},"classification"==l&&j.a.createElement(Jt,{model:t,open:r,setOpen:o,darkMode:a,subtype:c}),"object-detection"==l&&j.a.createElement(ua,{model:t,open:r,setOpen:o,darkMode:a,subtype:c}),"style-transfer"==l&&j.a.createElement(Sa,{model:t,open:r,setOpen:o,darkMode:a,subtype:c}),"image-generation"==l&&j.a.createElement(sa,{model:t,open:r,setOpen:o,darkMode:a,subtype:c})))},Ia=function(){return j.a.createElement("table",{style:{color:"white"},className:"table-striped"},j.a.createElement("tbody",null,j.a.createElement("tr",null,j.a.createElement("th",null,"Task"),j.a.createElement("th",null,"Dataset"),j.a.createElement("th",null,"Model"),j.a.createElement("th",null,"Metric Name"),j.a.createElement("th",null,"Metric Value"),j.a.createElement("th",null,"Global Rank"),j.a.createElement("th",null,"Result"),j.a.createElement("th",null,"Benchmark")),j.a.createElement("tr",null,j.a.createElement("td",{rowspan:"4",className:"rowspan-td"},"Lesion Segmentation"),j.a.createElement("td",{rowspan:"4",className:"rowspan-td"},"Anatomical Tracings of Lesions After Stroke (ATLAS)"),j.a.createElement("td",{rowspan:"4",className:"rowspan-td model-col"},"U-Net"),j.a.createElement("td",null,"Dice"),j.a.createElement("td",null,"0.4606"),j.a.createElement("td",null,"# 2"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=38102"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/lesion-segmentation-on-anatomical-tracings-of-1"},"Compare")))),j.a.createElement("tr",null,j.a.createElement("td",null,"IoU"),j.a.createElement("td",null,"0.3447"),j.a.createElement("td",null,"# 2"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=38102"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/lesion-segmentation-on-anatomical-tracings-of-1"},"Compare")))),j.a.createElement("tr",null,j.a.createElement("td",null,"Precision"),j.a.createElement("td",null,"0.5994"),j.a.createElement("td",null,"# 1"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=38102"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/lesion-segmentation-on-anatomical-tracings-of-1"},"Compare")))),j.a.createElement("tr",null,j.a.createElement("td",null,"Recall"),j.a.createElement("td",null,"0.4449"),j.a.createElement("td",null,"# 1"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=38102"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/lesion-segmentation-on-anatomical-tracings-of-1"},"Compare")))),j.a.createElement("tr",null,j.a.createElement("td",{rowspan:"2",className:"rowspan-td"},"Retinal Vessel Segmentation"),j.a.createElement("td",{rowspan:"2",className:"rowspan-td"},"CHASE_DB1"),j.a.createElement("td",{rowspan:"2",className:"rowspan-td model-col"},"U-Net"),j.a.createElement("td",null,"F1 score"),j.a.createElement("td",null,"0.7783"),j.a.createElement("td",null,"# 10"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=3244"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/retinal-vessel-segmentation-on-chase_db1"},"Compare")))),j.a.createElement("tr",null,j.a.createElement("td",null,"AUC"),j.a.createElement("td",null,"0.9772"),j.a.createElement("td",null,"# 10"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=3244"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/retinal-vessel-segmentation-on-chase_db1"},"Compare")))),j.a.createElement("tr",null,j.a.createElement("td",{rowspan:"3",className:"rowspan-td"},"Colorectal Gland Segmentation:"),j.a.createElement("td",{rowspan:"3",className:"rowspan-td"},"CRAG"),j.a.createElement("td",{rowspan:"3",className:"rowspan-td model-col"},"U-Net (e)"),j.a.createElement("td",null,"F1-score"),j.a.createElement("td",null,"0.827"),j.a.createElement("td",null,"# 8"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=16702"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/colorectal-gland-segmentation-on-crag"},"Compare")))),j.a.createElement("tr",null,j.a.createElement("td",null,"Dice"),j.a.createElement("td",null,"0.844"),j.a.createElement("td",null,"# 9"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=16702"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/colorectal-gland-segmentation-on-crag"},"Compare")))),j.a.createElement("tr",null,j.a.createElement("td",null,"Hausdorff Distance (mm)"),j.a.createElement("td",null,"196.9"),j.a.createElement("td",null,"# 5"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=16702"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/colorectal-gland-segmentation-on-crag"},"Compare")))),j.a.createElement("tr",null,j.a.createElement("td",{rowspan:"1",className:"rowspan-td"},"Colorectal Gland Segmentation:"),j.a.createElement("td",{rowspan:"1",className:"rowspan-td"},"CRAG"),j.a.createElement("td",{rowspan:"1",className:"rowspan-td model-col"},"U-Net"),j.a.createElement("td",null,"DiceOC"),j.a.createElement("td",null,"0.835"),j.a.createElement("td",null,"# 1"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=40302"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/colorectal-gland-segmentation-on-crag"},"Compare")))),j.a.createElement("tr",null,j.a.createElement("td",{rowspan:"2",className:"rowspan-td"},"Colorectal Gland Segmentation:"),j.a.createElement("td",{rowspan:"2",className:"rowspan-td"},"CRAG"),j.a.createElement("td",{rowspan:"2",className:"rowspan-td model-col"},"FCN8 (e)"),j.a.createElement("td",null,"F1-score"),j.a.createElement("td",null,"0.796"),j.a.createElement("td",null,"# 11"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=16701"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/colorectal-gland-segmentation-on-crag"},"Compare")))),j.a.createElement("tr",null,j.a.createElement("td",null,"Hausdorff Distance (mm)"),j.a.createElement("td",null,"199.5"),j.a.createElement("td",null,"# 4"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=16701"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/colorectal-gland-segmentation-on-crag"},"Compare")))),j.a.createElement("tr",null,j.a.createElement("td",{rowspan:"3",className:"rowspan-td"},"Pancreas Segmentation"),j.a.createElement("td",{rowspan:"3",className:"rowspan-td"},"CT-150"),j.a.createElement("td",{rowspan:"3",className:"rowspan-td model-col"},"U-Net"),j.a.createElement("td",null,"Dice Score"),j.a.createElement("td",null,"0.814"),j.a.createElement("td",null,"# 1"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=3236"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/pancreas-segmentation-on-ct-150"},"Compare")))),j.a.createElement("tr",null,j.a.createElement("td",null,"Precision"),j.a.createElement("td",null,"0.848"),j.a.createElement("td",null,"# 2"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=3236"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/pancreas-segmentation-on-ct-150"},"Compare")))),j.a.createElement("tr",null,j.a.createElement("td",null,"Recall"),j.a.createElement("td",null,"0.806"),j.a.createElement("td",null,"# 2"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=3236"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/pancreas-segmentation-on-ct-150"},"Compare")))),j.a.createElement("tr",null,j.a.createElement("td",{rowspan:"1",className:"rowspan-td"},"Medical Image Segmentation"),j.a.createElement("td",{rowspan:"1",className:"rowspan-td"},"CVC-ClinicDB"),j.a.createElement("td",{rowspan:"1",className:"rowspan-td model-col"},"U-Net"),j.a.createElement("td",null,"mean Dice"),j.a.createElement("td",null,"0.8230"),j.a.createElement("td",null,"# 16"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=19751"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/medical-image-segmentation-on-cvc-clinicdb"},"Compare")))),j.a.createElement("tr",null,j.a.createElement("td",{rowspan:"1",className:"rowspan-td"},"Cell Segmentation"),j.a.createElement("td",{rowspan:"1",className:"rowspan-td"},"DIC-HeLa"),j.a.createElement("td",{rowspan:"1",className:"rowspan-td model-col"},"U-Net"),j.a.createElement("td",null,"Mean IoU"),j.a.createElement("td",null,"0.7756"),j.a.createElement("td",null,"# 1"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=3234"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/cell-segmentation-on-dic-hela"},"Compare")))),j.a.createElement("tr",null,j.a.createElement("td",{rowspan:"1",className:"rowspan-td"},"Medical Image Segmentation"),j.a.createElement("td",{rowspan:"1",className:"rowspan-td"},"ISBI 2012 EM Segmentation"),j.a.createElement("td",{rowspan:"1",className:"rowspan-td model-col"},"U-Net"),j.a.createElement("td",null,"Warping Error"),j.a.createElement("td",null,"0.000353"),j.a.createElement("td",null,"# 1"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=3232"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/medical-image-segmentation-on-isbi-2012-em"},"Compare")))),j.a.createElement("tr",null,j.a.createElement("td",{rowspan:"2",className:"rowspan-td"},"Skin Cancer Segmentation"),j.a.createElement("td",{rowspan:"2",className:"rowspan-td"},"Kaggle Skin Lesion Segmentation"),j.a.createElement("td",{rowspan:"2",className:"rowspan-td model-col"},"U-Net"),j.a.createElement("td",null,"F1 score"),j.a.createElement("td",null,"0.8682"),j.a.createElement("td",null,"# 3"),j.a.createElement("td",{className:"results-icon"},j.a.createElement("a",{href:"/paper/u-net-convolutional-networks-for-biomedical/review/?hl=3250"},j.a.createElement("span",{className:" icon-wrapper icon-ion","data-name":"enter-outline"},j.a.createElement("svg",{xmlns:"http://www.w3.org/2000/svg",width:"51",height:"51",viewBox:"0 0 51 51"},j.a.createElement("path",{d:"M176 176v-40a40 40 0 0 1 40-40h208a40 40 0 0 1 40 40v240a40 40 0 0 1-40 40H216a40 40 0 0 1-40-40v-40",fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32"}),j.a.createElement("path",{fill:"none",stroke:"#000","stroke-linecap":"round","stroke-linejoin":"round","stroke-width":"32",d:"M272 336l80-80-80-80M48 256h288"}))))),j.a.createElement("td",null,j.a.createElement("div",{className:"sota-table-link"},j.a.createElement("a",{className:"btn btn-primary",href:"/sota/skin-cancer-segmentation-on-kaggle-skin"},"Compare"))))))},Aa=Object(R.c)(G)(C||(C=Object(z.a)(["\n    \n"]))),Ma=function(e){e.item;return j.a.createElement(Aa,null)},ja=function(e){var t=e.model;return j.a.createElement(U,null,j.a.createElement(W,{weight:"500",className:"light",size:"22"},"Articles about ",t.title),j.a.createElement(G,{style:{display:"block"}},t.articles.map(function(e){return j.a.createElement(Ma,{key:e.title})})))},Oa=function(e){e.model;var t=e.darkMode;return j.a.createElement(U,{flex:1},j.a.createElement(W,{size:"22",className:"bold",margin:"margin-bottom: 30px;"},"Model Specs"),j.a.createElement(G,{bg:Oe(t),br:"20px;",height:"500px"}))},Da=function(e){e.model;var t=e.darkMode;return j.a.createElement(U,{flex:2.5,margin:"margin-right: 30px;"},j.a.createElement(W,{size:"22",className:"bold",margin:"margin-bottom: 30px;"},"Results from the Paper"),j.a.createElement(G,{br:"20px;",height:"500px;",style:{overflow:"hidden"},bg:Oe(t)},j.a.createElement(Ia,null)))},Ta=function(e){var t=e.model;e.darkMode;return j.a.createElement(_,{margin:"margin-top: 20px; margin-bottom: 50px;"},j.a.createElement(U,{flex:1,margin:"margin-right: 25px;"},j.a.createElement(W,{size:"22",className:"bold",margin:"margin-bottom: 15px;"},"Tasks"),j.a.createElement(G,{style:{display:"block"}},t.task.tasks.map(function(e){return j.a.createElement(W,{margin:"margin-right: 10px;",key:e},e)}))),j.a.createElement(U,{flex:1,margin:"margin-left: 25px;"},j.a.createElement(W,{size:"22",className:"bold",margin:"margin-bottom: 15px;"},"Datasets"),j.a.createElement(G,{style:{display:"block"}},t.task.datasets.map(function(e){return j.a.createElement(W,{margin:"margin-right: 10px;",key:e},e)}))))},La=function(e){var t=e.model,a=e.darkMode;return j.a.createElement(U,{width:"100%",margin:"margin-top: 35px;"},t.demo&&j.a.createElement(Na,{model:t,darkMode:a}),t.articles&&j.a.createElement(ja,{model:t,darkMode:a}),j.a.createElement(Ta,{model:t}),j.a.createElement(_,{width:"100%"},j.a.createElement(Da,{darkMode:a}),j.a.createElement(Oa,{darkMode:a})))},za=function(e){var t=e.darkMode,a=e.setModel,n=e.model,i=e.task,r=e.subTask;return j.a.createElement(U,{width:"100%"},j.a.createElement(G,{onClick:function(){return a(null)},to:"cursor",position:"absolute",style:{left:"9.5%",top:180,opacity:n?1:0,transition:"all 300ms"}},j.a.createElement(fe,{width:20,height:20,stroke:t?"rgba(255, 255, 255, 0.7":"rgba(0, 0, 0, 0.7)"})),j.a.createElement(W,{className:"bold",style:{minHeight:36.67,marginLeft:n?30:0,marginTop:n?3:0,transition:"all 300ms"},size:n?27:"30"},n?n.paper.name:r?r.title.toUpperCase():i?i.title.toUpperCase():"Computer Vision".toUpperCase()))},Ra=function(e){var t=e.darkMode,a=e.model,n=e.subTask,i=e.task;return j.a.createElement(U,null,j.a.createElement(_,{margin:"margin: 20px 0px",opacity:.8},j.a.createElement(W,{className:"bold",weight:"500",size:"16"},i.title),n&&i.title!=n.title&&j.a.createElement(j.a.Fragment,null,j.a.createElement(W,{className:"bold",weight:"500",size:"16",margin:"margin: 0px 10px;"},">"),j.a.createElement(W,{className:"bold",weight:"500",size:"16"},n.title)),a&&j.a.createElement(j.a.Fragment,null,j.a.createElement(W,{className:"bold",weight:"500",size:"16",margin:"margin: 0px 10px;"},">"),j.a.createElement(W,{className:"bold",weight:"500",size:"16"},a.title))),a&&j.a.createElement(U,null,j.a.createElement(W,{style:{opacity:.8},lh:20,className:"light",margin:"margin-bottom: 10px;",weight:"500"},a.paper.conference&&null!=a.paper.conference.name&&j.a.createElement(j.a.Fragment,null,j.a.createElement(W,{size:"14"},a.paper.conference.name),j.a.createElement(W,{margin:"margin: 0px 5px;",size:"14"},"  \xb7  ")),a.paper.published_date&&j.a.createElement(j.a.Fragment,null,j.a.createElement(W,{size:"14"},a.paper.published_date),j.a.createElement(W,{margin:"margin: 0px 5px;",size:"14"},"  \xb7  ")),a.paper.authors.map(function(e,t){return 0!==t?j.a.createElement(W,{key:t,size:"14"},",  "+e):j.a.createElement(W,{key:t,size:"14"},e)})),a.task.featured&&j.a.createElement(W,{size:"15",color:t?"#00dcff":"#1C8BD8",className:"bold",margin:"margin: 10px 0px; margin-top: 10px;",weight:"600"},a.task.featured)))},Va=function(e){var t=e.model,a=e.darkMode;e.subTask,e.task;return j.a.createElement(j.a.Fragment,null,t?j.a.createElement(_,{margin:"margin-top: 30px;"},t.paper.url&&j.a.createElement(j.a.Fragment,null,j.a.createElement("a",{href:"".concat(t.paper.paper.replace("abs","pdf"))+".pdf",target:"_blank"},j.a.createElement(_,{shadow:!0,align:"center",margin:"margin-right: 18px;",padding:"padding: 10px 15px;",br:"10px",bg:Oe(a)},j.a.createElement(de,{stroke:Me,width:20,height:20}),j.a.createElement(W,{size:"12",className:"light",weight:"600",margin:"margin: 0px 10px;"},"Paper"))),j.a.createElement("a",{href:t.paper.paper,target:"_blank"},j.a.createElement(_,{shadow:!0,align:"center",margin:"margin-right: 18px;",padding:"padding: 10px 18px;",br:"10px",bg:Oe(a)},j.a.createElement(me,{stroke:Me,width:20,height:20}),j.a.createElement(W,{size:"12",className:"light",weight:"600",margin:"margin: 0px 10px;"},"Abstract")))),t.paper.conference&&j.a.createElement(j.a.Fragment,null,j.a.createElement("a",{href:t.paper.conference.paper,target:"_blank"},j.a.createElement(_,{shadow:!0,align:"center",margin:"margin-right: 18px;",padding:"padding: 10px 18px;",br:"10px",bg:Oe(a)},j.a.createElement(de,{stroke:Me,width:20,height:20}),j.a.createElement(W,{size:"12",className:"light",weight:"600",margin:"margin: 0px 10px;"},t.paper.conference.name," Paper"))),j.a.createElement("a",{href:t.paper.conference.abstract,target:"_blank"},j.a.createElement(_,{shadow:!0,align:"center",margin:"margin-right: 18px;",padding:"padding: 10px 18px;",br:"10px",bg:Oe(a)},j.a.createElement(me,{stroke:Me,width:20,height:20}),j.a.createElement(W,{size:"12",className:"light",weight:"600",margin:"margin: 0px 10px;"},t.paper.conference.name," Abstract")))),j.a.createElement("a",{href:t.code.url,target:"_blank"},j.a.createElement(_,{shadow:!0,align:"center",margin:"margin-right: 18px;",padding:"padding: 10px 18px;",br:"10px",bg:Oe(a)},j.a.createElement(be,{stroke:Me,width:20,height:20}),j.a.createElement(W,{size:"12",className:"light",weight:"600",margin:"margin: 0px 10px;"},"Code"))),t.code.colab&&j.a.createElement("a",{href:t.code.colab,target:"_blank"},j.a.createElement(_,{shadow:!0,align:"center",margin:"margin-right: 18px;",padding:"padding: 10px 18px;",br:"10px",bg:Oe(a)},j.a.createElement(we,{stroke:Me,width:20,height:20}),j.a.createElement(W,{size:"12",className:"light",weight:"600",margin:"margin: 0px 10px;"},"Colab")))):j.a.createElement(_,{margin:"margin-top: 30px;"},j.a.createElement(_,{align:"center"},j.a.createElement(ge,{width:20,height:20}),j.a.createElement(W,{size:"12",className:"light",weight:"600",margin:"margin: 0px 10px;"},"20 Demos")),j.a.createElement(_,{align:"center",margin:"margin-left: 10px;"},j.a.createElement(ue,{width:20,height:20}),j.a.createElement(W,{size:"12",className:"light",weight:"600",margin:"margin: 0px 10px;"},"20 Papers with Code")),j.a.createElement(_,{align:"center",margin:"margin-left: 10px;"},j.a.createElement(he,{width:20,height:20}),j.a.createElement(W,{size:"12",className:"light",weight:"600",margin:"margin: 0px 10px;"},"20 Benchmarks"))))},Ba=function(e){var t=e.darkMode,a=e.setModel,n=e.setTask,i=e.model,r=e.setSubTask,o=e.task,s=e.subTask;return j.a.createElement(U,null,j.a.createElement(za,{model:i,setModel:a,darkMode:t,task:o,setTask:n,subTask:s,setSubTask:r}),j.a.createElement(_,null,j.a.createElement(U,{margin:"margin-right: 30px;",flex:1},j.a.createElement(Ra,{darkMode:t,task:o,subTask:s,model:i}),j.a.createElement(W,{lh:27,className:"sans",weight:"400"},i?i.paper.description:s?s.description:o.description),j.a.createElement(Va,{subTask:s,task:o,darkMode:t,model:i})),!i&&j.a.createElement(G,{margin:"margin-top: 30px;"},j.a.createElement(G,{br:"10px",width:"300px;",align:"center",justify:"center",height:"180px;",style:{overflow:"hidden"}},j.a.createElement(K,{width:"108%",height:"108%",of:"cover",src:i?i.img:s?s.img:o.img})))))},Pa=function(e){var t=e.model,a=e.setModel,n=e.categoryBar,i=e.darkMode,r=e.task,o=(e.setTask,e.subTask);e.setSubTask;return j.a.createElement(U,{width:"100%",padding:n?"padding: 0% 10%; padding-right: 25%; padding-bottom: 100px;":"padding: 0% 10%; padding-bottom: 100px;"},j.a.createElement(Ba,{setModel:a,model:t,darkMode:i,task:r,subTask:o}),t?j.a.createElement(La,Object(Be.a)({model:t,setModel:a,darkMode:i},"model",t)):j.a.createElement(Dt,{setModel:a,darkMode:i,task:r,subTask:o}))},Fa=(a(95),{title:"Image Denoising",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000714-068a8901_2PQwzdm.jpg",imgcredit:{name:"Wide Inference Network for Image Denoising via Learning Pixel-distribution Prior",url:"https://arxiv.org/pdf/1707.05414v5.pdf"},description:"Image Denoising is the task of removing noise from an image, e.g. the application of Gaussian noise to an image.",datasets:["FFHQ","SIDD","CBSD68","DND","FMD","PolyU Dataset","CRVD","NIND"],models:[]}),Ha={title:"Salt-And-Pepper Noise Removal",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000074-57d5ad86.jpg",imgcredit:{name:"NAMF",url:"https://arxiv.org/pdf/1910.07787v1.pdf"},description:"Salt-and-pepper noise is a form of noise sometimes seen on images. It is also known as impulse noise. This noise can be caused by sharp and sudden disturbances in the image signal. It presents itself as sparsely occurring white and black pixels.",datasets:["BSD"],models:[]},Wa=[],Ga={title:"Denoising",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000813-398eae41_lgoZLWN.jpg",imgcredit:{name:"Beyond a Gaussian Denoiser",url:"https://arxiv.org/pdf/1608.03981v1.pdf"},description:"Denoising is the task of removing noise from an image.",datasets:["SIDD","CBSD68","BirdSong","PolyU Dataset","FMD","Raider","CRVD","NIND","TTS-Portuguese Corpus","PointDenoisingBenchmark"],subtasks:[Fa,Ha],models:Wa.concat(Fa.models,Ha.models)},_a=Ga,Ua={title:"3D Depth Estimation",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000603-3510e464.jpg",imgcredit:{name:"Monodepth2",url:"https://github.com/nianticlabs/monodepth2"},description:"",datasets:["Pano3D","EUEN17037_Daylight_and_View_Standard_TestDataSet","DurLAR"],models:[]},Za=a(38),Ka=a.n(Za),Ya={title:"Depth And Camera Motion",img:Ka.a,imgcredit:{name:"Depth Prediction Without the Sensors: Leveraging Structure for Unsupervised Learning from Monocular Videos",url:"https://arxiv.org/pdf/1811.06152v1.pdf"},description:"",datasets:["Face Anti-Spoofing"],models:[]},Ja=Ya,qa={title:"Monocular Depth Estimation",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000893-e0a158a0.gif",imgcredit:{name:"Defocus Deblurring Using Dual-Pixel Data",url:"https://arxiv.org/abs/2005.00305"},description:"The Monocular Depth Estimation is the task of estimating scene depth using a single image.",datasets:["KITTI","NYUv2","Make3D","Middlebury 2014","DIODE","ReDWeb","MannequinChallenge","IBims-1","WSVD","3D60"],subtasks:[],models:[]},Xa={title:"Stereo Depth Estimation",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000605-d9849a91.jpg",imgcredit:{name:"DIODE: A Dense Indoor and Outdoor DEpth Dataset",url:"https://arxiv.org/abs/1908.00463"},descriptioncredit:{name:"Depth Estimation using Monocular and Stereo Cues",url:"https://www.cs.cornell.edu/~asaxena/learningdepth/DepthEstimation_Saxena_IJCAI.pdf"},description:"Depth estimation in computer vision and robotics is most commonly done via stereo vision (stereop- sis), in which images from two cameras are used to triangulate and estimate distances.",datasets:["KITTI"],models:[]},Qa={title:"Stereo-LiDAR Fusion",img:"https://zswang666.github.io/Stereo-LiDAR-CCVNorm-Project-Page/resources/teaser_gif.gif",imgcredit:{name:"3D LiDAR and Stereo Fusion using Stereo Matching Network with Conditional Cost Volume Normalization",url:"https://zswang666.github.io/Stereo-LiDAR-CCVNorm-Project-Page/"},description:"Depth estimation using stereo cameras and a LiDAR sensor.",datasets:[],models:[]},$a=[],en={title:"Depth Estimation",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000605-d9849a91.jpg",imgcredit:{name:"DIODE: A Dense Indoor and Outdoor DEpth Dataset",url:"https://arxiv.org/abs/1908.00463"},description:"Depth Estimation is a crucial step towards inferring scene geometry from 2D images. The goal in monocular Depth Estimation is to predict the depth value of each pixel, given only a single RGB image as input.",datasets:["Cityscapes","KITTI","NYUv2","ScanNet","Matterport3D","SUNCG","Middlebury","TUM RGB-D","Make3D","Virtual KITTI"],subtasks:[Xa,qa,Ua,Ja,Qa],models:$a.concat(Xa.models,qa.models,Ua.models,Ja.models,Qa.models)},tn=en,an=a(39),nn=a.n(an),rn={title:"Few-Shot Image Classification",img:"https://production-media.paperswithcode.com/thumbnails/task/ba39ea79-54cf-4ea2-9158-457ddebaa108.jpg",imgcredit:{name:"Learning Embedding Adaptation for Few-Shot Learning",url:"https://github.com/Sha-Lab/FEAT"},description:"Few-shot image classification is the task of doing image classification with only a few examples for each category (typically < 6 examples).",datasets:["ImageNet","CIFAR-100","CUB-200-2011","Oxford 102 Flower","Caltech-256","Stanford Cars","AwA","tieredImageNet","iNaturalist","AwA2"],models:[]},on=a(40),sn=a.n(on),ln={title:"Few-Shot Semantic Segmentation",img:sn.a,imgcredit:{name:"Cost Aggregation Is All You Need for Few-Shot Segmentation",url:"https://arxiv.org/pdf/2112.11685v1.pdf"},descriptioncredit:{name:"Cost Aggregation Is All You Need for Few-Shot Segmentation",url:"https://arxiv.org/pdf/2112.11685v1.pdf"},description:"few-shot segmentation task has been introduced to address this, where only a handful of support samples are provided to make a mask prediction for a query, which mitigates the reliance on the labeled data.",datasets:["PASCAL-5i","FSS-1000"],models:[]},cn=ln,mn={title:"One-Shot Learning",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000154-e322d1df.jpg",imgcredit:{name:"Siamese Neural Networks for One-shot Image Recognition",url:"https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf"},description:"One-shot learning is the task of learning information about object categories from a single training example.",datasets:["MNIST","TopLogo-10","TACO"],models:[]},dn=[],pn={title:"Few Shot Learning",img:nn.a,imgcredit:{name:"Cost Aggregation Is All You Need for Few-Shot Segmentation",url:"https://arxiv.org/pdf/2112.11685v1.pdf"},datasets:["ImageNet","GLUE","CUB-200-2011","SST","MRPC","PASCAL-5i","Meta-Dataset","MR","Paris-Lille-3D","SUN397"],description:j.a.createElement("p",null,j.a.createElement("strong",null,"Few-Shot Learning")," is an example of meta-learning, where a learner is trained on several related tasks, during the meta-training phase, so that it can generalize well to unseen (but related) tasks with just few examples, during the meta-testing phase. An effective approach to the Few-Shot Learning problem is to learn a common representation for various tasks and train task specific classifiers on top of this representation."),descriptioncredit:{name:"Penalty Method for Inversion-Free Deep Bilevel Optimization",url:"https://arxiv.org/abs/1911.03432"},subtasks:[rn,cn,mn],models:dn.concat(rn.models,cn.models,mn.models)},hn=pn,gn=a(41),un=a.n(gn),fn={title:"Knowledge Distillation",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000951-52325f45_O0tAMly.jpg",description:"Knowledge distillation is the process of transferring knowledge from a large model to a smaller one. While large models (such as very deep neural networks or ensembles of many models) have higher knowledge capacity than small models, this capacity might not be fully utilized.",datasets:["ImageNet"],models:[]},bn={title:"Self-Supervised Image Classification",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000001822-74049cbf_Ww04ZD0.jpg",description:j.a.createElement("p",null,"This is the task of image classification using representations learnt with self-supervised learning. Self-supervised methods generally involve a pretext task that is solved to learn a good representation and a loss function to learn with. One example of a loss function is an autoencoder based loss where the goal is reconstruction of an image pixel-by-pixel. A more popular recent example is a contrastive loss, which measure the similarity of sample pairs in a representation space, and where there can be a varying target instead of a fixed target to reconstruct (as in the case of autoencoders).",j.a.createElement("br",null)),datasets:["ImageNet"],models:[]},wn=bn,En={title:"Sequential Image Classification",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000961-6da1b793_turJtnf.jpg",description:"Sequential image classification is the task of classifying a sequence of images.",datasets:["CIFAR-10","MNIST"],models:[]},kn={title:"Semi-Supervised Image Classification",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000001329-ea5992e0_Tmg8Zxv.jpg",description:j.a.createElement("p",null,"Semi-supervised image classification leverages unlabelled data as well as labelled data to increase classification performance. ",j.a.createElement("br",null),"You may want to read some blog posts to get an overview before reading the papers and checking the leaderboards:",j.a.createElement("ul",null,j.a.createElement("li",null,j.a.createElement("a",{href:"https://ruder.io/semi-supervised/"},"An overview of proxy-label approaches for semi-supervised learning")," - Sebastian Ruder"),j.a.createElement("li",null,j.a.createElement("a",{href:"https://amitness.com/2020/07/semi-supervised-learning/"},"Semi-Supervised Learning in Computer Vision")," - Amit Chaudhary"))),datasets:["CIFAR-10","ImageNet","MNIST","CIFAR-100","SVHN","CelebA","Fashion-MNIST","CUB-200-2011","STL-10","Places"],models:[]},vn={title:"Fine-Grained Image Classification",img:"https://production-media.paperswithcode.com/thumbnails/task/92cea4dd-25d0-4478-a4de-e17f4bebb32d.jpg",description:"The Fine-Grained Image Classification task focuses on differentiating between hard-to-distinguish object classes, such as species of birds, flowers, or animals; and identifying the makes or models of vehicles.",datasets:["MNIST","CUB-200-2011","STL-10","Oxford 102 Flower","Caltech-101","Stanford Cars","Food-101","iNaturalist","EMNIST","FGVC-Aircraft"],models:[]},xn={title:"Small Data Image Classification",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000951-52325f45_O0tAMly.jpg",description:"Supervised image classification with tens to hundreds of labeled training examples.",datasets:["CIFAR-10","ImageNet","CIFAR-100","CUB-200-2011","UCF-Crime","Ecoli","DEIC Benchmark","TMED","WikiChurches","ImageNet 50 samples per class"],models:[]},yn={title:"Few-Shot Image Classification",img:"https://production-media.paperswithcode.com/thumbnails/task/ba39ea79-54cf-4ea2-9158-457ddebaa108.jpg",description:"Few-shot image classification is the task of doing image classification with only a few examples for each category (typically < 6 examples).",datasets:["ImageNet","CIFAR-100","CUB-200-2011","Oxford 102 Flower","Caltech-256","Stanford Cars","AwA","tieredImageNet","iNaturalist","AwA2"],models:[]},Cn=[{writer_id:"Neuralverse",title:"ConvNeXt",subtitle:"A ConvNet for the 2020s",img:"https://production-media.paperswithcode.com/thumbnails/papergithubrepo/f50e5cde-ed50-4c24-96c0-656a1abb227a.jpg",demo:{type:"classification",api:"https://hf.space/gradioiframe/akhaliq/convnext/+/api/predict/",defaultSec:3},benchmark:!0,studio:!0,paper:{name:"A ConvNet for the 2020s",img:"https://production-media.paperswithcode.com/thumbnails/papergithubrepo/f50e5cde-ed50-4c24-96c0-656a1abb227a.jpg",description:'The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.',authors:["Zhuang Liu","Hanzi Mao","Chao-yuan Wu","Christoph Feichtenhofer","Trevor Darrell","Saining Xie"],paper:"https://arxiv.org/abs/2201.03545v1",published_date:"10 Jan 2022",conference:{}},code:{url:"https://github.com/facebookresearch/ConvNeXt",lib:"pytorch",official:!0,colab:"https://colab.research.google.com/drive/1CBYTIZ4tBMsVL5cqu9N_-Q3TBprqsfEO?usp=sharing"},task:{featured:"Ranked #1 on Domain Generalization on ImageNet-Sketch (using extra training data)",tasks:["Domain Generalization","Image Classification","Object Detection","Semantic Segmentation"],datasets:["ImageNet","COCO","ADE20K","ImageNet-C","ImageNet-A","ImageNet-R","ImageNet-Sketch"],paperswithcode:"https://paperswithcode.com/paper/a-convnet-for-the-2020s"}},{writer_id:"Neuralverse",title:"ViT",subtitle:"Transformers for Image Recognition at Scale",img:"https://1.bp.blogspot.com/-_mnVfmzvJWc/X8gMzhZ7SkI/AAAAAAAAG24/8gW2AHEoqUQrBwOqjhYB37A7OOjNyKuNgCLcBGAsYHQ/s1600/image1.gif",imgcredit:{name:"Vision Transformer",url:"https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html"},demo:{type:"classification",api:"https://hf.space/gradioiframe/abidlabs/vision-transformer/+/api/predict/",defaultSec:3},benchmark:!0,studio:!0,paper:{name:"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",img:"https://1.bp.blogspot.com/-_mnVfmzvJWc/X8gMzhZ7SkI/AAAAAAAAG24/8gW2AHEoqUQrBwOqjhYB37A7OOjNyKuNgCLcBGAsYHQ/s1600/image1.gif",description:"While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.",authors:["Alexey Dosovitskiy","Lucas Beyer","Alexander Kolesnikov","Dirk Weissenborn","Xiaohua Zhai","Thomas Unterthiner","Mostafa Dehghani","Matthias Minderer","Georg Heigold","Sylvain Gelly","Jakob Uszkoreit, Neil Houlsby"],paper:"https://arxiv.org/abs/2010.11929v2",published_date:"22 Oct 2020",conference:{name:"ICLR 2021",paper:"https://openreview.net/pdf?id=YicbFdNTTy",abstract:"https://openreview.net/forum?id=YicbFdNTTy"}},code:{url:"https://github.com/google-research/vision_transformer",lib:"tensorflow",official:!0,colab:"https://colab.research.google.com/github/google-research/vision_transformer/blob/master/vit_jax.ipynb"},task:{featured:"Ranked #1 on Fine-Grained Image Classification on Oxford 102 Flowers (Top 1 Accuracy metric, using extra training data)",tasks:["Document Image Classification","Fine-Grained Image Classification","Image Classification"],datasets:["CIFAR-10","ImageNet","CIFAR-100","Oxford 102 Flower","Tiny ImageNet","JFT-300M","Oxford-IIIT Pets"],paperswithcode:"https://paperswithcode.com/paper/an-image-is-worth-16x16-words-transformers-1"}}],Sn={title:"Image Classification",img:un.a,description:j.a.createElement("p",null,j.a.createElement("strong",null,"Image Classification")," is a fundamental task that attempts to comprehend an entire image as a whole. The goal is to classify the image by assigning it to a specific label. Typically, Image Classification refers to images in which only one object appears and is analyzed. In contrast, object detection involves both classification and localization tasks, and is used to analyze more realistic cases in which multiple objects may exist in an image."),datasets:["CIFAR-10","ImageNet","MNIST","CIFAR-100","SVHN","CelebA","Fashion-MNIST","CUB-200-2011","STL-10","Places"],subtasks:[fn,wn,En,kn,vn,xn,yn],models:Cn.concat(fn.models,wn.models,En.models,kn.models,vn.models,xn.models,yn.models)},Nn={title:"Conditional Image Generation",img:"https://raw.githubusercontent.com/openai/pixel-cnn/master/data/pixelcnn_samples.png",imgcredit:{name:"PixelCNN++",url:"https://github.com/openai/pixel-cnn"},description:"Conditional image generation is the task of generating new images from a dataset conditional on their class.",datasets:["CIFAR-10","ImageNet","COCO","CIFAR-100","Large Labelled Logo Dataset (L3D)"],models:[]},In={title:"Face Generation",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000990-2d591218_2jUWb8G.jpg",imgcredit:{name:"Progressive Growing of GANs for Improved Quality, Stability, and Variation",url:"https://arxiv.org/pdf/1710.10196v3.pdf"},description:j.a.createElement("p",null,"Face generation is the task of generating (or interpolating) new faces from an existing dataset. ",j.a.createElement("br",null),"The state-of-the-art results for this task are located in the Image Generation parent."),datasets:["iFakeFaceDB","VGGFace2 HQ"],models:[]},An={title:"Image-to-Image Translation",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000007-8f43e9db_P7gy9yG.jpg",imgcredit:{name:"Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks",url:"https://arxiv.org/pdf/1703.10593v6.pdf"},description:"Image-to-image translation is the task of taking images from one domain and transforming them so they have the style (or characteristics) of images from another domain.",datasets:["Cityscapes","KITTI","CelebA-HQ","ADE20K","SYNTHIA","GTA5","DeepFashion","Perceptual Similarity","COCO-Stuff","Foggy Cityscapes"],models:[]},Mn={title:"Text-to-Image Generation",img:"https://production-media.paperswithcode.com/thumbnails/task/2689c63a-5442-4934-861a-a1d76e2a0838.jpg",imgcredit:{name:"StackGAN++: Realistic Image Synthesis with Stacked Generative Adversarial Networks",url:"https://arxiv.org/pdf/1710.10916v3.pdf"},descriptioncredit:{name:"FuseDream: A Hands-On Tutorial on This Text-to-Image Generation Tool",url:"https://analyticsindiamag.com/fusedream-a-hands-on-tutorial-on-this-text-to-image-generation-tool/#:~:text=Text%2Dto%2Dimage%20generation%2C,they%20should%20be%20semantically%20related."},description:"Text-to-image generation, which generates realistic images that are semantically related to a given text input, is a landmark task in multi-modal machine learning. This is a difficult task because the generative model must comprehend the text, image and how they should be semantically related.",datasets:["COCO","CUB-200-2011","Oxford 102 Flower","100DOH","Multi-Modal CelebA-HQ"],models:[{writer_id:"Neuralverse",title:"DALL_E",subtitle:"Creating Images from Text",img:"https://cdn.openai.com/research-covers/dall-e/2x-no-mark.jpg",imgcredit:{name:"DALL\xb7E: Creating Images from Text",url:"https://openai.com/blog/dall-e/"},paper:{name:"Zero-Shot Text-to-Image Generation",paper:"https://arxiv.org/abs/2102.12092",img:"https://cdn.openai.com/research-covers/dall-e/2x-no-mark.jpg",description:"Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.",authors:["Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever"],published_date:"24 Feb 2021"},benchmark:!1,studio:!1,code:{url:"https://github.com/openai/DALL-E",lib:"pytorch",official:!0,colab:"https://colab.research.google.com/gist/afiaka87/b29213684a1dd633df20cab49d05209d/train_dalle_pytorch.ipynb"},task:{tasks:["Image Generation","Text-to-Image Generation","Zero-Shot Text-to-Image Generation"],datasets:["ImageNet","COCO","Conceptual Captions","YFCC100M"]},articles:[{title:"Title"}]}]},jn={title:"Image Inpainting",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000709-54774675.jpg",imgcredit:{name:"High-Resolution Image Inpainting with Iterative Confidence Feedback and Guided Upsampling",url:"https://arxiv.org/pdf/2005.11742.pdf"},description:j.a.createElement("p",null,j.a.createElement("strong",null,"Image Inpainting")," is a task of reconstructing missing regions in an image. It is an important problem in computer vision and an essential functionality in many imaging and graphics applications, e.g. object removal, image restoration, manipulation, re-targeting, compositing, and image-based rendering."),datasets:["ImageNet","Places","FFHQ","CelebA-HQ","ApolloScape","Places365","Fashion-Gen","CASIA V2","FVI","FDF"],models:[]},On=a(21),Dn=a.n(On),Tn=[{writer_id:"Neuralverse",title:"ArcaneGAN",img:"https://user-images.githubusercontent.com/11751592/144984243-6387263b-0827-478a-ac1e-1ee93f9ddec6.jpg",imgcredit:{name:"https://github.com/Sxela/ArcaneGAN",url:"https://github.com/Sxela/ArcaneGAN"},demo:{type:"image-generation/imagetoimage",api:"https://hf.space/gradioiframe/akhaliq/ArcaneGAN/+/api/predict/",defaultSec:10,models:["version 0.2","version 0.3","version 0.4"]},paper:{type:"nopaper",name:"ArcaneGAN",description:"",authors:["Alex Spirin"]},benchmark:!1,studio:!1,code:{url:"https://github.com/Sxela/ArcaneGAN",lib:"pytorch",official:!0,colab:"https://colab.research.google.com/drive/1r1hhciakk5wHaUn1eJk7TP58fV9mjy_W"},task:{tasks:["Image Generation"],datasets:[]}},{writer_id:"Neuralverse",title:"JoJoGAN",subtitle:"One Shot Face Stylization",img:Dn.a,demo:{type:"image-generation/imagetoimage",api:"https://hf.space/gradioiframe/akhaliq/JoJoGAN/+/api/predict/",defaultSec:10,models:["JoJo","Disney","Jinx","Caitlyn","Yasuho","Arcane Multi","Art","Spider-Verse"]},benchmark:!1,studio:!1,paper:{name:"JoJoGAN: One Shot Face Stylization",img:Dn.a,description:"A style mapper applies some fixed style to its input images (so, for example, taking faces to cartoons). This paper describes a simple procedure -- JoJoGAN -- to learn a style mapper from a single example of the style. JoJoGAN uses a GAN inversion procedure and StyleGAN's style-mixing property to produce a substantial paired dataset from a single example style. The paired dataset is then used to fine-tune a StyleGAN. An image can then be style mapped by GAN-inversion followed by the fine-tuned StyleGAN. JoJoGAN needs just one reference and as little as 30 seconds of training time. JoJoGAN can use extreme style references (say, animal faces) successfully. Furthermore, one can control what aspects of the style are used and how much of the style is applied. Qualitative and quantitative evaluation show that JoJoGAN produces high quality high resolution images that vastly outperform the current state-of-the-art.",authors:["Min Jin Chong","David Forsyth"],paper:"https://arxiv.org/abs/2112.11641v2",published_date:"22 Dec 2021"},code:{url:"https://github.com/mchong6/JoJoGAN",lib:"pytorch",official:!0,colab:"https://colab.research.google.com/github/mchong6/JoJoGAN/blob/main/stylize.ipynb"},task:{tasks:["Image Stylization","One-Shot Face Stylization"],datasets:[],paperswithcode:"https://paperswithcode.com/paper/jojogan-one-shot-face-stylization-1"}}],Ln={title:"Image Generation",img:"https://raw.githubusercontent.com/openai/pixel-cnn/master/data/pixelcnn_samples.png",imgcredit:{name:"PixelCNN++",url:"https://github.com/openai/pixel-cnn"},description:j.a.createElement("p",null,"Image generation (synthesis) is the task of generating new images from an existing dataset.",j.a.createElement("ul",null,j.a.createElement("li",null,"Unconditional generation refers to generating samples unconditionally from the dataset, i.e. "),j.a.createElement("li",null,j.a.createElement("a",{href:"https://neuralverse.us/space/conditional-image-generation"},"Conditional image generation")," (subtask) refers to generating samples conditionally from the dataset, based on a label, i.e. .")),"In this section, you can find state-of-the-art leaderboards for unconditional generation. For conditional generation, and other types of image generations, refer to the subtasks."),datasets:["CIFAR-10","ImageNet","MNIST","CIFAR-100","Cityscapes","CelebA","Fashion-MNIST","CUB-200-2011","STL-10","LSUN"],subtasks:[An,jn,Nn,In,Mn],models:Tn.concat(An.models,jn.models,Nn.models,In.models,Mn.models)},zn=a(42),Rn=a.n(zn),Vn=a(43),Bn={title:"Real-Time Object Detection",img:"https://miro.medium.com/max/3600/1*QOGcvHbrDZiCqTG6THIQ_w.png",imgcredit:"https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088",description:"Real-time object detection is the task of doing object detection in real-time with fast inference while maintaining a base level of accuracy.",datasets:["COCO","PASCAL VOC","PASCAL VOC 2007","Kvasir-SEG","Kvasir","Kvasir-Instrument","Endotect Polyp Segmentation Challenge Dataset","Hyper-Kvasir Dataset"],models:[{title:"YOLOv3",subtitle:"An Incremental Improvement",img:a.n(Vn).a,demo:{type:"object-detection",api:"https://hf.space/gradioiframe/akhaliq/yolov3/+/api/predict/",defaultSec:5},benchmark:!1,studio:!1,paper:{name:"YOLOv3: An Incremental Improvement",img:"",description:"We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/",authors:["Joseph Redmon","Ali Farhadi"],paper:"https://arxiv.org/pdf/1804.02767v1",published_date:"8 Apr 2018"},code:{url:"https://github.com/AlexeyAB/darknet",official:!0,lib:""},task:{tasks:["Object Detection","Real-Time Object Detection"],datasets:["COCO"],paperswithcode:""}}]},Pn={title:"Few-Shot Object Detection",img:"https://production-media.paperswithcode.com/thumbnails/task/79abe44f-f4d3-493a-a585-479607196833.jpg",imgcredit:"https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088",description:'Target: To detect objects of novel categories with just a few training samples. A clear explanation of the few-shot object detection task and its differences with few-shot classification can be found in "A Survey of Self-Supervised and Few-Shot Object Detection": https://gabrielhuang.github.io/fsod-survey/',datasets:["COCO","LVIS","FSOD","LOGO-Net"],models:[]},Fn={title:"RGB-D Salient Object Detection",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000742-bab03b67.jpg",imgcredit:{name:"Attentive Feedback Network for Boundary-Aware Salient Object Detection",url:"https://openaccess.thecvf.com/content_CVPR_2019/papers/Feng_Attentive_Feedback_Network_for_Boundary-Aware_Salient_Object_Detection_CVPR_2019_paper.pdf"},description:"RGB Salient object detection is a task-based on a visual attention mechanism, in which algorithms aim to explore objects or regions more attentive than the surrounding areas on the scene or RGB images.",datasets:["PASCAL-S","HKU-IS","DUTS","DUT-OMRON","ISTD","SOC","ECSSD","HS-SOD","VT5000","Salient Object Subitizing Dataset"],models:[]},Hn={title:"RGB Salient Object Detection",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000001687-6a8a8e68_pk6rhSI.jpg",imgcredit:{name:"Rethinking RGB-D Salient Object Detection: Models, Data Sets, and Large-Scale Benchmarks, TNNLS20",url:"https://ieeexplore.ieee.org/abstract/document/9107477"},description:"RGB-D Salient object detection (SOD) aims at distinguishing the most visually distinctive objects or regions in a scene from the given RGB and Depth data. It has a wide range of applications, including video/image segmentation, object recognition, visual tracking, foreground maps evaluation, image retrieval, content-aware image editing, information discovery, photosynthesis, and weakly supervised semantic segmentation. Here, depth information plays an important complementary role in finding salient objects. Online benchmark: http://dpfan.net/d3netbenchmark.",datasets:["NLPR","LFSD","NJU2K","SIP","DES","ReDWeb-S"],models:[]},Wn={title:"Video Object Detection",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000745-802706d9.jpg",imgcredit:{name:"Learning Motion Priors for Efficient Video Object Detection",url:"https://arxiv.org/pdf/1911.05253v1.pdf"},description:"Video object detection is the task of detecting objects from a video as opposed to images.",datasets:["DAVIS","DAVIS 2017","YT-BB","SYNTHIA-AL","OAK","THGP"],models:[]},Gn={title:"3D Object Detection",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000785-645bd197.jpg",imgcredit:{name:"AVOD",url:"https://github.com/kujason/avod"},description:"2D object detection classifies the object category and estimates oriented 2D bounding boxes of physical objects from 3D sensor data.",datasets:["KITTI","NYUv2","ScanNet","nuScenes","SUN RGB-D","S3DIS","Argoverse","Waymo Open Dataset","H3D","A*3D"],models:[{writer_id:"Neuralverse",title:"Objectron",subtitle:"A Large Scale Dataset of Object-Centric Videos in the Wild with Pose Annotations",img:"https://github.com/google-research-datasets/Objectron/blob/master/docs/images/objectron_samples.gif?raw=true",imgcredit:{name:"Objectron",url:"https://github.com/google-research-datasets/Objectron"},demo:!1,benchmark:!0,studio:!0,paper:{name:"Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild with Pose Annotations",img:"https://github.com/google-research-datasets/Objectron/blob/master/docs/images/objectron_samples.gif?raw=true",description:"3D object detection has recently become popular due to many applications in robotics, augmented reality, autonomy, and image retrieval. We introduce the Objectron dataset to advance the state of the art in 3D object detection and foster new research and applications, such as 3D object tracking, view synthesis, and improved 3D shape representation. The dataset contains object-centric short videos with pose annotations for nine categories and includes 4 million annotated images in 14,819 annotated videos. We also propose a new evaluation metric, 3D Intersection over Union, for 3D object detection. We demonstrate the usefulness of our dataset in 3D object detection tasks by providing baseline models trained on this dataset. Our dataset and evaluation source code are available online at http://www.objectron.dev",authors:["Adel Ahmadyan","Liangkai Zhang","Jianing Wei","Artsiom Ablavatski","Matthias Grundmann"],paper:"https://arxiv.org/abs/2012.09988v1",published_date:"18 Dec 2020",conference:{name:"CVPR 2021",paper:"https://openaccess.thecvf.com/content/CVPR2021/papers/Ahmadyan_Objectron_A_Large_Scale_Dataset_of_Object-Centric_Videos_in_the_CVPR_2021_paper.pdf",abstract:"https://openaccess.thecvf.com/content/CVPR2021/html/Ahmadyan_Objectron_A_Large_Scale_Dataset_of_Object-Centric_Videos_in_the_CVPR_2021_paper.html"}},code:{url:"https://github.com/google-research-datasets/Objectron",lib:"tensorflow",official:!0,colab:""},task:{featured:"Ranked #2 on Monocular 3D Object Detection on Google Objectron",tasks:["3D Object Detection","3D Object Tracking","3D Shape Representation","Image Retrieval","Monocular 3D Object Detection","Object Detection","Object Tracking"],datasets:[" Objectron","ImageNet","ShapeNet","LINEMOD"],paperswithcode:"https://paperswithcode.com/paper/objectron-a-large-scale-dataset-of-object"}}]},_n=[{writer_id:"Neuralverse",title:"DETR",subtitle:"End-to-End Object Detection with Transformers",img:"https://alcinos.github.io/detr_page/assets/elephants.png",imgcredit:{name:"DETR: End-to-End Object Detection With Transformers",url:"https://alcinos.github.io/detr_page/"},demo:{type:"object-detection",api:"https://hf.space/gradioiframe/akhaliq/DETR/+/api/predict/",defaultSec:3},benchmark:!0,studio:!0,paper:{name:"End-to-End Object Detection with Transformers",img:"https://alcinos.github.io/detr_page/assets/overview.jpg",description:"We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.",authors:["Nicolas Carion","Francisco Massa","Gabriel Synnaeve","Nicolas Usunier","Alexander Kirillov, Sergey Zagoruyko"],paper:"https://arxiv.org/abs/2005.12872v3",published_date:"28 May 2020",conference:{name:"ECCV 2020",paper:"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460205.pdf",abstract:"https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/832_ECCV_2020_paper.php"}},code:{url:"https://github.com/facebookresearch/detr",lib:"torch",official:!0,colab:"https://colab.research.google.com/github/facebookresearch/detr/blob/colab/notebooks/detr_demo.ipynb"},task:{featured:"Ranked #7 on Panoptic Segmentation on COCO minival",tasks:["Object Detection","Panoptic Segmentation"],datasets:["COCO"],paperswithcode:"https://paperswithcode.com/paper/end-to-end-object-detection-with-transformers"}}],Un={title:"Object Detection",img:Rn.a,description:"Object detection is the task of detecting instances of objects of a certain class within an image. The state-of-the-art methods can be categorized into two main types: one-stage methods and two stage-methods. One-stage methods prioritize inference speed, and example models include YOLO, SSD and RetinaNet. Two-stage methods prioritize detection accuracy, and example models include Faster R-CNN, Mask R-CNN and Cascade R-CNN. The most popular benchmark is the MSCOCO dataset. Models are typically evaluated according to a Mean Average Precision metric.",datasets:["COCO","KITTI","Visual Genome","nuScenes","MPII","SUN RGB-D","PASCAL VOC","PASCAL3D+","DAVIS 2016","LabelMe"],subtasks:[Bn,Gn,Pn,Fn,Hn,Wn],models:_n.concat(Bn.models,Gn.models,Pn.models,Fn.models,Hn.models,Wn.models)},Zn=a(44),Kn={title:"3D Segmentation",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000001516-2ef108ab_EO6cyBl.jpg",description:"With 3D image segmentation, data acquired from 3D imaging modalities such as Computed Tomography (CT), Micro-Computed Tomography (micro-CT or X-ray) or Magnetic Resonance Imaging (MRI) scanners is labelled to isolate regions of interest. These regions represent any subject or sub-region within the scan that will later be scrutinized. This could facilitate analysis of part of the human body, or a specific feature within an industrial component or assembly.",datasets:["nuScenes","S3DIS","SemanticKITTI","PartNet","SensatUrban","SemanticPOSS","Paris-Lille-3D","RELLIS-3D","3RScan","KiTS19"],models:[]},Yn={title:"Panoptic Segmentation",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000895-b36c6778.jpg",description:"Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance).",datasets:["COCO","Cityscapes","KITTI","ScanNet","SemanticKITTI","BDD100K","Mapillary Vistas Dataset","Cityscapes Panoptic Parts","Pascal Panoptic Parts","Panoptic nuScenes"],models:[]},Jn={title:"Scene Segmentation",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000883-587b0e4d.jpg",description:"Scene segmentation is the task of splitting a scene into its various object components.",datasets:["NYUv2","ScanNet","SUN RGB-D","Berkeley DeepDrive Video","Mila Simulated Floods"],models:[]},qn={title:"Medical Segmentation",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000876-6fbe75a2_gBlYteG.jpg",description:"Semantic segmentation, or image segmentation, is the task of clustering parts of an image together which belong to the same object class. It is a form of pixel-level prediction because each pixel in an image is classified according to a category. Some example benchmarks for this task are Cityscapes, PASCAL VOC and ADE20K. Models are usually evaluated with the Mean Intersection-Over-Union (Mean IoU) and Pixel Accuracy metrics.",datasets:["DRIVE","Medical Segmentation Decathlon","PROMISE12","GlaS","Kvasir-SEG","2018 Data Science Bowl","CHASE_DB1","Kvasir","CVC-ClinicDB","ACDC"],models:[]},Xn={title:"Weakly-Supervised Semantic Segmentation",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000378-6571e9af.jpg",description:"The semantic segmentation task is to assign a label from a label set to each pixel in an image. In the case of fully supervised setting, the dataset consists of images and their corresponding pixel-level class-specific annotations (expensive pixel-level annotations). However, in the weakly-supervised setting, the dataset consists of images and corresponding annotations that are relatively easy to obtain, such as tags/labels of objects present in the image.",datasets:["PASCAL VOC","ScribbleSup","SEN12MS","ACDC Scribbles"],models:[]},Qn={title:"Background Remove",img:"https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/peacock.jpg",description:"Semantic segmentation, or image segmentation, is the task of clustering parts of an image together which belong to the same object class. It is a form of pixel-level prediction because each pixel in an image is classified according to a category. Some example benchmarks for this task are Cityscapes, PASCAL VOC and ADE20K. Models are usually evaluated with the Mean Intersection-Over-Union (Mean IoU) and Pixel Accuracy metrics.",datasets:["Cityscapes","KITTI","ShapeNet","NYUv2","ScanNet","ADE20K","DAVIS","SYNTHIA","SUN RGB-D","GTA5"],models:[{title:"DIS",subtitle:"Highly Accurate Dichotomous Image Segmentation",img:"https://raw.githubusercontent.com/xuebinqin/DIS/main/figures/dis5k-v1-sailship.jpeg",demo:{type:"object-detection",api:"https://hf.space/embed/ECCV2022/dis-background-removal/+/api/predict",defaultSec:5},benchmark:!1,studio:!1,paper:{name:"DIS: Highly Accurate Dichotomous Image Segmentation",img:"",description:"We present a systematic study on a new task called dichotomous image segmentation (DIS) , which aims to segment highly accurate objects from natural images. To this end, we collected the first large-scale DIS dataset, called DIS5K, which contains 5,470 high-resolution (e.g., 2K, 4K or larger) images covering camouflaged, salient, or meticulous objects in various backgrounds. DIS is annotated with extremely fine-grained labels. Besides, we introduce a simple intermediate supervision baseline (IS-Net) using both feature-level and mask-level guidance for DIS model training. IS-Net outperforms various cutting-edge baselines on the proposed DIS5K, making it a general self-learned supervision network that can facilitate future research in DIS. Further, we design a new metric called human correction efforts (HCE) which approximates the number of mouse clicking operations required to correct the false positives and false negatives. HCE is utilized to measure the gap between models and real-world applications and thus can complement existing metrics. Finally, we conduct the largest-scale benchmark, evaluating 16 representative segmentation models, providing a more insightful discussion regarding object complexities, and showing several potential applications (e.g., background removal, art design, 3D reconstruction). Hoping these efforts can open up promising directions for both academic and industries.",authors:["Xuebin Qin","Luc Van Gool"],paper:"https://arxiv.org/pdf/1804.02767v1",published_date:"6 Mar 2022",conference:{name:"ECCV 2022",paper:"https://arxiv.org/pdf/2203.03041.pdf",abstract:"https://arxiv.org/abs/2203.03041v4"}},code:{url:"https://github.com/xuebinqin/DIS",official:!0,lib:""},task:{tasks:["3D Reconstruction","Image Segmentation","Semantic Segmentation"],datasets:["PASCAL-S"],paperswithcode:"https://paperswithcode.com/paper/highly-accurate-dichotomous-image"}}]},$n=[{writer_id:"Neuralverse",title:"U-Net",subtitle:"Convolutional Networks for Biomedical Image Segmentation",img:"https://www.researchgate.net/profile/Yanfeng-Liu-5/publication/338531111/figure/fig3/AS:846138370433024@1578746516002/Semantic-segmentation-on-a-street-view-in-the-CityScape-dataset-3.jpg",demo:!1,benchmark:!0,studio:!0,paper:{name:"U-Net: Convolutional Networks for Biomedical Image Segmentation",img:"https://production-media.paperswithcode.com/thumbnails/paper/1505.04597.jpg",description:"There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .",authors:["Olaf Ronneberger","Philipp Fischer","Thomas Brox"],paper:"https://arxiv.org/abs/1505.04597v1",published_date:"18 May 2015"},code:{url:"https://github.com/labmlai/annotated_deep_learning_paper_implementations",lib:"torch",official:!1},task:{featured:"Ranked #1 on Colorectal Gland Segmentation: on CRAG (DiceOC metric)",tasks:["Cell Segmentation","Medical Image Segmentation","Multi-tissue Nucleus Segmentation","Pancreas Segmentation","Retinal Vessel Segmentation","Semantic Segmentation","Skin Cancer Segmentation"],datasets:["DRIVE","STARE","LUNA","Kvasir-SEG","CHASE_DB1","Kvasir","CVC-ClinicDB","Kumar","Kvasir-Instrument","RITE","PhC-U373"],paperswithcode:"https://paperswithcode.com/paper/u-net-convolutional-networks-for-biomedical"}}],ei={title:"Semantic Segmentation",img:a.n(Zn).a,description:"Semantic segmentation, or image segmentation, is the task of clustering parts of an image together which belong to the same object class. It is a form of pixel-level prediction because each pixel in an image is classified according to a category. Some example benchmarks for this task are Cityscapes, PASCAL VOC and ADE20K. Models are usually evaluated with the Mean Intersection-Over-Union (Mean IoU) and Pixel Accuracy metrics.",datasets:["Cityscapes","KITTI","ShapeNet","NYUv2","ScanNet","ADE20K","DAVIS","SYNTHIA","SUN RGB-D","GTA5"],subtasks:[Kn,Yn,Jn,qn,Xn,Qn],models:$n.concat(Kn.models,Yn.models,Jn.models,qn.models,Xn.models,Qn.models)},ti=a(45),ai=[{writer_id:"Neuralverse",title:"Adain",subtitle:"Real-time with Adaptive Instance Normalization",img:"https://miro.medium.com/max/1050/1*-O7XstBEW-miUNtPGsKw7A.png",imgcredit:{name:"Adain : A Machine Learning Model for Style Transfer",url:"https://medium.com/axinc-ai/adain-a-machine-learning-model-for-style-transfer-341b242c554b"},demo:{type:"style-transfer",api:"https://hf.space/gradioiframe/aravinds1811/neural-style-transfer/+/api/predict/",defaultSec:10},benchmark:!0,studio:!0,paper:{name:"Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization",img:"https://medium.com/axinc-ai/adain-a-machine-learning-model-for-style-transfer-341b242c554b",description:"Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color & spatial controls, all using a single feed-forward neural network.",authors:["Xun Huang","Serge Belongie"],paper:"https://arxiv.org/abs/1703.06868v2",published_date:"20 Mar 2017",conference:{name:"ICCV 2017",paper:"https://openaccess.thecvf.com/content_ICCV_2017/papers/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.pdf",abstract:"https://openaccess.thecvf.com/content_iccv_2017/html/Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html"}},code:{url:"https://github.com/xunhuang1995/AdaIN-style",lib:"torch",official:!0,colab:""},task:{featured:"",tasks:["Style Transfer"],datasets:["COCO"],paperswithcode:"https://paperswithcode.com/paper/arbitrary-style-transfer-in-real-time-with"}}],ni={title:"Style Transfer",img:a.n(ti).a,imgcredit:{name:" A Neural Algorithm of Artistic Style",url:"https://arxiv.org/pdf/1508.06576v2.pdf"},description:"Style transfer is the task of changing the style of an image in one domain to the style of an image in another domain.",datasets:["DukeMTMC-reID","MPI Sintel","MPIIGaze","iKala","LaMem","Touchdown Dataset","POP909","WebCaricature Dataset","PASTEL","DeepWriting"],subtasks:[],models:ai},ii=a(46),ri=[],oi={title:"Self-Supervised Learning",img:a.n(ii).a,description:"Self-Supervised Learning is proposed for utilizing unlabeled data with the success of supervised learning. Producing a dataset with good labels is expensive, while unlabeled data is being generated all the time. The motivation of Self-Supervised Learning is to make use of the large amount of unlabeled data. The main idea of Self-Supervised Learning is to generate the labels from unlabeled data, according to the structure or characteristics of the data itself, and then train on this unsupervised data in a supervised manner. Self-Supervised Learning is wildly used in representation learning to make a model learn the latent features of the data. This technique is often employed in computer vision, video processing and robot control.",datasets:["STL-10","NSynth","AVA","2D-3D-S","CATER","AVA-ActiveSpeaker","MotionSense"],subtasks:[],models:ri},si=a(47),li={title:"Image Super-Resolution",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000005-775e7af9_NTfuPF6.jpg",description:"In this task, we try to upsample the image and create the high resolution image with help of a low resolution image.",datasets:["KITTI","FFHQ","BSD","CelebA-HQ","VGGFace2","Set14","CASIA-WebFace","Urban100","Set5","DIV2K"],models:[]},ci={title:"Video Super-Resolution",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000036-ea3a7f70_voXPYb4.jpg",imgcredit:{name:"Detail-revealing Deep Video Super-Resolution",url:"https://github.com/jiangsutx/SPMC_VideoSR"},description:"Video super-resolution is the task of upscaling a video from a low-resolution to a high-resolution.",datasets:["Vimeo90K","MSU Video Super Resolution Benchmark","TbD-3D","TbD","Falling Objects","Inter4K","QST"],models:[]},mi={title:"3D Object Super-Resolution",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000001066-4a2c47ae.jpg",imgcredit:{name:"Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation",url:"https://github.com/EdwardSmith1884/Multi-View-Silhouette-and-Depth-Decomposition-for-High-Resolution-3D-Object-Representation"},description:"3D object super-resolution is the task of up-sampling 3D objects.",datasets:["CAMELS Multifield Dataset"],models:[]},di=[{writer_id:"Neuralverse",title:"VRT",subtitle:"A Video Restoration Transformer",img:"https://production-media.paperswithcode.com/thumbnails/papergithubrepo/b012d908-8a55-4b67-8fb4-9215663651d6.gif",imgcredit:{name:"VRT: A Video Restoration Transformer",url:"https://paperswithcode.com/paper/vrt-a-video-restoration-transformer"},demo:{type:"object-detection",api:"https://hf.space/embed/SuwoE/SuperResolution/+/api/predict/",defaultSec:10},benchmark:!0,studio:!0,paper:{name:"VRT: A Video Restoration Transformer",img:"https://raw.githubusercontent.com/JingyunLiang/VRT/main/assets/framework.jpeg",description:"Video restoration (e.g., video super-resolution) aims to restore high-quality frames from low-quality frames. Different from single image restoration, video restoration generally requires to utilize temporal information from multiple adjacent but usually misaligned video frames. Existing deep methods generally tackle with this by exploiting a sliding window strategy or a recurrent architecture, which either is restricted by frame-by-frame restoration or lacks long-range modelling ability. In this paper, we propose a Video Restoration Transformer (VRT) with parallel frame prediction and long-range temporal dependency modelling abilities. More specifically, VRT is composed of multiple scales, each of which consists of two kinds of modules: temporal mutual self attention (TMSA) and parallel warping. TMSA divides the video into small clips, on which mutual attention is applied for joint motion estimation, feature alignment and feature fusion, while self attention is used for feature extraction. To enable cross-clip interactions, the video sequence is shifted for every other layer. Besides, parallel warping is used to further fuse information from neighboring frames by parallel feature warping. Experimental results on three tasks, including video super-resolution, video deblurring and video denoising, demonstrate that VRT outperforms the state-of-the-art methods by large margins () on nine benchmark datasets.",authors:["Jingyun Liang","JieZhang Cao","Yuchen Fan","Kai Zhang","Rakesh Ranjan","Yawei Li","Radu Timofte","Luc van Gool"],paper:"https://arxiv.org/abs/2005.12872v3",published_date:"28 Jan 2022",conference:{}},code:{url:"https://github.com/jingyunliang/vrt",lib:"torch",official:!0,colab:"https://colab.research.google.com/gist/JingyunLiang/deb335792768ad9eb73854a8efca4fe0#file-vrt-demo-on-video-restoration-ipynb="},task:{featured:"Ranked #7 on Panoptic Segmentation on COCO minival",tasks:["Deblurring","Denoising","Image Restoration","Motion Estimation","Super-Resolution","Video Denoising","Video Restoration","Video Super-Resolution"],datasets:["GoPro"],paperswithcode:"https://paperswithcode.com/paper/vrt-a-video-restoration-transformer"}}],pi={title:"Super Resolution",img:a.n(si).a,imgcredit:{name:"MemNet",url:"https://github.com/tyshiwo/MemNet"},description:"Super resolution is the task of taking an input of a low resolution (LR) and upscaling it to that of a high resolution.",datasets:["Perceptual Similarity","Vimeo90K","xView","PIRM","General-100","Flickr1024","Stanford Light Field","QMUL-SurvFace","CATS","TextZoom"],subtasks:[li,ci,mi],models:di.concat(li.models,ci.models,mi.models)},hi=a(48),gi={title:"Domain Generalization",img:a.n(hi).a,imgcredit:{name:"CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features",url:"https://arxiv.org/pdf/1905.04899v2.pdf"},descriptioncredit:{name:"Diagram Image Retrieval using Sketch-Based Deep Learning and Transfer Learning",url:"https://arxiv.org/abs/2004.10780"},description:j.a.createElement("p",null,"The idea of ",j.a.createElement("strong",null,"Domain Generalization")," is to learn from one or multiple training domains, to extract a domain-agnostic model which can be applied to an unseen domain"),datasets:["ImageNet","Fashion-MNIST","Office-Home","PACS","ImageNet-C","DomainNet","ImageNet-A","ImageNet-R","Stylized ImageNet","ImageNet-Sketch"],models:[]},ui=a(49),fi={title:"Unsupervised Domain Adaptation",img:a.n(ui).a,imgcredit:{name:"Adversarial Discriminative Domain Adaptation",url:"https://arxiv.org/pdf/1702.05464v1.pdf"},descriptioncredit:{name:"Domain-Specific Batch Normalization for Unsupervised Domain Adaptation",url:"https://arxiv.org/abs/1906.03950"},description:j.a.createElement("p",null,j.a.createElement("strong",null,"Unsupervised Domain Adaptation")," is a learning framework to transfer knowledge learned from source domains with a large number of annotated training examples to target domains with unlabeled data only."),datasets:["ImageNet","KITTI","Market-1501","Office-Home","Office-31","SYNTHIA","GTA5","ImageNet-C","VisDA-2017","Foggy Cityscapes"],models:[]},bi=a(50),wi={title:"Partial Domain Adaptation",img:a.n(bi).a,imgcredit:{name:"Importance Weighted Adversarial Nets for Partial Domain Adaptation",url:"https://arxiv.org/pdf/1702.05464v1.pdf"},descriptioncredit:{name:"Deep Residual Correction Network for Partial Domain Adaptation",url:"https://arxiv.org/abs/2004.04914"},description:j.a.createElement("p",null,j.a.createElement("strong",null,"Partial Domain Adaptation")," is a transfer learning paradigm, which manages to transfer relevant knowledge from a large-scale source domain to a small-scale target domain."),datasets:["ImageNet","Office-Home","Office-31","DomainNet","VisDA-2017"],models:[]},Ei={title:"Domain Adaptation",img:"https://production-media.paperswithcode.com/thumbnails/task/task-0000000588-823db955.jpg",imgcredit:{name:"Unsupervised Image-to-Image Translation Networks",url:"https://arxiv.org/pdf/1703.00848v6.pdf"},description:"Domain adaptation is the task of adapting models across domains. This is motivated by the challenge where the test and training datasets fall from different data distributions due to some factor. Domain adaptation aims to build machine learning models that can be generalized into a target domain and dealing with the discrepancy across domain distributions.",datasets:["MNIST","SVHN","HMDB51","Office-Home","Office-31","USPS","SYNTHIA","GTA5","GTSRB","OpenSubtitles"],subtasks:[fi,gi,wi],models:[].concat(fi.models,gi.models,wi.models)},ki=[{title:"Computer Vision",demoNum:20,benchmarkNum:100,paperswithcodeNum:100,img:"https://miro.medium.com/max/1400/1*8gmgaAkFdI-9OHY5cA93xQ.png",imgcredit:{name:"MEDIUM",url:"https://medium.com/mlearning-ai/open-cv-computer-vision-ai-how-does-it-work-801d4cdec462"},description:"Computer vision is a field of artificial intelligence (AI) that enables computers and systems to derive meaningful information from digital images, videos and other visual inputs \u2014 and take actions or make recommendations based on that information. If AI enables computers to think, computer vision enables them to see, observe and understand.",datasets:["Cityscapes","KITTI","ShapeNet","NYUv2","ScanNet","ADE20K","DAVIS","SYNTHIA","SUN RGB-D","GTA5"],subtasks:[],models:Sn.models.concat(pi.models,Un.models,ni.models,Ln.models,ei.models,oi.models,tn.models,hn.models,Ei.models,_a.models)},{title:"Natural Language Processing",demoNum:20,benchmarkNum:100,paperswithcodeNum:100,img:"https://miro.medium.com/max/1400/1*8gmgaAkFdI-9OHY5cA93xQ.png",imgcredit:{name:"MEDIUM",url:"https://medium.com/mlearning-ai/open-cv-computer-vision-ai-how-does-it-work-801d4cdec462"},description:"Computer vision is a field of artificial intelligence (AI) that enables computers and systems to derive meaningful information from digital images, videos and other visual inputs \u2014 and take actions or make recommendations based on that information. If AI enables computers to think, computer vision enables them to see, observe and understand.",datasets:["Cityscapes","KITTI","ShapeNet","NYUv2","ScanNet","ADE20K","DAVIS","SYNTHIA","SUN RGB-D","GTA5"],subtasks:[],models:[]},Sn,ei,Un,Ln,ni,oi,tn,hn,Ei,pi,_a],vi=Object(R.c)(U)(S||(S=Object(z.a)(["\n    height: 100%;\n    overflow: hidden;\n    overflow-y: scroll;\n    display: block;\n    padding-bottom: 100px;\n    transition: all 300ms;\n"]))),xi=Object(R.c)(Z)(N||(N=Object(z.a)(["\n    &:first-child {\n      margin-top: 0px;  \n    };\n    height: 130px; \n    border-radius: 20px;\n    position: relative;\n    align-items: center;\n    justify-content: center;\n    overflow: hidden;\n    margin-bottom: 20px;\n    cursor: pointer;\n    margin: 15px;\n"]))),yi=function(e){var t=e.setAbove,a=e.darkMode,n=e.content,i=e.setTask,r=e.setSubTask,o=e.type,s=e.setModel;return j.a.createElement(xi,{onClick:function(){return s(null),void("task"==o?(n.subtasks.length>0&&t(!0),r(null),i(n)):(t(!0),r(n)))}},j.a.createElement(U,{justify:"center",align:"flex-start",position:"absolute",zIndex:101,style:{left:0,backdropFilter:"blur(5px)",bottom:0},padding:"padding: 20px;",width:"70%",height:"100%",bg:Oe(a)},j.a.createElement(W,{className:"light",margin:"margin-bottom: 10px;"},n.title.toUpperCase()),j.a.createElement(U,null,j.a.createElement(_,{align:"center"},j.a.createElement(ge,{stroke:a?"#00B3DA":"#25A2F9",width:12.8,height:12.8}),j.a.createElement(W,{className:"light",weight:"400",margin:"margin-left: 10px;",size:"11"},"20 Demos")),j.a.createElement(_,{margin:"margin: 5px 0px;",align:"center"},j.a.createElement(he,{stroke:a?"#00B3DA":"#25A2F9",width:12.8,height:12.8}),j.a.createElement(W,{className:"light",weight:"400",margin:"margin-left: 10px;",size:"11"},n.benchmarkNum," Benchmarks")),j.a.createElement(_,{align:"center"},j.a.createElement(ue,{stroke:a?"#00B3DA":"#25A2F9",width:12.8,height:12.8}),j.a.createElement(W,{className:"light",weight:"400",margin:"margin-left: 10px;",size:"11"},n.paperswithcodeNum," Papers with Code")))),j.a.createElement(K,{src:n.img,zIndex:100,style:{marginLeft:50},position:"absolute",of:"cover",width:"100%",height:"100%"}))},Ci=function(e){var t=e.setModel,a=e.task,n=e.setTask,i=e.setSubTask,r=e.categoryBar,o=e.setCategoryBar,s=e.darkMode,l=Object(M.useState)(!1),c=Object(T.a)(l,2),m=c[0],d=c[1];return j.a.createElement(Z,{position:"fixed",zIndex:100,width:"21%;",height:Ke-80+"px",style:{right:r?0:-380,top:80,transition:"all 300ms"}},j.a.createElement(G,{height:"35px;",margin:"margin: 15px; margin-bottom: 20px;",position:"relative"},j.a.createElement(G,{style:{position:"absolute",left:0,bottom:12}},j.a.createElement(re,{width:16,height:16,stroke:"rgb(100, 100, 100)"})),j.a.createElement(q,{height:"35px;",placeholder:"Search",color:s?"white":"black",width:"85%",bg:"rgba(0,0,0,0)",padding:"padding-left: 28px;",style:{border:0,borderBottomWidth:1,borderBottomColor:"rgb(100, 100, 100)",letterSpacing:.4,borderBottomStyle:"solid"}})),j.a.createElement(G,{to:"cursor",onClick:function(){return o(!r)},position:"absolute",zIndex:100,style:{right:15,top:15}},j.a.createElement(ne,{width:30,height:30,bg:s?"rgb(35, 35, 35)":"rgb(245, 245, 245)",stroke:s?"rgb(100, 100, 100)":"rgb(120, 120, 120)"})),m&&j.a.createElement(G,{to:"cursor",onClick:function(){return d(!1)},position:"absolute",style:{left:-5,top:82}},j.a.createElement(fe,{height:20,stroke:s?"white":"black"})),j.a.createElement(W,{size:"20",className:"bold",margin:m?"margin: 10px 45px; margin-bottom: 20px;":"margin: 10px 15px; margin-bottom: 20px;",style:{transition:"all 300ms"}},m?a.title:"TASKS"),j.a.createElement(vi,{scrollbarFalse:!0},ki.map(function(e){return j.a.createElement(yi,{type:"task",setAbove:d,setModel:t,setSubTask:i,setTask:n,darkMode:s,key:e.title,content:e})})),j.a.createElement(vi,{bg:s?"rgb(15, 15, 15)":"white",position:"absolute",zIndex:"1000",width:"100%",style:{height:"85%",top:124,right:m?0:"-100%",transition:"all 300ms"},scrollbarFalse:!0},j.a.createElement(yi,{type:"subtask",setAbove:d,setModel:t,setSubTask:i,darkMode:s,content:a}),a.subtasks.map(function(e){return j.a.createElement(yi,{type:"subtask",setAbove:d,setSubTask:i,setModel:t,darkMode:s,key:e.title,content:e})})))},Si=function(e){var t=e.route,a=B().darkMode,n=Object(M.useState)(ki[0]),i=Object(T.a)(n,2),r=i[0],o=i[1],s=Object(M.useState)(null),l=Object(T.a)(s,2),c=l[0],m=l[1],d=Object(M.useState)(null),p=Object(T.a)(d,2),h=p[0],g=p[1],u=Object(M.useState)(!0),f=Object(T.a)(u,2),b=f[0],w=f[1];return j.a.createElement(U,{align:"center",bg:De(a),style:{display:3==t?"flex":"none",paddingTop:170}},j.a.createElement(Pa,{model:h,setModel:g,categoryBar:b,setCategoryBar:w,task:r,setTask:o,setSubTask:m,subTask:c,darkMode:a}),j.a.createElement(Ci,{task:r,setTask:o,setSubTask:m,subTask:c,setModel:g,darkMode:a,categoryBar:b,setCategoryBar:w}))},Ni=function(e){var t=e.width,a=void 0===t?270:t,n=e.height,i=void 0===n?270:n,r=e.stroke,o=void 0===r?"black":r;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 270 270",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{d:"M50 10H220C242.091 10 260 27.9086 260 50V220C260 242.091 242.091 260 220 260H50C27.9086 260 10 242.091 10 220V50C10 27.9086 27.9086 10 50 10Z",stroke:o,"stroke-width":"20"}),j.a.createElement("circle",{cx:"202.5",cy:"65.5",r:"19.5",stroke:o,"stroke-width":"10"}),j.a.createElement("path",{d:"M13 250L74.3886 130.371C81.5891 116.339 101.44 115.774 109.428 129.373L144.685 189.398C148.08 195.178 156.089 196.046 160.644 191.129L176.245 174.287C185.635 164.15 202.231 166.362 208.638 178.605L246 250",stroke:o,"stroke-width":"20"}))},Ii=function(e){var t=e.width,a=void 0===t?270:t,n=e.height,i=void 0===n?270:n,r=e.stroke,o=void 0===r?"black":r;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 270 270",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{d:"M50 10H220C242.091 10 260 27.9086 260 50V220C260 242.091 242.091 260 220 260H50C27.9086 260 10 242.091 10 220V50C10 27.9086 27.9086 10 50 10Z",stroke:o,"stroke-width":"20"}),j.a.createElement("path",{d:"M192 135L106.5 184.363L106.5 85.6366L192 135Z",stroke:o,"stroke-width":"20"}))},Ai=function(e){var t=e.width,a=void 0===t?270:t,n=e.height,i=void 0===n?270:n,r=e.stroke,o=void 0===r?"black":r;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 270 270",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{d:"M50 10H220C242.091 10 260 27.9086 260 50V220C260 242.091 242.091 260 220 260H50C27.9086 260 10 242.091 10 220V50C10 27.9086 27.9086 10 50 10Z",stroke:o,"stroke-width":"20"}),j.a.createElement("line",{x1:"43",y1:"67",x2:"170",y2:"67",stroke:o,"stroke-width":"20"}),j.a.createElement("line",{x1:"93",y1:"113",x2:"220",y2:"113",stroke:o,"stroke-width":"20"}),j.a.createElement("line",{x1:"47",y1:"160",x2:"174",y2:"160",stroke:o,"stroke-width":"20"}),j.a.createElement("line",{x1:"93",y1:"204",x2:"220",y2:"204",stroke:o,"stroke-width":"20"}))},Mi=function(e){var t=e.width,a=void 0===t?270:t,n=e.height,i=void 0===n?270:n,r=e.stroke,o=void 0===r?"black":r;return j.a.createElement("svg",{width:a,height:i,viewBox:"0 0 270 270",fill:"none",xmlns:"http://www.w3.org/2000/svg"},j.a.createElement("path",{d:"M50 10H220C242.091 10 260 27.9086 260 50V220C260 242.091 242.091 260 220 260H50C27.9086 260 10 242.091 10 220V50C10 27.9086 27.9086 10 50 10Z",stroke:o,"stroke-width":"20"}),j.a.createElement("path",{d:"M21 18L33.494 37.1768C37.5934 43.4689 46.9067 43.1659 50.5884 36.6206L55.9813 27.0333C59.868 20.1235 69.8658 20.2682 73.551 27.2876L77.8153 35.41C81.4229 42.2818 91.143 42.6032 95.1968 35.9848L101.794 25.2142C105.737 18.7762 115.119 18.8624 118.943 25.3718L125.378 36.3241C129.243 42.9034 138.757 42.9034 142.622 36.3241L148.689 25.9967C152.597 19.3455 162.245 19.4362 166.027 26.1596L171.186 35.3308C175.03 42.1646 184.885 42.1175 188.664 35.2473L193.137 27.1148C196.957 20.169 206.954 20.2172 210.707 27.1996L216.203 37.4241C219.632 43.8039 228.495 44.5354 232.924 38.8042L249 18",stroke:o,"stroke-width":"10"}))},ji=function(e){var t=e.darkMode,a=e.user,n=e.Components;return j.a.createElement(U,null,j.a.createElement(_,{align:"center",justify:"space-between",margin:"margin: 5px 0px"},j.a.createElement("a",{href:"/"+a.userId},j.a.createElement(_,{align:"center"},j.a.createElement(J,{image:a.avatar,size:40,of:"cover"}),j.a.createElement(U,{margin:"margin-left: 10px;"},j.a.createElement(W,{weight:"500"},a.name),j.a.createElement(W,{color:t?"rgb(200, 200, 200)":"rgb(30, 30, 30)",size:"12"},a.briefIntroduction)))),n))},Oi=function(e){var t=e.darkMode,a=e.Components;return j.a.createElement(_,{align:"center",justify:"space-between",padding:"padding-top: 15px",style:{borderTopWidth:.3,borderTopColor:t?"rgb(50, 50, 50)":"rgb(200, 200, 200)",borderTopStyle:"solid"}},j.a.createElement(_,{to:"cursor",cursorOpaFalse:!0,align:"center"},j.a.createElement(oe,{stroke:t?"rgb(230, 230, 230)":"black",height:18,width:18}),j.a.createElement(W,{margin:"margin-left: 10px; margin-bottom: 3px;",size:"12.8",color:t?"rgb(230, 230, 230)":"rgb(25, 25, 25)",weight:"500"},"Upvote")),j.a.createElement(_,{to:"cursor",cursorOpaFalse:!0,align:"center"},j.a.createElement(se,{stroke:t?"rgb(230, 230, 230)":"black",height:18,width:18}),j.a.createElement(W,{margin:"margin-left: 10px; margin-bottom: 3px;",size:"12.8",color:t?"rgb(230, 230, 230)":"rgb(25, 25, 25)",weight:"500"},"Comment")),j.a.createElement(_,{to:"cursor",cursorOpaFalse:!0,align:"center"},j.a.createElement(ce,{stroke:t?"rgb(230, 230, 230)":"black",height:18,width:18}),j.a.createElement(W,{margin:"margin-left: 10px; margin-bottom: 3px;",size:"12.8",color:t?"rgb(230, 230, 230)":"rgb(25, 25, 25)",weight:"500"},"Repost")),j.a.createElement(_,{to:"cursor",cursorOpaFalse:!0,align:"center"},j.a.createElement(G,{margin:"margin-bottom: 2px;"},j.a.createElement(le,{stroke:t?"rgb(230, 230, 230)":"black",height:16,width:18})),j.a.createElement(W,{margin:"margin-left: 10px; margin-bottom: 3px;",size:"12.8",color:t?"rgb(230, 230, 230)":"rgb(25, 25, 25)",weight:"500"},"Share")),a)},Di=function(e){var t=e.model,a=e.darkMode,n=e.setVisualizer,i=e.visualizer;return j.a.createElement(_,{justify:"space-between",width:"230px"},j.a.createElement(_,{padding:"padding: 3px",to:"cursor",cursorOpaFalse:!0,align:"center"},j.a.createElement(G,{margin:"margin-bottom: 2px;"},j.a.createElement(me,{stroke:a?"white":"black",height:15,width:15})),j.a.createElement(W,{className:"light",margin:"margin-left: 10px; margin-bottom: 3px;",size:"11",weight:"500"},j.a.createElement("a",{style:{color:a?"white":"black"},href:t.paper,target:"_blank"},"Paper"))),j.a.createElement(_,{padding:"padding: 3px",to:"cursor",cursorOpaFalse:!0,align:"center"},j.a.createElement(pe,{stroke:a?"white":"black",height:18,width:18}),j.a.createElement(W,{className:"light",margin:"margin-left: 10px; margin-bottom: 3px;",size:"11",weight:"500"},j.a.createElement("a",{style:{color:a?"white":"black"},href:t.code,target:"_blank"},"Code"))),j.a.createElement(_,{onClick:function(){return n(!i)},padding:"padding: 3px",to:"cursor",cursorOpaFalse:!0,align:"center"},j.a.createElement(ae,{stroke1:a?"white":"black",stroke2:a?"white":"black",height:18,width:18}),j.a.createElement(W,{className:"light",margin:"margin-left: 10px; margin-bottom: 3px;",size:"11",weight:"500"},"Studio")))},Ti=function(e){var t=e.content,a=e.darkMode,n=F(),i=n.visualizer,r=n.setVisualizer;return j.a.createElement(Z,{margin:"margin: 15px 30px 15px 30px",padding:"padding: 10px 15px",style:{width:650},bg:Oe(a),br:"20px;"},j.a.createElement(ji,{user:t.writer_id,darkMode:a,Components:j.a.createElement(Di,{model:t,darkMode:a,visualizer:i,setVisualizer:r})}),j.a.createElement(_,{margin:"margin: 10px 0px;"},j.a.createElement(U,null,j.a.createElement(W,{weight:"600"},t.title),j.a.createElement(W,{margin:"margin: 10px 0px;",lh:21,size:"14",style:{maxHeight:125,overflow:"hidden"}},t.description))),j.a.createElement(G,{style:{height:"100%",width:"100%",maxHeight:619},align:"center",justify:"center"},j.a.createElement(K,{src:t.thumbnail,style:{height:"100%",width:"100%"},of:"cover"})),j.a.createElement(Oi,{darkMode:a}))},Li=a(51),zi=a.n(Li),Ri=function(e){var t=e.user;return j.a.createElement(U,{padding:"padding: 5px 10px"},j.a.createElement(_,{margin:"margin-bottom: 7px;"},j.a.createElement("a",{href:"/"+t.userId,style:{marginTop:3}},j.a.createElement(J,{size:40,of:"cover",image:t.avatar})),j.a.createElement(U,{margin:"margin-left: 15px;",flex:1},j.a.createElement(_,{justify:"space-between",align:"center",margin:"margin-bottom: 3px;"},j.a.createElement("a",{href:"/"+t.userId,style:{marginBottom:2.5}},j.a.createElement(W,{size:"13",weight:"600",style:{overflow:"hidden"}},t.name)),j.a.createElement(G,{margin:"margin-top: 1px;"},j.a.createElement(at,{scale:.9}))),j.a.createElement("a",{href:"/"+t.userId,style:{lineHeight:1}},j.a.createElement(W,{weight:"400",size:"11",style:{overflow:"hidden",maxHeight:47}},t.briefIntroduction)))))},Vi=a(52),Bi=a.n(Vi),Pi=a(53),Fi=a.n(Pi),Hi=a(54),Wi=a.n(Hi),Gi=a(55),_i=a.n(Gi),Ui=[{id:"00001",userId:"jonychoi",name:"Su Hyung Choi",avatar:Bi.a,briefIntroduction:"Undergraduate student researching in Artificial Intelligence, Computer Vision",fullIntroduction:"",workingPlace:"CVLAB @ Korea University",location:"Seoul, South Korea",position:"Research Intern",contact:{email:"thesky7@korea.ac.kr",phone:"",homepage:"jonychoi.github.io",github:"jonychoi",twitter:""},activities:[],googleScholar:"",awards:"",researchInterests:["Computer Vision","Deep Learning"]},{id:"00004",userId:"rohit",name:"Rohit Girdhar",avatar:Wi.a,briefIntroduction:"Research Scientist at Facebook AI",fullIntroduction:"I am a research scientist at Facebook AI Research (FAIR) working on computer vision and machine learning. My current research focuses on modeling people and scenes over (3D) space and time, with applications including 3D recognition, video understanding and physical reasoning. I obtained a PhD from Carnegie Mellon University (CMU) where I worked with Deva Ramanan (here's a link to my dissertation). Earlier I graduated with a masters from CMU as well, working with Martial Hebert, Abhinav Gupta, Kris Kitani and David Fouhey as a Siebel Scholar. Even before I was a CS undergrad at IIIT, Hyderabad, working with C. V. Jawahar. I have also been fortunate to work with some amazing people through internships, at DeepMind (with Andrew Zisserman, Jo\xe3o Carreira and Carl Doersch), Adobe Research (with Josef Sivic and Bryan Russell) and Facebook AI (with Lorenzo Torresani, Georgia Gkioxari and Du Tran).",workingPlace:"Facebook AI Research (FAIR)",location:"Newyork",position:"Research Scientist",contact:{email:"rgirdhar@alumni.cmu.edu",phone:"",homepage:"https://rohitgirdhar.github.io/",github:"rohitgirdhar",twitter:"_rohitgirdhar_"},activities:[],googleScholar:"https://scholar.google.co.in/citations?user=7cuwdr8AAAAJ&hl=en",awards:[],researchInterests:["3D recognition","Video Understanding","Physical Reasoning"]},{id:"00002",userId:"kaiminghe",name:"Kaming He",avatar:Fi.a,briefIntroduction:"Research Scientist at Facebook AI",fullIntroduction:"I am a Research Scientist at Facebook AI Research (FAIR) as of 2016. Before that I was with Microsoft Research Asia (MSRA), which I joined in 2011 after receiving my PhD. My research interests are in computer vision and deep learning. I am a recipient of the PAMI Young Researcher Award in 2018, the Best Paper Award in CVPR 2009, CVPR 2016, ICCV 2017, the Best Student Paper Award in ICCV 2017, the Best Paper Honorable Mention in ECCV 2018, CVPR 2021, and the Everingham Prize in ICCV 2021. My paper on Deep Residual Networks (ResNets) is the most cited paper in all areas in Google Scholar Metrics 2020. Applications of ResNets also include language, speech, and AlphaGo.",workingPlace:"Facebook AI Research (FAIR)",location:"Menlo Park, CA",position:"Research Scientist",contact:{email:"kaiminghe@fb.com",phone:"",homepage:"http://kaiminghe.com",github:"",twitter:""},activities:["Program Chair: ICCV 2023","Area Chair: CVPR 2016, ICCV 2017, CVPR 2018, ECCV 2018, CVPR 2020, CVPR 2021","Associate Editor: IJCV 2016 - 2019","Co-organize a tutorial on Visual Recognition at ECCV 2018.","Co-organize a tutorial on Visual Recognition at CVPR 2018.","Co-organize a tutorial on Instance-level Recognition at ICCV 2017.","Co-organize a tutorial on Deep Learning for Objects and Scenes at CVPR 2017.","Invited to give a tutorial on Deep Residual Networks at ICML 2016.","Co-organize a tutorial on Object Detection at ICCV 2015."],googleScholar:"https://scholar.google.com/citations?user=DhtAFkwAAAAJ&hl=en",awards:["PAMI Everingham Prize, 2021","CVPR Best Paper Honorable Mention, 2021","ECCV Best Paper Honorable Mention, 2018","PAMI Young Researcher Award, 2018","ICCV Best Paper Award (Marr Prize), 2017","ICCV Best Student Paper Award, 2017","CVPR Best Paper Award, 2016","CVPR Best Paper Award, 2009","Outstanding Reviewer: CVPR 2015, ICCV 2015, CVPR 2017","Microsoft Research Asia Fellowship, 2009","Microsoft Research Asia Young Fellowship, 2006"],researchInterests:["Computer Vision","Deep Learning"]},{id:"00005",userId:"mannat",name:"Mannat Singh",avatar:"https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=QOO8OCcAAAAJ&citpid=4",briefIntroduction:"Research Engineer at Facebook AI",fullIntroduction:"",workingPlace:"Facebook AI Research (FAIR)",location:"Greater Newyork City Area",position:"Research Engineer",contact:{email:"",phone:"",homepage:"https://github.com/mannatsingh",github:"mannatsingh",twitter:""},activities:[],googleScholar:"https://scholar.google.com/citations?user=QOO8OCcAAAAJ&hl=en",awards:[],researchInterests:["Computer Vision","Machine Learning"]},{id:"00006",userId:"thomas",name:"Thomas M\xfcller",avatar:_i.a,briefIntroduction:"Senior Research Scientist at NVIDIA",fullIntroduction:"Hi there, I'm Thomas! I'm passionate about all kinds of nerdy things like AI, physics, space travel, computers, music, etc. I am also a senior research scientist at NVIDIA where I work on machine learning and computer graphics. Before that, I received a PhD from ETH and Disney Research, where I had the pleasure to integrate some of my work into Disney's Hyperion Renderer. In my free time, I am most notably involved in developing the online rhythm game osu!. I also enjoy working on all kinds of pet projects. Drop me an email if you would like to chat!",workingPlace:"NVIDIA",location:"",position:"Senior Research Scientist",contact:{email:"thomas94@gmx.net",phone:"",homepage:"https://tom94.net/",github:"",twitter:""},activities:[],googleScholar:"https://scholar.google.ch/citations?user=csrnbFYAAAAJ&hl=en",awards:[],researchInterests:["Artificial Intelligence","Machine Learning","Real-Time Rendering"]}],Zi=function(e){var t=e.darkMode;return j.a.createElement(Z,{width:"280px",margin:"margin-bottom: 20px;",position:"relative",bg:Oe(t),style:{overflow:"hidden"},br:"10px"},j.a.createElement("a",{href:"/jonychoi"},j.a.createElement(U,null,j.a.createElement(G,null,j.a.createElement(K,{width:"100%",of:"cover",height:"100px",src:zi.a})),j.a.createElement(G,{style:{alignSelf:"center",top:50},align:"center",justify:"center",position:"absolute"},j.a.createElement(J,{image:Ae.a,size:80}))),j.a.createElement(U,{margin:"margin-top: 35px;",padding:"padding: 15px"},j.a.createElement(G,{style:{alignSelf:"center"},margin:"margin-bottom: 10px;"},j.a.createElement(W,{className:"bold",weight:"700",size:"14",margin:"margin-bottom: 3px;"},"Su Hyung Choi")),j.a.createElement(G,null,j.a.createElement(W,{size:"13",weight:"400"},"Undergraduate student researching in Artificial Intelligence, Computer Vision")))),j.a.createElement(U,{padding:"padding: 5px 15px;padding-bottom: 20px;"},j.a.createElement(_,{align:"center",justify:"space-between"},j.a.createElement(U,null,j.a.createElement(_,null,j.a.createElement(W,{size:"10.8",weight:"500",className:"light",margin:"margin-right: 5px"},"12"),j.a.createElement(W,{size:"10.8",weight:"500",className:"light"},"MODELS"))),j.a.createElement(U,null,j.a.createElement(_,null,j.a.createElement(W,{size:"10.8",weight:"500",className:"light",margin:"margin-right: 5px"},"12"),j.a.createElement(W,{size:"10.8",weight:"500",className:"light"},"BADGES"))),j.a.createElement(U,null,j.a.createElement(_,null,j.a.createElement(W,{size:"10.8",weight:"500",className:"light",margin:"margin-right: 5px"},"12"),j.a.createElement(W,{size:"10.8",weight:"500",className:"light"},"NFTS"))))))},Ki=function(e){var t=e.darkMode;return j.a.createElement(Z,{width:"280px",padding:"padding: 5px;",bg:Oe(t),style:{overflow:"hidden"},br:"10px"},j.a.createElement(W,{className:"light",size:"14",weight:"600",margin:"margin: 10px; margin-top: 5px;"},"Recommend Friends"),j.a.createElement(U,null,Ui.slice(1,8).map(function(e){return j.a.createElement(Ri,{key:e.name,user:e})})),j.a.createElement(W,{className:"light",weight:"600",margin:"margin: 10px;",size:"12.8"},"View all Recommendations"))},Yi=function(e){var t=e.darkMode;return j.a.createElement(U,{width:"280px"},j.a.createElement(U,null,j.a.createElement(Zi,{darkMode:t}),j.a.createElement(Ki,{darkMode:t})))},Ji=function(e){var t=e.darkMode;return j.a.createElement(_,{align:"center",position:"relative",width:"100%"},j.a.createElement(q,{color:t?"white":"black",width:"100%",padding:"padding: 10px 0px; padding-right: 20px;",className:"light",weight:"400",margin:"margin: 5px 0px;",size:"14",bg:"rgba(0,0,0,0)",placeholder:"Search",style:{border:0,borderBottomWidth:1,borderBottomColor:"rgb(100, 100, 100)",borderBottomStyle:"solid"}}),j.a.createElement(G,{position:"absolute",style:{right:0}},j.a.createElement(re,{stroke:"rgb(100, 100, 100)",width:18,height:18})))},qi=["StyleGAN3","VRT","moolib","Objectron","A ConvNet for the 2020s","Learning Super-Features for Image Retrieval","Instant Neural Graphics Primitives"],Xi=Object(R.c)(G)(I||(I=Object(z.a)(["\n    display: inline-block;\n    cursor: pointer;\n    margin-right: 10px;\n    margin-bottom: 10px;\n    padding: 5px 15px;\n    padding-bottom: 7px;\n    border-radius: 20px;\n    background-color: ","\n"])),function(e){return e.darkMode?"rgb(18, 18, 18)":"rgb(255, 255, 255)"}),Qi=function(e){var t=e.darkMode;return j.a.createElement(Z,{width:"280px",padding:"padding: 15px;",margin:"margin-bottom: 10px;",bg:Oe(t),style:{overflow:"hidden"},br:"10px"},j.a.createElement(W,{className:"bold",size:"15"},"Trendings"),j.a.createElement(Ji,{darkMode:t}),j.a.createElement(G,{style:{display:"block"},margin:"margin-top: 15px;"},qi.map(function(e){return j.a.createElement(Xi,{shadow:!0,darkMode:t},j.a.createElement(W,{size:"13",className:"light",weight:"500"},"# ",e))})),j.a.createElement(W,{className:"light",weight:"600",margin:"margin: 10px 0px;",size:"12.8"},"View all trendings"))},$i=function(e){var t=e.darkMode;return j.a.createElement(U,{width:"280px"},j.a.createElement(U,null,j.a.createElement(Qi,{darkMode:t})))},er=a(56),tr=[{id:"team-00001",userId:"neuralverse",name:"Neuralverse",avatar:a.n(er).a,briefIntroduction:"Beyond the State of the Arts",fullIntroudction:"Neuralverse's mission aims to express the infinite creativity of humans and to pursue truth within the universe.",workingPlace:"NEURALVERSE",location:"",members:[],manager:[],vice_manager:[],contact:{email:"team-neuralverse@neuralverse.us",phone:"",homepage:"https://www.neuralverse.us",github:"neuralverse",twitter:""},activities:[],papers:[],researchInterests:["Artificial Intelligence","Deep Learning","Neural Ops"]}],ar=[{writer_id:Ui[2],title:"Omnivore: A Single Model for Many Visual Modalities",description:"Prior work has studied different visual modalities in isolation and developed separate architectures for recognition of images, videos, and 3D data. Instead, in this paper, we propose a single model which excels at classifying images, videos, and single-view 3D data using exactly the same model parameters. Our 'Omnivore' model leverages the flexibility of transformer-based architectures and is trained jointly on classification tasks from different modalities. Omnivore is simple to train, uses off-the-shelf standard datasets, and performs at-par or better than modality-specific models of the same size. A single Omnivore model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN RGB-D. After finetuning, our models outperform prior work on a variety of vision tasks and generalize across modalities. Omnivore's shared visual representation naturally enables cross-modal recognition without access to correspondences between modalities. We hope our results motivate researchers to model visual modalities together.",date:"20 Jan 2022",authors:["Rohit Girdhar","Mannat Singh","Nikhila Ravi","Laurens van der Maaten","Armand Joulin","Ishan Misra"],paper:"https://arxiv.org/abs/2201.08377v1",code:"https://github.com/facebookresearch/omnivore",thumbnail:"https://raw.githubusercontent.com/facebookresearch/omnivore/main/.github/fig1.jpg",feautred:"Ranked #1 on Semantic Segmentation on NYU Depth v2",tasks:["Action Classification","Action Recognition","Image Classification","Scene Recognition","Semantic Segmentation"],datasets:["Kinetics","Places","NYUv2","SUN RGB-D","Kinetics 400","iNaturalist","Something-Something V2","Oxford-IIIT Pets","EPIC-KITCHENS-100"],paperswithcode:"https://paperswithcode.com/paper/omnivore-a-single-model-for-many-visual"},{writer_id:Ui[4],title:"Revisiting Weakly Supervised Pre-Training of Visual Perception Models",description:"Model pre-training is a cornerstone of modern visual recognition systems. Although fully supervised pre-training on datasets like ImageNet is still the de-facto standard, recent studies suggest that large-scale weakly supervised pre-training can outperform fully supervised approaches. This paper revisits weakly-supervised pre-training of models using hashtag supervision with modern versions of residual networks and the largest-ever dataset of images and corresponding hashtags. We study the performance of the resulting models in various transfer-learning settings including zero-shot transfer. We also compare our models with those obtained via large-scale self-supervised learning. We find our weakly-supervised models to be very competitive across all settings, and find they substantially outperform their self-supervised counterparts. We also include an investigation into whether our models learned potentially troubling associations or stereotypes. Overall, our results provide a compelling argument for the use of weakly supervised learning in the development of visual recognition systems. Our models, Supervised Weakly through hashtAGs (SWAG), are available publicly.",date:"20 Jan 2022",authors:["Mannat Singh","Laura Gustafson","Aaron Adcock","Vinicius de Freitas Reis","Bugra Gedik","Raj Prateek Kosaraju","Dhruv Mahajan","Ross Girshick","Piotr Doll\xe1r","Laurens van der Maaten"],paper:"https://arxiv.org/abs/2201.08371v1",code:"https://github.com/facebookresearch/SWAG",thumbnail:"https://production-media.paperswithcode.com/thumbnails/paper/2201.08371.jpg",tasks:["Fine-Grained","Image Classification","mage Classification","Self-Supervised Learning","Transfer Learning"],featured:"Ranked #1 on Image Classification on Places365-Standard (using extra training data)",datasets:["ImageNet","CUB-200-2011","Places","iNaturalist","ObjectNet","Places365","JFT-3B"],paperswithcode:"https://paperswithcode.com/paper/revisiting-weakly-supervised-pre-training-of"},{writer_id:Ui[3],title:"Instant Neural Graphics Primitives with a Multiresolution Hash Encoding",description:"Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920 X 1080.",date:"16 Jan 2022",authors:["Thomas M\xfcller","Alex Evans","Christoph Schied","Alexander Keller"],paper:"https://arxiv.org/abs/2201.05989v1",code:"https://github.com/nvlabs/instant-ngp",thumbnail:"https://raw.githubusercontent.com/NVlabs/instant-ngp/master/docs/assets_readme/fox.gif",tasks:[],datasets:[],paperswithcode:"https://paperswithcode.com/paper/instant-neural-graphics-primitives-with-a"},{writer_id:tr[0],title:"A ConvNet for the 2020s",description:'The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.',date:"10 Jan 2022",authors:["Zhuang Liu","Hanzi Mao","Chao-yuan Wu","Christoph Feichtenhofer","Trevor Darrell","Saining Xie"],paper:"https://arxiv.org/abs/2201.03545v1",code:"https://github.com/facebookresearch/ConvNeXt",thumbnail:"https://user-images.githubusercontent.com/8370623/148624004-e9581042-ea4d-4e10-b3bd-42c92b02053b.png",tasks:["Domain Generalization","Image Classification","Object Detection","Semantic Segmentation"],datasets:["ImageNet","COCO","ADE20K","ImageNet-C","ImageNet-A","ImageNet-R","ImageNet-Sketch"],paperswithcode:"https://paperswithcode.com/paper/a-convnet-for-the-2020s"},{writer_id:Ui[1],title:"Masked Autoencoders Are Scalable Vision Learners",description:"Model pre-training is a cornerstone of modern visual recognition systems. Although fully supervised pre-training on datasets like ImageNet is still the de-facto standard, recent studies suggest that large-scale weakly supervised pre-training can outperform fully supervised approaches. This paper revisits weakly-supervised pre-training of models using hashtag supervision with modern versions of residual networks and the largest-ever dataset of images and corresponding hashtags. We study the performance of the resulting models in various transfer-learning settings including zero-shot transfer. We also compare our models with those obtained via large-scale self-supervised learning. We find our weakly-supervised models to be very competitive across all settings, and find they substantially outperform their self-supervised counterparts. We also include an investigation into whether our models learned potentially troubling associations or stereotypes. Overall, our results provide a compelling argument for the use of weakly supervised learning in the development of visual recognition systems. Our models, Supervised Weakly through hashtAGs (SWAG), are available publicly.",date:"11 Nov 2021",authors:["Kaiming He","Xinlei Chen","Saining Xie","Yanghao Li","Piotr Doll\xe1r","Ross Girshick"],paper:"https://arxiv.org/abs/2111.06377v2",code:"https://github.com/facebookresearch/mae",thumbnail:"https://user-images.githubusercontent.com/11435359/146857310-f258c86c-fde6-48e8-9cee-badd2b21bd2c.png",tasks:["Domain Generalization","Image Classification","Object Detection","Self-Supervised Image Classification","Self-Supervised Learning","Semantic Segmentation"],datasets:["ImageNet","COCO","Places","Places205","ADE20K","ImageNet-C","iNaturalist","ImageNet-A","ImageNet-R","ImageNet-Sketch","Places365"],paperswithcode:"https://paperswithcode.com/paper/masked-autoencoders-are-scalable-vision"},{writer_id:tr[0],title:"Deep Learning Interviews: Hundreds of fully solved job interview questions from a wide range of key topics in AI",description:"The second edition of Deep Learning Interviews is home to hundreds of fully-solved problems, from a wide range of key topics in AI. It is designed to both rehearse interview or exam specific topics and provide machine learning MSc / PhD. students, and those awaiting an interview a well-organized overview of the field. The problems it poses are tough enough to cut your teeth on and to dramatically improve your skills-but they're framed within thought-provoking questions and engaging stories. That is what makes the volume so specifically valuable to students and job seekers: it provides them with the ability to speak confidently and quickly on any relevant topic, to answer technical questions clearly and correctly, and to fully understand the purpose and meaning of interview questions and answers. Those are powerful, indispensable advantages to have when walking into the interview room. The book's contents is a large inventory of numerous topics relevant to DL job interviews and graduate level exams. That places this work at the forefront of the growing trend in science to teach a core set of practical mathematical and computational skills. It is widely accepted that the training of every computer scientist must include the fundamental theorems of ML, and AI appears in the curriculum of nearly every university. This volume is designed as an excellent reference for graduates of such programs.",authors:["Shlomo Kashani","Amir Ivry"],paper:"https://arxiv.org/abs/2201.00650v2",code:"https://github.com/BoltzmannEntropy/interviews.ai",thumbnail:"https://raw.githubusercontent.com/BoltzmannEntropy/interviews.ai/main/assets/cover-amazon-print2.png",tasks:[],datasets:[],paperswithcode:"https://paperswithcode.com/paper/deep-learning-interviews-hundreds-of-fully"},{writer_id:tr[0],title:"Detecting Twenty-thousand Classes using Image-level Supervision",description:"Current object detectors are limited in vocabulary size due to the small scale of detection datasets. Image classifiers, on the other hand, reason about much larger vocabularies, as their datasets are larger and easier to collect. We propose Detic, which simply trains the classifiers of a detector on image classification data and thus expands the vocabulary of detectors to tens of thousands of concepts. Unlike prior work, Detic does not assign image labels to boxes based on model predictions, making it much easier to implement and compatible with a range of detection architectures and backbones. Our results show that Detic yields excellent detectors even for classes without box annotations. It outperforms prior work on both open-vocabulary and long-tail detection benchmarks. Detic provides a gain of 2.4 mAP for all classes and 8.3 mAP for novel classes on the open-vocabulary LVIS benchmark. On the standard LVIS benchmark, Detic reaches 41.7 mAP for all classes and 41.7 mAP for rare classes. For the first time, we train a detector with all the twenty-one-thousand classes of the ImageNet dataset and show that it generalizes to new datasets without fine-tuning. Code is available at https://github.com/facebookresearch/Detic.",date:"7 Jan 2022",authors:["Xingyi Zhou","Rohit Girdhar","Armand Joulin","Phillip Kr\xe4henb\xfchl","Ishan Misra"],paper:"https://arxiv.org/abs/2201.02605v2",code:"https://github.com/facebookresearch/Detic",thumbnail:"https://raw.githubusercontent.com/facebookresearch/Detic/main/docs/teaser.jpeg",tasks:["Image Classification"],datasets:["ImageNet","COCO","Conceptual Captions","LVIS","Objects365"],paperswithcode:"https://paperswithcode.com/paper/detecting-twenty-thousand-classes-using-image"}],nr=function(e){var t=e.darkMode,a=na(""),n=a.value,i=a.onChange,r=new Date,o=String(r).split(" ")[2];return j.a.createElement(Z,{justify:"space-between",margin:"margin: 0px 30px 10px 30px",padding:"padding: 15px;",bg:Oe(t),br:"10px",width:"650px",height:"120px"},j.a.createElement(_,null,j.a.createElement(J,{image:Ae.a,size:50}),j.a.createElement(q,{margin:"margin-left: 10px;",placeholderTextColor:t?"white":"black",padding:"padding-left: 10px;",color:t?"white":"black",bg:"rgba(0,0,0,0)",style:{borderTop:0,borderLeft:0,borderRight:0,borderBottom:"1px solid rgb(100, 100, 100)",width:"100%"},placeholder:"Write a Verse",size:"14",value:n,onChange:i})),j.a.createElement(_,{align:"center",justify:"space-between"},j.a.createElement(_,{to:"cursor",align:"center"},j.a.createElement(Ni,{stroke:t?"rgb(200, 200, 200)":"rgb(100, 100, 100)",width:18,height:18}),j.a.createElement(W,{size:"11",className:"light",weight:"700",margin:"margin-left: 10px;"},"PHOTO")),j.a.createElement(_,{to:"cursor",align:"center"},j.a.createElement(Ii,{stroke:t?"rgb(200, 200, 200)":"rgb(100, 100, 100)",width:18,height:18}),j.a.createElement(W,{size:"11",className:"light",weight:"700",margin:"margin-left: 10px;"},"VIDEO")),j.a.createElement(_,{to:"cursor",align:"center"},j.a.createElement($,{stroke1:t?"rgb(200, 200, 200)":"rgb(100, 100, 100)",stroke2:t?"rgb(200, 200, 200)":"rgb(100, 100, 100)",width:20,height:20}),j.a.createElement(W,{size:"11",className:"light",weight:"700",margin:"margin-left: 10px;"},"MODEL")),j.a.createElement(_,{to:"cursor",align:"center"},j.a.createElement(Ai,{stroke:t?"rgb(200, 200, 200)":"rgb(100, 100, 100)",width:18,height:18}),j.a.createElement(W,{size:"11",className:"light",weight:"700",margin:"margin-left: 10px;"},"DOCUMENT")),j.a.createElement(_,{to:"cursor",align:"center"},j.a.createElement(G,{width:"18px",height:"18px",align:"center",justify:"center",position:"relative"},j.a.createElement(W,{color:t?"rgb(200, 200, 200)":"rgb(100, 100, 100)",size:"11",className:"light",weight:"700",position:"absolute"},o),j.a.createElement(G,{position:"absolute"},j.a.createElement(Mi,{stroke:t?"rgb(200, 200, 200)":"rgb(100, 100, 100)",width:18,height:18}))),j.a.createElement(W,{size:"11",className:"light",weight:"700",margin:"margin-left: 10px;"},"EVENT"))))},ir=function(e){var t=e.darkMode;return j.a.createElement(_,null,j.a.createElement(Yi,{darkMode:t}),j.a.createElement(U,null,j.a.createElement(nr,{darkMode:t}),ar.map(function(e){return j.a.createElement(Ti,{key:e.title,content:e,darkMode:t})})),j.a.createElement($i,{darkMode:t}))},rr=function(e){var t=e.route,a=B().darkMode;return j.a.createElement(_,{bg:a?"rgb(15, 15, 15)":"rgb(240, 240, 240)",style:{display:1==t?"flex":"none",paddingTop:110},justify:"center"},j.a.createElement(ir,{darkMode:a}))},or=function(e){e.bar,e.setBar;return j.a.createElement(_,null)},sr=function(e){e.darkMode;return j.a.createElement(U,null)},lr=(a(18),a(57),a(20),function(e){var t=e.bar,a=e.setBar;return j.a.createElement(G,null,j.a.createElement(or,{bar:t,setBar:a}))}),cr=function(e){var t=e.route,a=B().darkMode,n=Object(M.useState)(0),i=Object(T.a)(n,2),r=i[0],o=i[1];return j.a.createElement(U,{align:"center",bg:De(a),style:{display:4==t?"flex":"none",paddingTop:80}},j.a.createElement(lr,{bar:r,setBar:o,darkMode:a}),j.a.createElement(sr,{bar:r,setBar:o,darkMode:a}))},mr=function(e){var t=e.route,a=B().darkMode;return j.a.createElement(U,{align:"center",bg:De(a),style:{display:5==t?"flex":"none",paddingTop:80}})},dr=(a(96),Object(R.a)(A||(A=Object(z.a)(["\n.landing {\n    // filter: progid: DXImageTransform.Microsoft.gradient(gradientType=1, startColorstr='#003073', endColorstr='#029797');\n    // background-image: url(//img.alicdn.com/tps/TB1d.u8MXXXXXXuXFXXXXXXXXXX-1900-790.jpg);\n    // background-size: 100%;\n    // background-image: -webkit-gradient(linear, 0 0, 100% 100%, color-stop(0, #003073), color-stop(100%, #029797));\n    // background-image: -webkit-linear-gradient(135deg, #003073, #029797);\n    // background-image: -moz-linear-gradient(45deg, #003073, #029797);\n    // background-image: -ms-linear-gradient(45deg, #003073 0, #029797 100%);\n    // background-image: -o-linear-gradient(45deg, #003073, #029797);\n    // background-image: linear-gradient(135deg, #003073, #029797);\n    // text-align: center;\n    // margin: 0px;\n    // overflow: hidden;\n  }\n  a {\n      color: ","\n  }\n  a {\n    text-decoration: none;\n  }\n\n  a:visited {\n    color: inherit;\n  }\n  ::-webkit-scrollbar {\n    width: 5px;\n  }\n\n  /* Track */\n  ::-webkit-scrollbar-track {\n    background: rgba(0,0,0,0); \n  }\n  \n  /* Handle */\n  ::-webkit-scrollbar-thumb {\n    background: #888; \n  }\n\n  /* Handle on hover */\n  ::-webkit-scrollbar-thumb:hover {\n    background: #555; \n  }\n"])),function(e){return e.darkMode?"#1C8BD8":"#00B3DA"})),pr=function(e){Object(L.a)(e);var t=Object(M.useState)(0),a=Object(T.a)(t,2),n=a[0],i=a[1],r=B().darkMode;return j.a.createElement("div",{style:{overflowX:"hidden"}},j.a.createElement(dr,{darkMode:r}),j.a.createElement(Le,{route:n,setRoute:i}),j.a.createElement(St,{route:n,setRoute:i}),j.a.createElement(Ve,{route:n}),j.a.createElement(Pe,{route:n}),j.a.createElement(Fe,{route:n}),j.a.createElement(Si,{route:n}),j.a.createElement(rr,{route:n}),j.a.createElement(cr,{route:n}),j.a.createElement(mr,{route:n}),j.a.createElement(Qe,null),j.a.createElement(Ge,null),j.a.createElement(lt,null))};D.a.render(j.a.createElement(j.a.StrictMode,null,j.a.createElement(H,null,j.a.createElement(pr,null))),document.getElementById("root"))}],[[58,2,1]]]);
//# sourceMappingURL=main.ec8a37c3.chunk.js.map