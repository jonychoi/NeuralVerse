export default [
    {
        title: "Omnivore: A Single Model for Many Visual Modalities",
        description: "Prior work has studied different visual modalities in isolation and developed separate architectures for recognition of images, videos, and 3D data. Instead, in this paper, we propose a single model which excels at classifying images, videos, and single-view 3D data using exactly the same model parameters. Our 'Omnivore' model leverages the flexibility of transformer-based architectures and is trained jointly on classification tasks from different modalities. Omnivore is simple to train, uses off-the-shelf standard datasets, and performs at-par or better than modality-specific models of the same size. A single Omnivore model obtains 86.0% on ImageNet, 84.1% on Kinetics, and 67.1% on SUN RGB-D. After finetuning, our models outperform prior work on a variety of vision tasks and generalize across modalities. Omnivore's shared visual representation naturally enables cross-modal recognition without access to correspondences between modalities. We hope our results motivate researchers to model visual modalities together.",
        date: "20 Jan 2022",
        authors: ["Rohit Girdhar", "Mannat Singh", "Nikhila Ravi", "Laurens van der Maaten", "Armand Joulin", "Ishan Misra"],
        paper: "https://arxiv.org/abs/2201.08377v1",
        code: "https://github.com/facebookresearch/omnivore",
        thumbnail: "https://raw.githubusercontent.com/facebookresearch/omnivore/main/.github/fig1.jpg",
        feautred: "Ranked #1 on Semantic Segmentation on NYU Depth v2",
        tasks: ["Action Classification", "Action Recognition", "Image Classification", "Scene Recognition", "Semantic Segmentation",],
        datasets: ["Kinetics", "Places", "NYUv2", "SUN RGB-D", "Kinetics 400", "iNaturalist", "Something-Something V2", "Oxford-IIIT Pets", "EPIC-KITCHENS-100"],
        paperswithcode: "https://paperswithcode.com/paper/omnivore-a-single-model-for-many-visual",
    },
    {
        title: "Revisiting Weakly Supervised Pre-Training of Visual Perception Models", 
        description: "Model pre-training is a cornerstone of modern visual recognition systems. Although fully supervised pre-training on datasets like ImageNet is still the de-facto standard, recent studies suggest that large-scale weakly supervised pre-training can outperform fully supervised approaches. This paper revisits weakly-supervised pre-training of models using hashtag supervision with modern versions of residual networks and the largest-ever dataset of images and corresponding hashtags. We study the performance of the resulting models in various transfer-learning settings including zero-shot transfer. We also compare our models with those obtained via large-scale self-supervised learning. We find our weakly-supervised models to be very competitive across all settings, and find they substantially outperform their self-supervised counterparts. We also include an investigation into whether our models learned potentially troubling associations or stereotypes. Overall, our results provide a compelling argument for the use of weakly supervised learning in the development of visual recognition systems. Our models, Supervised Weakly through hashtAGs (SWAG), are available publicly.",
        date: "20 Jan 2022",
        authors: ["Mannat Singh", "Laura Gustafson", "Aaron Adcock", "Vinicius de Freitas Reis", "Bugra Gedik", "Raj Prateek Kosaraju", "Dhruv Mahajan", "Ross Girshick", "Piotr Dollár", "Laurens van der Maaten"],
        paper: "https://arxiv.org/abs/2201.08371v1",
        code: "https://github.com/facebookresearch/SWAG",
        thumbnail: 'https://production-media.paperswithcode.com/thumbnails/paper/2201.08371.jpg',
        tasks: ["Fine-Grained", "Image Classification","mage Classification", "Self-Supervised Learning" ,"Transfer Learning"],
        featured: "Ranked #1 on Image Classification on Places365-Standard (using extra training data)",
        datasets: ["ImageNet", "CUB-200-2011", "Places", "iNaturalist", "ObjectNet", "Places365", "JFT-3B"],
        paperswithcode: "https://paperswithcode.com/paper/revisiting-weakly-supervised-pre-training-of",
    },
    {
        title: "Instant Neural Graphics Primitives with a Multiresolution Hash Encoding", 
        description: "Neural graphics primitives, parameterized by fully connected neural networks, can be costly to train and evaluate. We reduce this cost with a versatile new input encoding that permits the use of a smaller network without sacrificing quality, thus significantly reducing the number of floating point and memory access operations: a small neural network is augmented by a multiresolution hash table of trainable feature vectors whose values are optimized through stochastic gradient descent. The multiresolution structure allows the network to disambiguate hash collisions, making for a simple architecture that is trivial to parallelize on modern GPUs. We leverage this parallelism by implementing the whole system using fully-fused CUDA kernels with a focus on minimizing wasted bandwidth and compute operations. We achieve a combined speedup of several orders of magnitude, enabling training of high-quality neural graphics primitives in a matter of seconds, and rendering in tens of milliseconds at a resolution of 1920 X 1080.",
        date: "16 Jan 2022",
        authors: ["Thomas Müller", "Alex Evans", "Christoph Schied", "Alexander Keller"],
        paper: "https://arxiv.org/abs/2201.05989v1",
        code: "https://github.com/nvlabs/instant-ngp",
        thumbnail: "https://raw.githubusercontent.com/NVlabs/instant-ngp/master/docs/assets_readme/fox.gif",
        tasks: [],
        datasets: [],
        paperswithcode: "https://paperswithcode.com/paper/instant-neural-graphics-primitives-with-a",
    },
    {
        title: "A ConvNet for the 2020s", 
        description: 'The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.',
        date: "10 Jan 2022",
        authors: ["Zhuang Liu", "Hanzi Mao", "Chao-yuan Wu", "Christoph Feichtenhofer", "Trevor Darrell", "Saining Xie"],
        paper: "https://arxiv.org/abs/2201.03545v1",
        code: "https://github.com/facebookresearch/ConvNeXt",
        thumbnail: "https://user-images.githubusercontent.com/8370623/148624004-e9581042-ea4d-4e10-b3bd-42c92b02053b.png",
        tasks: ["Domain Generalization", "Image Classification", "Object Detection", "Semantic Segmentation"],
        datasets: ["ImageNet", "COCO", "ADE20K", "ImageNet-C", "ImageNet-A", "ImageNet-R", "ImageNet-Sketch"],
        paperswithcode: "https://paperswithcode.com/paper/a-convnet-for-the-2020s",
    },
    {
        title: "Masked Autoencoders Are Scalable Vision Learners",
        description: "Model pre-training is a cornerstone of modern visual recognition systems. Although fully supervised pre-training on datasets like ImageNet is still the de-facto standard, recent studies suggest that large-scale weakly supervised pre-training can outperform fully supervised approaches. This paper revisits weakly-supervised pre-training of models using hashtag supervision with modern versions of residual networks and the largest-ever dataset of images and corresponding hashtags. We study the performance of the resulting models in various transfer-learning settings including zero-shot transfer. We also compare our models with those obtained via large-scale self-supervised learning. We find our weakly-supervised models to be very competitive across all settings, and find they substantially outperform their self-supervised counterparts. We also include an investigation into whether our models learned potentially troubling associations or stereotypes. Overall, our results provide a compelling argument for the use of weakly supervised learning in the development of visual recognition systems. Our models, Supervised Weakly through hashtAGs (SWAG), are available publicly.",
        date: "11 Nov 2021",
        authors: ["Kaiming He", "Xinlei Chen", "Saining Xie", "Yanghao Li", "Piotr Dollár", "Ross Girshick"],
        paper: "https://arxiv.org/abs/2111.06377v2",
        code: "https://github.com/facebookresearch/mae",
        thumbnail: "https://user-images.githubusercontent.com/11435359/146857310-f258c86c-fde6-48e8-9cee-badd2b21bd2c.png",
        tasks: ["Domain Generalization", "Image Classification", "Object Detection", "Self-Supervised Image Classification", "Self-Supervised Learning", "Semantic Segmentation"],
        datasets: ["ImageNet", "COCO", "Places", "Places205", "ADE20K", "ImageNet-C", "iNaturalist", "ImageNet-A", "ImageNet-R", "ImageNet-Sketch", "Places365"],
        paperswithcode: "https://paperswithcode.com/paper/masked-autoencoders-are-scalable-vision",
    },
    {
        title: "Deep Learning Interviews: Hundreds of fully solved job interview questions from a wide range of key topics in AI",
        description: "The second edition of Deep Learning Interviews is home to hundreds of fully-solved problems, from a wide range of key topics in AI. It is designed to both rehearse interview or exam specific topics and provide machine learning MSc / PhD. students, and those awaiting an interview a well-organized overview of the field. The problems it poses are tough enough to cut your teeth on and to dramatically improve your skills-but they're framed within thought-provoking questions and engaging stories. That is what makes the volume so specifically valuable to students and job seekers: it provides them with the ability to speak confidently and quickly on any relevant topic, to answer technical questions clearly and correctly, and to fully understand the purpose and meaning of interview questions and answers. Those are powerful, indispensable advantages to have when walking into the interview room. The book's contents is a large inventory of numerous topics relevant to DL job interviews and graduate level exams. That places this work at the forefront of the growing trend in science to teach a core set of practical mathematical and computational skills. It is widely accepted that the training of every computer scientist must include the fundamental theorems of ML, and AI appears in the curriculum of nearly every university. This volume is designed as an excellent reference for graduates of such programs.",
        authors: ["Shlomo Kashani", "Amir Ivry"],
        paper: "https://arxiv.org/abs/2201.00650v2",
        code: "https://github.com/BoltzmannEntropy/interviews.ai",
        thumbnail: "https://github.com/BoltzmannEntropy/interviews.ai/blob/main/assets/cover-amazon-print2.png",
        tasks: [],
        datasets: [],
        paperswithcode: "https://paperswithcode.com/paper/deep-learning-interviews-hundreds-of-fully",
    },
    {
        title: "Detecting Twenty-thousand Classes using Image-level Supervision",
        description: "Current object detectors are limited in vocabulary size due to the small scale of detection datasets. Image classifiers, on the other hand, reason about much larger vocabularies, as their datasets are larger and easier to collect. We propose Detic, which simply trains the classifiers of a detector on image classification data and thus expands the vocabulary of detectors to tens of thousands of concepts. Unlike prior work, Detic does not assign image labels to boxes based on model predictions, making it much easier to implement and compatible with a range of detection architectures and backbones. Our results show that Detic yields excellent detectors even for classes without box annotations. It outperforms prior work on both open-vocabulary and long-tail detection benchmarks. Detic provides a gain of 2.4 mAP for all classes and 8.3 mAP for novel classes on the open-vocabulary LVIS benchmark. On the standard LVIS benchmark, Detic reaches 41.7 mAP for all classes and 41.7 mAP for rare classes. For the first time, we train a detector with all the twenty-one-thousand classes of the ImageNet dataset and show that it generalizes to new datasets without fine-tuning. Code is available at https://github.com/facebookresearch/Detic.",
        date: "7 Jan 2022",
        authors: ["Xingyi Zhou", "Rohit Girdhar", "Armand Joulin", "Phillip Krähenbühl", "Ishan Misra"],
        paper: "https://arxiv.org/abs/2201.02605v2",
        code: "https://github.com/facebookresearch/Detic",
        thumbnail: "https://raw.githubusercontent.com/facebookresearch/Detic/main/docs/teaser.jpeg",
        tasks: ["Image Classification"],
        datasets: ["ImageNet", "COCO", "Conceptual Captions", "LVIS", "Objects365"],
        paperswithcode: "https://paperswithcode.com/paper/detecting-twenty-thousand-classes-using-image",
    },
]